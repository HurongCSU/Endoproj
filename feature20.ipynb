{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(849, 6097)\n",
      "[121, 62, 193, 810, 288, 237, 40, 34, 179, 17, 258, 153, 623, 49, 285, 624, 93, 848, 640, 296, 203, 228, 212, 211, 115, 264, 2, 209, 174, 649, 108, 91, 122, 817, 180, 243, 166, 635, 700, 771, 349, 701, 585, 367, 432, 687, 356, 445, 658, 495, 745, 430, 569, 684, 599, 660, 546, 756, 733, 368, 805, 679, 415, 464, 399, 419, 402, 499, 688, 611, 448, 500, 414, 406, 665, 777, 720, 407, 664, 527, 523, 409, 710]\n",
      "681\n",
      "369\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1]\n",
      "38\n",
      "38\n",
      "312\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "import joblib\n",
    "\n",
    "file_feature = \"./csv/endometrium.csv\"\n",
    "file_train = \"./csv/train.csv\"\n",
    "file_validate = \"./csv/validation.csv\"\n",
    "file_test = \"./csv/test.csv\"\n",
    "\n",
    "f = open(file_feature)\n",
    "csv_f = csv.reader(f)\n",
    "features = next(csv_f)\n",
    "dataset = pd.read_csv(file_feature, names=features, usecols=range(1,6098), dtype=np.float64, skiprows=1, low_memory=False)\n",
    "f = open(file_train)\n",
    "csv_f = csv.reader(f)\n",
    "features = next(csv_f)\n",
    "dataset_train = pd.read_csv(file_train, names=features, usecols=range(1,1), dtype=np.float64, skiprows=1, low_memory=False)\n",
    "\n",
    "with open('./csv/train.csv','r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    train_list = [row[1] for row in reader]\n",
    "with open('./csv/validation.csv','r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    validation_list = [row['patient'] for row in reader]\n",
    "with open('./csv/test.csv','r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    test_list = [row['patient'] for row in reader]\n",
    "\n",
    "dataset['outcome'] = pd.to_numeric(dataset['outcome'],errors='coerce')\n",
    "array_OG = dataset.values\n",
    "print(array_OG.shape)\n",
    "train_list = train_list[1:]\n",
    "validation_list = validation_list[0:]\n",
    "test_list = test_list[0:]\n",
    "#print(test_list)\n",
    "#print(train_list)\n",
    "#print(validation_list)\n",
    "\n",
    "def cat_str(num_list):\n",
    "    n_list = []\n",
    "    for i in num_list:\n",
    "        temp = i[12:]\n",
    "        n_list.append(temp)\n",
    "    n_list = [int(x) for x in n_list]\n",
    "    return n_list\n",
    "\n",
    "train_list = cat_str(train_list)\n",
    "validation_list = cat_str(validation_list)\n",
    "test_list = cat_str(test_list)\n",
    "\n",
    "#print(train_list)\n",
    "#print(validation_list)\n",
    "#print(test_list)\n",
    "print(test_list)\n",
    "\n",
    "train_feature = []\n",
    "validate_feature = []\n",
    "test_feature = []\n",
    "count = 1\n",
    "for i in range(len(array_OG)):\n",
    "    num = i + 1\n",
    "    if num in train_list:\n",
    "        train_feature.append(array_OG[i])\n",
    "    elif num in validation_list:\n",
    "        validate_feature.append(array_OG[i])\n",
    "    elif num in test_list:\n",
    "        #print(count)\n",
    "        count = count + 1\n",
    "        test_feature.append(array_OG[i])\n",
    "        #print(num)\n",
    "        #print(array_OG[i,6096])\n",
    "        \n",
    "train_feature = np.array(train_feature)\n",
    "validate_feature = np.array(validate_feature)\n",
    "test_feature = np.array(test_feature)\n",
    "\n",
    "train_feature = pd.DataFrame(train_feature)\n",
    "\n",
    "train_feature.dropna(axis=1, thresh=2, inplace=True)\n",
    "#train_feature.dropna(how='all',thresh = 20,inplace=True)\n",
    "train_feature = np.array(train_feature)\n",
    "wh_inf = np.isinf(train_feature)\n",
    "train_feature[wh_inf]=0\n",
    "wh_nan = np.isnan(train_feature)\n",
    "train_feature[wh_nan]=0\n",
    "\n",
    "validate_feature = pd.DataFrame(validate_feature)\n",
    "validate_feature.dropna(axis=1, thresh=2, inplace=True)\n",
    "#validate_feature.dropna(how='all',thresh = 20,inplace=True)\n",
    "validate_feature = np.array(validate_feature)\n",
    "wh_inf = np.isinf(validate_feature)\n",
    "validate_feature[wh_inf]=0\n",
    "wh_nan = np.isnan(validate_feature)\n",
    "validate_feature[wh_nan]=0\n",
    "\n",
    "test_feature = pd.DataFrame(test_feature)\n",
    "test_feature.dropna(axis=1, thresh=2, inplace=True)\n",
    "#test_feature.dropna(how='all',thresh = 20,inplace=True)\n",
    "test_feature = np.array(test_feature)\n",
    "wh_inf = np.isinf(test_feature)\n",
    "test_feature[wh_inf]=0\n",
    "wh_nan = np.isnan(test_feature)\n",
    "test_feature[wh_nan]=0\n",
    "\n",
    "#only use image features\n",
    "X_train = train_feature[:,:6093]\n",
    "Y_train = train_feature[:,6093]\n",
    "Y_train = Y_train.astype('int32')\n",
    "\n",
    "X_validate = validate_feature[:,:6093]\n",
    "Y_validate = validate_feature[:,6093]\n",
    "Y_validate = Y_validate.astype('int32')\n",
    "\n",
    "X_test = test_feature[:,:6093]\n",
    "Y_test = test_feature[:,6093]\n",
    "Y_test = Y_test.astype('int32')\n",
    "seed = 7\n",
    "\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(X_train) \n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(Y_train)\n",
    "\n",
    "print(len(X_train[:,-1]))\n",
    "print(len(Y_train[Y_train==0]))\n",
    "print(Y_validate)\n",
    "print(Y_test)\n",
    "\n",
    "print(len(Y_test[Y_test==1]))\n",
    "print(len(Y_validate[Y_validate==1]))\n",
    "print(len(Y_train[Y_train==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from tpot import TPOTClassifier\n",
    "import tools\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#from pyearth import Earth\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from skfeature.function.statistical_based import t_score\n",
    "from skfeature.function.statistical_based import gini_index\n",
    "from skfeature.function.similarity_based import fisher_score\n",
    "from skfeature.function.similarity_based import reliefF\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from skfeature.function.information_theoretical_based import LCSI\n",
    "from skfeature.function.information_theoretical_based import MIM\n",
    "from skfeature.function.information_theoretical_based import MIFS\n",
    "from skfeature.function.information_theoretical_based import MRMR\n",
    "from skfeature.function.information_theoretical_based import CIFE\n",
    "from skfeature.function.information_theoretical_based import JMI\n",
    "from skfeature.function.information_theoretical_based import CMIM\n",
    "from skfeature.function.information_theoretical_based import ICAP\n",
    "from skfeature.function.information_theoretical_based import DISR\n",
    "\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "# CLASSIFICATION METHODS\n",
    "models = []\n",
    "models.append(('GLM', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "models.append(('BY', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "models.append(('BAG', BaggingClassifier()))\n",
    "models.append(('NNet', MLPClassifier()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('BST', AdaBoostClassifier()))\n",
    "\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC CHSQ GLM: 0.801766 (0.029354)\n",
      "\n",
      "Accuracy: 0.7227272727272728\n",
      "Average Precision Score: 0.6157791663605616\n",
      "Kappa: 0.44038771614433714\n",
      "Hamming Loss: 0.2772727272727273\n",
      "AUC: 0.7205412830412832\n",
      "Sensitivity: 0.6993243243243243\n",
      "Specificity: 0.7417582417582418\n",
      "Accuracy: 0.7831325301204819\n",
      "Average Precision Score: 0.6884908053265694\n",
      "Kappa: 0.564938846825859\n",
      "Hamming Loss: 0.21686746987951808\n",
      "AUC0.7836257309941521\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.7777777777777778\n",
      "Accuracy: 0.6385542168674698\n",
      "Average Precision Score: 0.5503064891143522\n",
      "Kappa: 0.28653295128939826\n",
      "Hamming Loss: 0.3614457831325301\n",
      "AUC: 0.6461988304093568\n",
      "Sensitivity0.7368421052631579\n",
      "Specificity0.5555555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC ANOVA GLM: 0.799216 (0.021344)\n",
      "\n",
      "Accuracy: 0.7348484848484849\n",
      "Average Precision Score: 0.6286900536900537\n",
      "Kappa: 0.4641763625229639\n",
      "Hamming Loss: 0.26515151515151514\n",
      "AUC: 0.7321614196614197\n",
      "Sensitivity: 0.706081081081081\n",
      "Specificity: 0.7582417582417582\n",
      "Accuracy: 0.7710843373493976\n",
      "Average Precision Score: 0.6759101182056161\n",
      "Kappa: 0.5398307557630581\n",
      "Hamming Loss: 0.2289156626506024\n",
      "AUC0.7704678362573099\n",
      "Sensitivity0.7631578947368421\n",
      "Specificity0.7777777777777778\n",
      "Accuracy: 0.7349397590361446\n",
      "Average Precision Score: 0.6346630541303971\n",
      "Kappa: 0.47255921432697867\n",
      "Hamming Loss: 0.26506024096385544\n",
      "AUC: 0.7391812865497076\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.6888888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC TSCR GLM: 0.797334 (0.021606)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7363636363636363\n",
      "Average Precision Score: 0.6301504914004914\n",
      "Kappa: 0.467741935483871\n",
      "Hamming Loss: 0.2636363636363636\n",
      "AUC: 0.7341661716661716\n",
      "Sensitivity: 0.7128378378378378\n",
      "Specificity: 0.7554945054945055\n",
      "Accuracy: 0.7710843373493976\n",
      "Average Precision Score: 0.6759101182056161\n",
      "Kappa: 0.5398307557630581\n",
      "Hamming Loss: 0.2289156626506024\n",
      "AUC0.7704678362573099\n",
      "Sensitivity0.7631578947368421\n",
      "Specificity0.7777777777777778\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.6463256534911577\n",
      "Kappa: 0.49754972614586335\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC: 0.7523391812865498\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.6888888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC FSCR GLM: 0.799216 (0.021344)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7348484848484849\n",
      "Average Precision Score: 0.6286900536900537\n",
      "Kappa: 0.4641763625229639\n",
      "Hamming Loss: 0.26515151515151514\n",
      "AUC: 0.7321614196614197\n",
      "Sensitivity: 0.706081081081081\n",
      "Specificity: 0.7582417582417582\n",
      "Accuracy: 0.7710843373493976\n",
      "Average Precision Score: 0.6759101182056161\n",
      "Kappa: 0.5398307557630581\n",
      "Hamming Loss: 0.2289156626506024\n",
      "AUC0.7704678362573099\n",
      "Sensitivity0.7631578947368421\n",
      "Specificity0.7777777777777778\n",
      "Accuracy: 0.7349397590361446\n",
      "Average Precision Score: 0.6346630541303971\n",
      "Kappa: 0.47255921432697867\n",
      "Hamming Loss: 0.26506024096385544\n",
      "AUC: 0.7391812865497076\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.6888888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC RELF GLM: 0.840573 (0.041726)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7878787878787878\n",
      "Average Precision Score: 0.6893118303832588\n",
      "Kappa: 0.5709350273041347\n",
      "Hamming Loss: 0.21212121212121213\n",
      "AUC: 0.7852873477873478\n",
      "Sensitivity: 0.7601351351351351\n",
      "Specificity: 0.8104395604395604\n",
      "Accuracy: 0.8674698795180723\n",
      "Average Precision Score: 0.7901696645374824\n",
      "Kappa: 0.7346701540249927\n",
      "Hamming Loss: 0.13253012048192772\n",
      "AUC0.8695906432748538\n",
      "Sensitivity0.8947368421052632\n",
      "Specificity0.8444444444444444\n",
      "Accuracy: 0.7710843373493976\n",
      "Average Precision Score: 0.6699834050648282\n",
      "Kappa: 0.5472293999425781\n",
      "Hamming Loss: 0.2289156626506024\n",
      "AUC: 0.7786549707602339\n",
      "Sensitivity0.868421052631579\n",
      "Specificity0.6888888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC CHSQ LDA: 0.787205 (0.028707)\n",
      "\n",
      "Accuracy: 0.7318181818181818\n",
      "Average Precision Score: 0.6253664960186699\n",
      "Kappa: 0.45839437716744247\n",
      "Hamming Loss: 0.2681818181818182\n",
      "AUC: 0.7294141669141669\n",
      "Sensitivity: 0.706081081081081\n",
      "Specificity: 0.7527472527472527\n",
      "Accuracy: 0.7710843373493976\n",
      "Average Precision Score: 0.6806504212338074\n",
      "Kappa: 0.5360400117681672\n",
      "Hamming Loss: 0.2289156626506024\n",
      "AUC0.7663742690058479\n",
      "Sensitivity0.7105263157894737\n",
      "Specificity0.8222222222222222\n",
      "Accuracy: 0.6626506024096386\n",
      "Average Precision Score: 0.568534905170923\n",
      "Kappa: 0.32871172732524556\n",
      "Hamming Loss: 0.3373493975903614\n",
      "AUC: 0.666374269005848\n",
      "Sensitivity0.7105263157894737\n",
      "Specificity0.6222222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC ANOVA LDA: 0.790027 (0.021915)\n",
      "\n",
      "Accuracy: 0.740909090909091\n",
      "Average Precision Score: 0.6353419675453573\n",
      "Kappa: 0.47609321325782195\n",
      "Hamming Loss: 0.2590909090909091\n",
      "AUC: 0.7379714879714878\n",
      "Sensitivity: 0.7094594594594594\n",
      "Specificity: 0.7664835164835165\n",
      "Accuracy: 0.7951807228915663\n",
      "Average Precision Score: 0.7036729915613873\n",
      "Kappa: 0.5882696235774729\n",
      "Hamming Loss: 0.20481927710843373\n",
      "AUC0.7947368421052632\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.8\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.6463256534911577\n",
      "Kappa: 0.49754972614586335\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC: 0.7523391812865498\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.6888888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC TSCR LDA: 0.785947 (0.022415)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7424242424242424\n",
      "Average Precision Score: 0.6369256618412024\n",
      "Kappa: 0.47932135432135436\n",
      "Hamming Loss: 0.25757575757575757\n",
      "AUC: 0.7396606771606771\n",
      "Sensitivity: 0.7128378378378378\n",
      "Specificity: 0.7664835164835165\n",
      "Accuracy: 0.7831325301204819\n",
      "Average Precision Score: 0.6884908053265694\n",
      "Kappa: 0.564938846825859\n",
      "Hamming Loss: 0.21686746987951808\n",
      "AUC0.7836257309941521\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.7777777777777778\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.6227013316423589\n",
      "Kappa: 0.44969731911213606\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC: 0.7280701754385965\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC FSCR LDA: 0.790027 (0.021915)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.740909090909091\n",
      "Average Precision Score: 0.6353419675453573\n",
      "Kappa: 0.47609321325782195\n",
      "Hamming Loss: 0.2590909090909091\n",
      "AUC: 0.7379714879714878\n",
      "Sensitivity: 0.7094594594594594\n",
      "Specificity: 0.7664835164835165\n",
      "Accuracy: 0.7951807228915663\n",
      "Average Precision Score: 0.7036729915613873\n",
      "Kappa: 0.5882696235774729\n",
      "Hamming Loss: 0.20481927710843373\n",
      "AUC0.7947368421052632\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.8\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.6463256534911577\n",
      "Kappa: 0.49754972614586335\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC: 0.7523391812865498\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.6888888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC RELF LDA: 0.836495 (0.038216)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7833333333333333\n",
      "Average Precision Score: 0.6821375121049388\n",
      "Kappa: 0.5635324367820345\n",
      "Hamming Loss: 0.21666666666666667\n",
      "AUC: 0.7827442827442826\n",
      "Sensitivity: 0.777027027027027\n",
      "Specificity: 0.7884615384615384\n",
      "Accuracy: 0.891566265060241\n",
      "Average Precision Score: 0.8282197616376438\n",
      "Kappa: 0.7820250948351327\n",
      "Hamming Loss: 0.10843373493975904\n",
      "AUC0.891812865497076\n",
      "Sensitivity0.8947368421052632\n",
      "Specificity0.8888888888888888\n",
      "Accuracy: 0.7710843373493976\n",
      "Average Precision Score: 0.6699834050648282\n",
      "Kappa: 0.5472293999425781\n",
      "Hamming Loss: 0.2289156626506024\n",
      "AUC: 0.7786549707602339\n",
      "Sensitivity0.868421052631579\n",
      "Specificity0.6888888888888889\n",
      "###########################################\n",
      "TRAIN_AUC CHSQ KNN: 0.732322 (0.038023)\n",
      "\n",
      "Accuracy: 0.7787878787878788\n",
      "Average Precision Score: 0.677677108927109\n",
      "Kappa: 0.553392658509455\n",
      "Hamming Loss: 0.22121212121212122\n",
      "AUC: 0.7770455895455896\n",
      "Sensitivity: 0.7601351351351351\n",
      "Specificity: 0.7939560439560439\n",
      "Accuracy: 0.7108433734939759\n",
      "Average Precision Score: 0.6127223575743417\n",
      "Kappa: 0.4175438596491228\n",
      "Hamming Loss: 0.2891566265060241\n",
      "AUC0.7087719298245614\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.7333333333333333\n",
      "Accuracy: 0.7108433734939759\n",
      "Average Precision Score: 0.6121353836398225\n",
      "Kappa: 0.4199184624344787\n",
      "Hamming Loss: 0.2891566265060241\n",
      "AUC: 0.7108187134502923\n",
      "Sensitivity0.7105263157894737\n",
      "Specificity0.7111111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC ANOVA KNN: 0.745656 (0.020767)\n",
      "\n",
      "Accuracy: 0.793939393939394\n",
      "Average Precision Score: 0.6960303882283747\n",
      "Kappa: 0.5837198085556339\n",
      "Hamming Loss: 0.20606060606060606\n",
      "AUC: 0.7920441045441046\n",
      "Sensitivity: 0.7736486486486487\n",
      "Specificity: 0.8104395604395604\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.6265513180541716\n",
      "Kappa: 0.4383642247719918\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC0.7178362573099415\n",
      "Sensitivity0.6578947368421053\n",
      "Specificity0.7777777777777778\n",
      "Accuracy: 0.7831325301204819\n",
      "Average Precision Score: 0.6884908053265694\n",
      "Kappa: 0.564938846825859\n",
      "Hamming Loss: 0.21686746987951808\n",
      "AUC: 0.7836257309941521\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.7777777777777778\n",
      "###########################################\n",
      "TRAIN_AUC TSCR KNN: 0.746559 (0.015662)\n",
      "\n",
      "Accuracy: 0.7984848484848485\n",
      "Average Precision Score: 0.7014067503197938\n",
      "Kappa: 0.5930308031823155\n",
      "Hamming Loss: 0.2015151515151515\n",
      "AUC: 0.7967961092961093\n",
      "Sensitivity: 0.7804054054054054\n",
      "Specificity: 0.8131868131868132\n",
      "Accuracy: 0.6746987951807228\n",
      "Average Precision Score: 0.5783475295206431\n",
      "Kappa: 0.34339290946381484\n",
      "Hamming Loss: 0.3253012048192771\n",
      "AUC0.671345029239766\n",
      "Sensitivity0.631578947368421\n",
      "Specificity0.7111111111111111\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.6231216174366991\n",
      "Kappa: 0.44746743849493487\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC: 0.7260233918128655\n",
      "Sensitivity0.7631578947368421\n",
      "Specificity0.6888888888888889\n",
      "###########################################\n",
      "TRAIN_AUC FSCR KNN: 0.745656 (0.020767)\n",
      "\n",
      "Accuracy: 0.793939393939394\n",
      "Average Precision Score: 0.6960303882283747\n",
      "Kappa: 0.5837198085556339\n",
      "Hamming Loss: 0.20606060606060606\n",
      "AUC: 0.7920441045441046\n",
      "Sensitivity: 0.7736486486486487\n",
      "Specificity: 0.8104395604395604\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.6265513180541716\n",
      "Kappa: 0.4383642247719918\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC0.7178362573099415\n",
      "Sensitivity0.6578947368421053\n",
      "Specificity0.7777777777777778\n",
      "Accuracy: 0.7831325301204819\n",
      "Average Precision Score: 0.6884908053265694\n",
      "Kappa: 0.564938846825859\n",
      "Hamming Loss: 0.21686746987951808\n",
      "AUC: 0.7836257309941521\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.7777777777777778\n",
      "###########################################\n",
      "TRAIN_AUC RELF KNN: 0.773743 (0.054591)\n",
      "\n",
      "Accuracy: 0.8287878787878787\n",
      "Average Precision Score: 0.7373949412784364\n",
      "Kappa: 0.6553158449337253\n",
      "Hamming Loss: 0.1712121212121212\n",
      "AUC: 0.829002079002079\n",
      "Sensitivity: 0.831081081081081\n",
      "Specificity: 0.8269230769230769\n",
      "Accuracy: 0.7590361445783133\n",
      "Average Precision Score: 0.663418215799486\n",
      "Kappa: 0.5146198830409356\n",
      "Hamming Loss: 0.24096385542168675\n",
      "AUC0.7573099415204678\n",
      "Sensitivity0.7368421052631579\n",
      "Specificity0.7777777777777778\n",
      "Accuracy: 0.7951807228915663\n",
      "Average Precision Score: 0.6954492100541023\n",
      "Kappa: 0.5948894631065174\n",
      "Hamming Loss: 0.20481927710843373\n",
      "AUC: 0.8029239766081873\n",
      "Sensitivity0.8947368421052632\n",
      "Specificity0.7111111111111111\n",
      "###########################################\n",
      "TRAIN_AUC CHSQ DT: 0.642272 (0.032754)\n",
      "\n",
      "Accuracy: 1.0\n",
      "Average Precision Score: 1.0\n",
      "Kappa: 1.0\n",
      "Hamming Loss: 0.0\n",
      "AUC: 1.0\n",
      "Sensitivity: 1.0\n",
      "Specificity: 1.0\n",
      "Accuracy: 0.7349397590361446\n",
      "Average Precision Score: 0.6423589093214965\n",
      "Kappa: 0.4594434576672587\n",
      "Hamming Loss: 0.26506024096385544\n",
      "AUC0.7269005847953216\n",
      "Sensitivity0.631578947368421\n",
      "Specificity0.8222222222222222\n",
      "Accuracy: 0.6385542168674698\n",
      "Average Precision Score: 0.5482305160491591\n",
      "Kappa: 0.2778422273781902\n",
      "Hamming Loss: 0.3614457831325301\n",
      "AUC: 0.6400584795321638\n",
      "Sensitivity0.6578947368421053\n",
      "Specificity0.6222222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC ANOVA DT: 0.576475 (0.070625)\n",
      "\n",
      "Accuracy: 1.0\n",
      "Average Precision Score: 1.0\n",
      "Kappa: 1.0\n",
      "Hamming Loss: 0.0\n",
      "AUC: 1.0\n",
      "Sensitivity: 1.0\n",
      "Specificity: 1.0\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.600718664130205\n",
      "Kappa: 0.39451415231981324\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC0.6976608187134503\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.7111111111111111\n",
      "Accuracy: 0.6265060240963856\n",
      "Average Precision Score: 0.5398999506799126\n",
      "Kappa: 0.2582876909772268\n",
      "Hamming Loss: 0.37349397590361444\n",
      "AUC: 0.6309941520467836\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.5777777777777777\n",
      "###########################################\n",
      "TRAIN_AUC TSCR DT: 0.592429 (0.028024)\n",
      "\n",
      "Accuracy: 1.0\n",
      "Average Precision Score: 1.0\n",
      "Kappa: 1.0\n",
      "Hamming Loss: 0.0\n",
      "AUC: 1.0\n",
      "Sensitivity: 1.0\n",
      "Specificity: 1.0\n",
      "Accuracy: 0.7349397590361446\n",
      "Average Precision Score: 0.6373777659112906\n",
      "Kappa: 0.4660818713450292\n",
      "Hamming Loss: 0.26506024096385544\n",
      "AUC0.7330409356725145\n",
      "Sensitivity0.7105263157894737\n",
      "Specificity0.7555555555555555\n",
      "Accuracy: 0.5662650602409639\n",
      "Average Precision Score: 0.49495085605580214\n",
      "Kappa: 0.12987769365171808\n",
      "Hamming Loss: 0.43373493975903615\n",
      "AUC: 0.5652046783625732\n",
      "Sensitivity0.5526315789473685\n",
      "Specificity0.5777777777777777\n",
      "###########################################\n",
      "TRAIN_AUC FSCR DT: 0.598254 (0.075993)\n",
      "\n",
      "Accuracy: 1.0\n",
      "Average Precision Score: 1.0\n",
      "Kappa: 1.0\n",
      "Hamming Loss: 0.0\n",
      "AUC: 1.0\n",
      "Sensitivity: 1.0\n",
      "Specificity: 1.0\n",
      "Accuracy: 0.7108433734939759\n",
      "Average Precision Score: 0.6144951322317133\n",
      "Kappa: 0.41273584905660377\n",
      "Hamming Loss: 0.2891566265060241\n",
      "AUC0.7046783625730995\n",
      "Sensitivity0.631578947368421\n",
      "Specificity0.7777777777777778\n",
      "Accuracy: 0.6626506024096386\n",
      "Average Precision Score: 0.5681372104961199\n",
      "Kappa: 0.3259860788863108\n",
      "Hamming Loss: 0.3373493975903614\n",
      "AUC: 0.6643274853801169\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.6444444444444445\n",
      "###########################################\n",
      "TRAIN_AUC RELF DT: 0.709078 (0.029645)\n",
      "\n",
      "Accuracy: 1.0\n",
      "Average Precision Score: 1.0\n",
      "Kappa: 1.0\n",
      "Hamming Loss: 0.0\n",
      "AUC: 1.0\n",
      "Sensitivity: 1.0\n",
      "Specificity: 1.0\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.601757405562098\n",
      "Kappa: 0.3895263312739041\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC0.6935672514619883\n",
      "Sensitivity0.631578947368421\n",
      "Specificity0.7555555555555555\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.600718664130205\n",
      "Kappa: 0.39451415231981324\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC: 0.6976608187134503\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.7111111111111111\n",
      "###########################################\n",
      "TRAIN_AUC CHSQ BY: 0.790885 (0.040433)\n",
      "\n",
      "Accuracy: 0.7151515151515152\n",
      "Average Precision Score: 0.6079846557787735\n",
      "Kappa: 0.4380231167795935\n",
      "Hamming Loss: 0.28484848484848485\n",
      "AUC: 0.7244022869022868\n",
      "Sensitivity: 0.8141891891891891\n",
      "Specificity: 0.6346153846153846\n",
      "Accuracy: 0.7710843373493976\n",
      "Average Precision Score: 0.669030579892071\n",
      "Kappa: 0.5490420360308836\n",
      "Hamming Loss: 0.2289156626506024\n",
      "AUC0.780701754385965\n",
      "Sensitivity0.8947368421052632\n",
      "Specificity0.6666666666666666\n",
      "Accuracy: 0.6626506024096386\n",
      "Average Precision Score: 0.5719890841561736\n",
      "Kappa: 0.3446136491821772\n",
      "Hamming Loss: 0.3373493975903614\n",
      "AUC: 0.678654970760234\n",
      "Sensitivity0.868421052631579\n",
      "Specificity0.4888888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC ANOVA BY: 0.800655 (0.023948)\n",
      "\n",
      "Accuracy: 0.7242424242424242\n",
      "Average Precision Score: 0.6165063765063764\n",
      "Kappa: 0.4519072823507939\n",
      "Hamming Loss: 0.27575757575757576\n",
      "AUC: 0.7298039798039799\n",
      "Sensitivity: 0.7837837837837838\n",
      "Specificity: 0.6758241758241759\n",
      "Accuracy: 0.7951807228915663\n",
      "Average Precision Score: 0.6954492100541023\n",
      "Kappa: 0.5948894631065174\n",
      "Hamming Loss: 0.20481927710843373\n",
      "AUC0.8029239766081873\n",
      "Sensitivity0.8947368421052632\n",
      "Specificity0.7111111111111111\n",
      "Accuracy: 0.6867469879518072\n",
      "Average Precision Score: 0.5905077801082874\n",
      "Kappa: 0.38658328595793057\n",
      "Hamming Loss: 0.3132530120481928\n",
      "AUC: 0.6988304093567251\n",
      "Sensitivity0.8421052631578947\n",
      "Specificity0.5555555555555556\n",
      "###########################################\n",
      "TRAIN_AUC TSCR BY: 0.796557 (0.023612)\n",
      "\n",
      "Accuracy: 0.7181818181818181\n",
      "Average Precision Score: 0.6107336099139378\n",
      "Kappa: 0.44262830990519775\n",
      "Hamming Loss: 0.2818181818181818\n",
      "AUC: 0.7262028512028511\n",
      "Sensitivity: 0.8040540540540541\n",
      "Specificity: 0.6483516483516484\n",
      "Accuracy: 0.7831325301204819\n",
      "Average Precision Score: 0.6819647009088988\n",
      "Kappa: 0.571919770773639\n",
      "Hamming Loss: 0.21686746987951808\n",
      "AUC0.7918128654970761\n",
      "Sensitivity0.8947368421052632\n",
      "Specificity0.6888888888888889\n",
      "Accuracy: 0.6867469879518072\n",
      "Average Precision Score: 0.5905077801082874\n",
      "Kappa: 0.38658328595793057\n",
      "Hamming Loss: 0.3132530120481928\n",
      "AUC: 0.6988304093567251\n",
      "Sensitivity0.8421052631578947\n",
      "Specificity0.5555555555555556\n",
      "###########################################\n",
      "TRAIN_AUC FSCR BY: 0.800655 (0.023948)\n",
      "\n",
      "Accuracy: 0.7242424242424242\n",
      "Average Precision Score: 0.6165063765063764\n",
      "Kappa: 0.4519072823507939\n",
      "Hamming Loss: 0.27575757575757576\n",
      "AUC: 0.7298039798039799\n",
      "Sensitivity: 0.7837837837837838\n",
      "Specificity: 0.6758241758241759\n",
      "Accuracy: 0.7951807228915663\n",
      "Average Precision Score: 0.6954492100541023\n",
      "Kappa: 0.5948894631065174\n",
      "Hamming Loss: 0.20481927710843373\n",
      "AUC0.8029239766081873\n",
      "Sensitivity0.8947368421052632\n",
      "Specificity0.7111111111111111\n",
      "Accuracy: 0.6867469879518072\n",
      "Average Precision Score: 0.5905077801082874\n",
      "Kappa: 0.38658328595793057\n",
      "Hamming Loss: 0.3132530120481928\n",
      "AUC: 0.6988304093567251\n",
      "Sensitivity0.8421052631578947\n",
      "Specificity0.5555555555555556\n",
      "###########################################\n",
      "TRAIN_AUC RELF BY: 0.774311 (0.036002)\n",
      "\n",
      "Accuracy: 0.6803030303030303\n",
      "Average Precision Score: 0.5797734781789451\n",
      "Kappa: 0.381649290446335\n",
      "Hamming Loss: 0.3196969696969697\n",
      "AUC: 0.6994356994356994\n",
      "Sensitivity: 0.8851351351351351\n",
      "Specificity: 0.5137362637362637\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.6009559588901784\n",
      "Kappa: 0.4113475177304965\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC0.7119883040935673\n",
      "Sensitivity0.868421052631579\n",
      "Specificity0.5555555555555556\n",
      "Accuracy: 0.7108433734939759\n",
      "Average Precision Score: 0.6121181641265607\n",
      "Kappa: 0.44044943820224725\n",
      "Hamming Loss: 0.2891566265060241\n",
      "AUC: 0.7292397660818712\n",
      "Sensitivity0.9473684210526315\n",
      "Specificity0.5111111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC CHSQ SVM: 0.800527 (0.030110)\n",
      "\n",
      "Accuracy: 0.7227272727272728\n",
      "Average Precision Score: 0.6159136409136409\n",
      "Kappa: 0.43968156766687083\n",
      "Hamming Loss: 0.2772727272727273\n",
      "AUC: 0.7199101574101574\n",
      "Sensitivity: 0.6925675675675675\n",
      "Specificity: 0.7472527472527473\n",
      "Accuracy: 0.7831325301204819\n",
      "Average Precision Score: 0.6967716065500391\n",
      "Kappa: 0.5595518867924527\n",
      "Hamming Loss: 0.21686746987951808\n",
      "AUC0.777485380116959\n",
      "Sensitivity0.7105263157894737\n",
      "Specificity0.8444444444444444\n",
      "Accuracy: 0.6385542168674698\n",
      "Average Precision Score: 0.5495781754018362\n",
      "Kappa: 0.2836593785960875\n",
      "Hamming Loss: 0.3614457831325301\n",
      "AUC: 0.6441520467836257\n",
      "Sensitivity0.7105263157894737\n",
      "Specificity0.5777777777777777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC ANOVA SVM: 0.799424 (0.024473)\n",
      "\n",
      "Accuracy: 0.7318181818181818\n",
      "Average Precision Score: 0.6254618254618255\n",
      "Kappa: 0.4580526638089406\n",
      "Hamming Loss: 0.2681818181818182\n",
      "AUC: 0.729098604098604\n",
      "Sensitivity: 0.7027027027027027\n",
      "Specificity: 0.7554945054945055\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.6471811358039257\n",
      "Kappa: 0.49551374819102756\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC0.7502923976608188\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.7111111111111111\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.6463256534911577\n",
      "Kappa: 0.49754972614586335\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC: 0.7523391812865498\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.6888888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC TSCR SVM: 0.795561 (0.023213)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7303030303030303\n",
      "Average Precision Score: 0.6237213349713349\n",
      "Kappa: 0.4555061179087876\n",
      "Hamming Loss: 0.2696969696969697\n",
      "AUC: 0.7280405405405406\n",
      "Sensitivity: 0.706081081081081\n",
      "Specificity: 0.75\n",
      "Accuracy: 0.7349397590361446\n",
      "Average Precision Score: 0.6373777659112906\n",
      "Kappa: 0.4660818713450292\n",
      "Hamming Loss: 0.26506024096385544\n",
      "AUC0.7330409356725145\n",
      "Sensitivity0.7105263157894737\n",
      "Specificity0.7555555555555555\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.6463256534911577\n",
      "Kappa: 0.49754972614586335\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC: 0.7523391812865498\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.6888888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC FSCR SVM: 0.799424 (0.024473)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7318181818181818\n",
      "Average Precision Score: 0.6254618254618255\n",
      "Kappa: 0.4580526638089406\n",
      "Hamming Loss: 0.2681818181818182\n",
      "AUC: 0.729098604098604\n",
      "Sensitivity: 0.7027027027027027\n",
      "Specificity: 0.7554945054945055\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.6471811358039257\n",
      "Kappa: 0.49551374819102756\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC0.7502923976608188\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.7111111111111111\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.6463256534911577\n",
      "Kappa: 0.49754972614586335\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC: 0.7523391812865498\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.6888888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC RELF SVM: 0.814754 (0.049154)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.759090909090909\n",
      "Average Precision Score: 0.6541504380732677\n",
      "Kappa: 0.515306593751732\n",
      "Hamming Loss: 0.2409090909090909\n",
      "AUC: 0.7588728838728839\n",
      "Sensitivity: 0.7567567567567568\n",
      "Specificity: 0.760989010989011\n",
      "Accuracy: 0.8433734939759037\n",
      "Average Precision Score: 0.7632473212688812\n",
      "Kappa: 0.6851473592063029\n",
      "Hamming Loss: 0.1566265060240964\n",
      "AUC0.8432748538011696\n",
      "Sensitivity0.8421052631578947\n",
      "Specificity0.8444444444444444\n",
      "Accuracy: 0.7710843373493976\n",
      "Average Precision Score: 0.669030579892071\n",
      "Kappa: 0.5490420360308836\n",
      "Hamming Loss: 0.2289156626506024\n",
      "AUC: 0.780701754385965\n",
      "Sensitivity0.8947368421052632\n",
      "Specificity0.6666666666666666\n",
      "###########################################\n",
      "TRAIN_AUC CHSQ BAG: 0.752576 (0.019959)\n",
      "\n",
      "Accuracy: 0.9803030303030303\n",
      "Average Precision Score: 0.9713378052478397\n",
      "Kappa: 0.9600952504976467\n",
      "Hamming Loss: 0.019696969696969695\n",
      "AUC: 0.978987228987229\n",
      "Sensitivity: 0.9662162162162162\n",
      "Specificity: 0.9917582417582418\n",
      "Accuracy: 0.6385542168674698\n",
      "Average Precision Score: 0.545582329317269\n",
      "Kappa: 0.25980975029726516\n",
      "Hamming Loss: 0.3614457831325301\n",
      "AUC0.6277777777777778\n",
      "Sensitivity0.5\n",
      "Specificity0.7555555555555555\n",
      "Accuracy: 0.7349397590361446\n",
      "Average Precision Score: 0.6423589093214965\n",
      "Kappa: 0.4594434576672587\n",
      "Hamming Loss: 0.26506024096385544\n",
      "AUC: 0.7269005847953216\n",
      "Sensitivity0.631578947368421\n",
      "Specificity0.8222222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC ANOVA BAG: 0.712049 (0.031754)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9833333333333333\n",
      "Average Precision Score: 0.9780011243425878\n",
      "Kappa: 0.966213071725088\n",
      "Hamming Loss: 0.016666666666666666\n",
      "AUC: 0.9817344817344817\n",
      "Sensitivity: 0.9662162162162162\n",
      "Specificity: 0.9972527472527473\n",
      "Accuracy: 0.6626506024096386\n",
      "Average Precision Score: 0.5674187979990135\n",
      "Kappa: 0.31767469172049323\n",
      "Hamming Loss: 0.3373493975903614\n",
      "AUC0.658187134502924\n",
      "Sensitivity0.6052631578947368\n",
      "Specificity0.7111111111111111\n",
      "Accuracy: 0.6385542168674698\n",
      "Average Precision Score: 0.5465722539279927\n",
      "Kappa: 0.2689371697005285\n",
      "Hamming Loss: 0.3614457831325301\n",
      "AUC: 0.6339181286549708\n",
      "Sensitivity0.5789473684210527\n",
      "Specificity0.6888888888888889\n",
      "###########################################\n",
      "TRAIN_AUC TSCR BAG: 0.721430 (0.037734)\n",
      "\n",
      "Accuracy: 0.9818181818181818\n",
      "Average Precision Score: 0.9761379386379386\n",
      "Kappa: 0.9631298648095044\n",
      "Hamming Loss: 0.01818181818181818\n",
      "AUC: 0.9800452925452925\n",
      "Sensitivity: 0.9628378378378378\n",
      "Specificity: 0.9972527472527473\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.600718664130205\n",
      "Kappa: 0.39451415231981324\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC0.6976608187134503\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.7111111111111111\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.600718664130205\n",
      "Kappa: 0.39451415231981324\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC: 0.6976608187134503\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.7111111111111111\n",
      "###########################################\n",
      "TRAIN_AUC FSCR BAG: 0.714005 (0.028808)\n",
      "\n",
      "Accuracy: 0.9833333333333333\n",
      "Average Precision Score: 0.9780011243425878\n",
      "Kappa: 0.966213071725088\n",
      "Hamming Loss: 0.016666666666666666\n",
      "AUC: 0.9817344817344817\n",
      "Sensitivity: 0.9662162162162162\n",
      "Specificity: 0.9972527472527473\n",
      "Accuracy: 0.7951807228915663\n",
      "Average Precision Score: 0.7099556119213697\n",
      "Kappa: 0.5848779052662547\n",
      "Hamming Loss: 0.20481927710843373\n",
      "AUC0.7906432748538013\n",
      "Sensitivity0.7368421052631579\n",
      "Specificity0.8444444444444444\n",
      "Accuracy: 0.7710843373493976\n",
      "Average Precision Score: 0.6759101182056161\n",
      "Kappa: 0.5398307557630581\n",
      "Hamming Loss: 0.2289156626506024\n",
      "AUC: 0.7704678362573099\n",
      "Sensitivity0.7631578947368421\n",
      "Specificity0.7777777777777778\n",
      "###########################################\n",
      "TRAIN_AUC RELF BAG: 0.823581 (0.038688)\n",
      "\n",
      "Accuracy: 0.9772727272727273\n",
      "Average Precision Score: 0.9705483832692313\n",
      "Kappa: 0.9538685206240332\n",
      "Hamming Loss: 0.022727272727272728\n",
      "AUC: 0.9749777249777251\n",
      "Sensitivity: 0.9527027027027027\n",
      "Specificity: 0.9972527472527473\n",
      "Accuracy: 0.8313253012048193\n",
      "Average Precision Score: 0.7498498147715515\n",
      "Kappa: 0.660233918128655\n",
      "Hamming Loss: 0.1686746987951807\n",
      "AUC0.8301169590643276\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.8444444444444444\n",
      "Accuracy: 0.8433734939759037\n",
      "Average Precision Score: 0.7632473212688812\n",
      "Kappa: 0.6851473592063029\n",
      "Hamming Loss: 0.1566265060240964\n",
      "AUC: 0.8432748538011696\n",
      "Sensitivity0.8421052631578947\n",
      "Specificity0.8444444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC CHSQ NNet: 0.801861 (0.027765)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7378787878787879\n",
      "Average Precision Score: 0.6309670151941444\n",
      "Kappa: 0.473621125228199\n",
      "Hamming Loss: 0.26212121212121214\n",
      "AUC: 0.7383798633798634\n",
      "Sensitivity: 0.7432432432432432\n",
      "Specificity: 0.7335164835164835\n",
      "Accuracy: 0.7831325301204819\n",
      "Average Precision Score: 0.6884908053265694\n",
      "Kappa: 0.564938846825859\n",
      "Hamming Loss: 0.21686746987951808\n",
      "AUC0.7836257309941521\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.7777777777777778\n",
      "Accuracy: 0.6626506024096386\n",
      "Average Precision Score: 0.5695082963432678\n",
      "Kappa: 0.33409742120343844\n",
      "Hamming Loss: 0.3373493975903614\n",
      "AUC: 0.6704678362573099\n",
      "Sensitivity0.7631578947368421\n",
      "Specificity0.5777777777777777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC ANOVA NNet: 0.801419 (0.020823)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7363636363636363\n",
      "Average Precision Score: 0.6298509739686211\n",
      "Kappa: 0.4687465304762962\n",
      "Hamming Loss: 0.2636363636363636\n",
      "AUC: 0.7351128601128601\n",
      "Sensitivity: 0.722972972972973\n",
      "Specificity: 0.7472527472527473\n",
      "Accuracy: 0.7831325301204819\n",
      "Average Precision Score: 0.6864676752121268\n",
      "Kappa: 0.5667053364269141\n",
      "Hamming Loss: 0.21686746987951808\n",
      "AUC0.7856725146198831\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.7555555555555555\n",
      "Accuracy: 0.7349397590361446\n",
      "Average Precision Score: 0.6341085164456454\n",
      "Kappa: 0.47468354430379744\n",
      "Hamming Loss: 0.26506024096385544\n",
      "AUC: 0.7412280701754387\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC TSCR NNet: 0.797784 (0.020465)\n",
      "\n",
      "Accuracy: 0.7287878787878788\n",
      "Average Precision Score: 0.6215982215982216\n",
      "Kappa: 0.4550235261555494\n",
      "Hamming Loss: 0.27121212121212124\n",
      "AUC: 0.728875853875854\n",
      "Sensitivity: 0.7297297297297297\n",
      "Specificity: 0.728021978021978\n",
      "Accuracy: 0.7710843373493976\n",
      "Average Precision Score: 0.6724646443792305\n",
      "Kappa: 0.5435600578871201\n",
      "Hamming Loss: 0.2289156626506024\n",
      "AUC0.774561403508772\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.7333333333333333\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.6224112575722824\n",
      "Kappa: 0.4519092736146999\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC: 0.7301169590643274\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.6444444444444445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC FSCR NNet: 0.802004 (0.021671)\n",
      "\n",
      "Accuracy: 0.7287878787878788\n",
      "Average Precision Score: 0.6215406903892708\n",
      "Kappa: 0.455365210496229\n",
      "Hamming Loss: 0.27121212121212124\n",
      "AUC: 0.7291914166914166\n",
      "Sensitivity: 0.7331081081081081\n",
      "Specificity: 0.7252747252747253\n",
      "Accuracy: 0.7831325301204819\n",
      "Average Precision Score: 0.6847293480140658\n",
      "Kappa: 0.5684575389948008\n",
      "Hamming Loss: 0.21686746987951808\n",
      "AUC0.787719298245614\n",
      "Sensitivity0.8421052631578947\n",
      "Specificity0.7333333333333333\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.6004490572386216\n",
      "Kappa: 0.4066342579353732\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC: 0.7078947368421054\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC RELF NNet: 0.853115 (0.032895)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8015151515151515\n",
      "Average Precision Score: 0.7061756994350852\n",
      "Kappa: 0.5983909626354025\n",
      "Hamming Loss: 0.1984848484848485\n",
      "AUC: 0.7989122364122364\n",
      "Sensitivity: 0.7736486486486487\n",
      "Specificity: 0.8241758241758241\n",
      "Accuracy: 0.8674698795180723\n",
      "Average Precision Score: 0.7950587776206038\n",
      "Kappa: 0.7335862270207179\n",
      "Hamming Loss: 0.13253012048192772\n",
      "AUC0.8675438596491228\n",
      "Sensitivity0.868421052631579\n",
      "Specificity0.8666666666666667\n",
      "Accuracy: 0.7831325301204819\n",
      "Average Precision Score: 0.6864676752121268\n",
      "Kappa: 0.5667053364269141\n",
      "Hamming Loss: 0.21686746987951808\n",
      "AUC: 0.7856725146198831\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.7555555555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC CHSQ RF: 0.742140 (0.038085)\n",
      "\n",
      "Accuracy: 0.9772727272727273\n",
      "Average Precision Score: 0.966178029240313\n",
      "Kappa: 0.9539560582665153\n",
      "Hamming Loss: 0.022727272727272728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9759244134244134\n",
      "Sensitivity: 0.9628378378378378\n",
      "Specificity: 0.989010989010989\n",
      "Accuracy: 0.7108433734939759\n",
      "Average Precision Score: 0.6192929613189601\n",
      "Kappa: 0.4053731343283582\n",
      "Hamming Loss: 0.2891566265060241\n",
      "AUC0.6985380116959066\n",
      "Sensitivity0.5526315789473685\n",
      "Specificity0.8444444444444444\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.625374899312756\n",
      "Kappa: 0.44066803398769405\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC: 0.7198830409356725\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.7555555555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC ANOVA RF: 0.726518 (0.033148)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9818181818181818\n",
      "Average Precision Score: 0.9732006693213591\n",
      "Kappa: 0.9631764924679189\n",
      "Hamming Loss: 0.01818181818181818\n",
      "AUC: 0.9806764181764183\n",
      "Sensitivity: 0.9695945945945946\n",
      "Specificity: 0.9917582417582418\n",
      "Accuracy: 0.6385542168674698\n",
      "Average Precision Score: 0.546150546458279\n",
      "Kappa: 0.2659198113207547\n",
      "Hamming Loss: 0.3614457831325301\n",
      "AUC0.6318713450292399\n",
      "Sensitivity0.5526315789473685\n",
      "Specificity0.7111111111111111\n",
      "Accuracy: 0.7349397590361446\n",
      "Average Precision Score: 0.6423589093214965\n",
      "Kappa: 0.4594434576672587\n",
      "Hamming Loss: 0.26506024096385544\n",
      "AUC: 0.7269005847953216\n",
      "Sensitivity0.631578947368421\n",
      "Specificity0.8222222222222222\n",
      "###########################################\n",
      "TRAIN_AUC TSCR RF: 0.706374 (0.020494)\n",
      "\n",
      "Accuracy: 0.9863636363636363\n",
      "Average Precision Score: 0.9802470933914232\n",
      "Kappa: 0.9723910982207596\n",
      "Hamming Loss: 0.013636363636363636\n",
      "AUC: 0.985428422928423\n",
      "Sensitivity: 0.9763513513513513\n",
      "Specificity: 0.9945054945054945\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.6280048423358506\n",
      "Kappa: 0.43604135893648455\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC0.7157894736842105\n",
      "Sensitivity0.631578947368421\n",
      "Specificity0.8\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.6297891054881666\n",
      "Kappa: 0.43369919905072685\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC: 0.7137426900584795\n",
      "Sensitivity0.6052631578947368\n",
      "Specificity0.8222222222222222\n",
      "###########################################\n",
      "TRAIN_AUC FSCR RF: 0.733123 (0.048481)\n",
      "\n",
      "Accuracy: 0.9848484848484849\n",
      "Average Precision Score: 0.9769264049229802\n",
      "Kappa: 0.9693331350828934\n",
      "Hamming Loss: 0.015151515151515152\n",
      "AUC: 0.9840547965547966\n",
      "Sensitivity: 0.9763513513513513\n",
      "Specificity: 0.9917582417582418\n",
      "Accuracy: 0.7349397590361446\n",
      "Average Precision Score: 0.6447579792855633\n",
      "Kappa: 0.4571938168846611\n",
      "Hamming Loss: 0.26506024096385544\n",
      "AUC0.7248538011695905\n",
      "Sensitivity0.6052631578947368\n",
      "Specificity0.8444444444444444\n",
      "Accuracy: 0.6506024096385542\n",
      "Average Precision Score: 0.5566808587734396\n",
      "Kappa: 0.2918505442777287\n",
      "Hamming Loss: 0.3493975903614458\n",
      "AUC: 0.645029239766082\n",
      "Sensitivity0.5789473684210527\n",
      "Specificity0.7111111111111111\n",
      "###########################################\n",
      "TRAIN_AUC RELF RF: 0.802050 (0.019306)\n",
      "\n",
      "Accuracy: 0.9863636363636363\n",
      "Average Precision Score: 0.9787892764001979\n",
      "Kappa: 0.9724085394176994\n",
      "Hamming Loss: 0.013636363636363636\n",
      "AUC: 0.9857439857439858\n",
      "Sensitivity: 0.9797297297297297\n",
      "Specificity: 0.9917582417582418\n",
      "Accuracy: 0.7951807228915663\n",
      "Average Precision Score: 0.7099556119213697\n",
      "Kappa: 0.5848779052662547\n",
      "Hamming Loss: 0.20481927710843373\n",
      "AUC0.7906432748538013\n",
      "Sensitivity0.7368421052631579\n",
      "Specificity0.8444444444444444\n",
      "Accuracy: 0.8192771084337349\n",
      "Average Precision Score: 0.7364993401772095\n",
      "Kappa: 0.6352182830354527\n",
      "Hamming Loss: 0.18072289156626506\n",
      "AUC: 0.8169590643274853\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.8444444444444444\n",
      "###########################################\n",
      "TRAIN_AUC CHSQ BST: 0.744416 (0.045050)\n",
      "\n",
      "Accuracy: 0.7878787878787878\n",
      "Average Precision Score: 0.6881560142321732\n",
      "Kappa: 0.5720161559269278\n",
      "Hamming Loss: 0.21212121212121213\n",
      "AUC: 0.7865495990495991\n",
      "Sensitivity: 0.7736486486486487\n",
      "Specificity: 0.7994505494505495\n",
      "Accuracy: 0.7349397590361446\n",
      "Average Precision Score: 0.6373777659112906\n",
      "Kappa: 0.4660818713450292\n",
      "Hamming Loss: 0.26506024096385544\n",
      "AUC0.7330409356725145\n",
      "Sensitivity0.7105263157894737\n",
      "Specificity0.7555555555555555\n",
      "Accuracy: 0.7349397590361446\n",
      "Average Precision Score: 0.6362714013950539\n",
      "Kappa: 0.4682585905649388\n",
      "Hamming Loss: 0.26506024096385544\n",
      "AUC: 0.7350877192982457\n",
      "Sensitivity0.7368421052631579\n",
      "Specificity0.7333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "TRAIN_AUC ANOVA BST: 0.739105 (0.039729)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: UserWarning: Features [51 54 57] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7954545454545454\n",
      "Average Precision Score: 0.6980321230321231\n",
      "Kappa: 0.5866503368034293\n",
      "Hamming Loss: 0.20454545454545456\n",
      "AUC: 0.7934177309177309\n",
      "Sensitivity: 0.7736486486486487\n",
      "Specificity: 0.8131868131868132\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.625374899312756\n",
      "Kappa: 0.44066803398769405\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC0.7198830409356725\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.7555555555555555\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.600437694294508\n",
      "Kappa: 0.39697762278407445\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC: 0.6997076023391813\n",
      "Sensitivity0.7105263157894737\n",
      "Specificity0.6888888888888889\n",
      "###########################################\n",
      "TRAIN_AUC TSCR BST: 0.746855 (0.030699)\n",
      "\n",
      "Accuracy: 0.8045454545454546\n",
      "Average Precision Score: 0.7127687071235458\n",
      "Kappa: 0.6027658025866414\n",
      "Hamming Loss: 0.19545454545454546\n",
      "AUC: 0.7997661122661123\n",
      "Sensitivity: 0.7533783783783784\n",
      "Specificity: 0.8461538461538461\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.625374899312756\n",
      "Kappa: 0.44066803398769405\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC0.7198830409356725\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.7555555555555555\n",
      "Accuracy: 0.7108433734939759\n",
      "Average Precision Score: 0.6127223575743417\n",
      "Kappa: 0.4175438596491228\n",
      "Hamming Loss: 0.2891566265060241\n",
      "AUC: 0.7087719298245614\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.7333333333333333\n",
      "###########################################\n",
      "TRAIN_AUC FSCR BST: 0.740266 (0.039098)\n",
      "\n",
      "Accuracy: 0.7954545454545454\n",
      "Average Precision Score: 0.6980321230321231\n",
      "Kappa: 0.5866503368034293\n",
      "Hamming Loss: 0.20454545454545456\n",
      "AUC: 0.7934177309177309\n",
      "Sensitivity: 0.7736486486486487\n",
      "Specificity: 0.8131868131868132\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.625374899312756\n",
      "Kappa: 0.44066803398769405\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC0.7198830409356725\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.7555555555555555\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.600437694294508\n",
      "Kappa: 0.39697762278407445\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC: 0.6997076023391813\n",
      "Sensitivity0.7105263157894737\n",
      "Specificity0.6888888888888889\n",
      "###########################################\n",
      "TRAIN_AUC RELF BST: 0.810280 (0.026411)\n",
      "\n",
      "Accuracy: 0.8636363636363636\n",
      "Average Precision Score: 0.7853564280716598\n",
      "Kappa: 0.7248675288101678\n",
      "Hamming Loss: 0.13636363636363635\n",
      "AUC: 0.8631199881199881\n",
      "Sensitivity: 0.8581081081081081\n",
      "Specificity: 0.8681318681318682\n",
      "Accuracy: 0.8433734939759037\n",
      "Average Precision Score: 0.7790876424357719\n",
      "Kappa: 0.6812407680945347\n",
      "Hamming Loss: 0.1566265060240964\n",
      "AUC0.8371345029239766\n",
      "Sensitivity0.7631578947368421\n",
      "Specificity0.9111111111111111\n",
      "Accuracy: 0.8433734939759037\n",
      "Average Precision Score: 0.7730772714919829\n",
      "Kappa: 0.6825536922624301\n",
      "Hamming Loss: 0.1566265060240964\n",
      "AUC: 0.8391812865497076\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "#crossvalidation on trainset and select the best model on validation set, test on test set\n",
    "import xlwt\n",
    "\n",
    "num_fea = 20\n",
    "sel = []\n",
    "sel.append(('CHSQ', SelectKBest(chi2, k=num_fea)))\n",
    "sel.append(('ANOVA', SelectKBest(f_classif, k=num_fea)))\n",
    "sel.append(('TSCR', SelectKBest(t_score.t_score, k=num_fea)))\n",
    "sel.append(('FSCR', SelectKBest(fisher_score.fisher_score, k=num_fea)))\n",
    "sel.append(('RELF', SelectKBest(reliefF.reliefF, k=num_fea)))\n",
    "\n",
    "output = open(\"endohand20.txt\",\"w\")\n",
    "file = 'fea20.xls'\n",
    "\n",
    "\n",
    "book = xlwt.Workbook()\n",
    "sheet = book.add_sheet('train_avg_auc')\n",
    "sheet_train = book.add_sheet('train_auc')\n",
    "sheet_validate = book.add_sheet('validate_auc')\n",
    "sheet_test = book.add_sheet('test_auc')\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "# SMOTErandom_state\n",
    "kfold = model_selection.KFold(n_splits=5, random_state=seed)\n",
    "r = 0\n",
    "c = 0\n",
    "for name, model in models:\n",
    "    for kind, selection in sel:\n",
    "        pipe = make_pipeline(MinMaxScaler(), selection, model)\n",
    "        cv_results = model_selection.cross_val_score(pipe, X_train, Y_train, scoring='roc_auc', cv=kfold)\n",
    "        \n",
    "        sheet.write(r,c,cv_results.mean())\n",
    "\n",
    "        print(\"###########################################\")\n",
    "        msg = \"%s %s %s: %f (%f)\\n\" % (\"TRAIN_AUC\", kind, name, cv_results.mean(), cv_results.std())\n",
    "        print(msg)\n",
    "        output.write(msg)\n",
    "        pipe.fit(X_train,Y_train)\n",
    "        joblib.dump(pipe,'./handpkl/Endo'+name+kind+'20.pkl')\n",
    "        \n",
    "        Y_pred = pipe.predict(X_train)\n",
    "        print(\"Accuracy: \" + repr(accuracy_score(Y_train, Y_pred)))\n",
    "        print(\"Average Precision Score: \" + repr(average_precision_score(Y_train, Y_pred)))\n",
    "        print(\"Kappa: \" + repr(cohen_kappa_score(Y_train, Y_pred)))\n",
    "        print(\"Hamming Loss: \" + repr(hamming_loss(Y_train, Y_pred)))\n",
    "        print(\"AUC: \" + repr(roc_auc_score(Y_train, Y_pred)))\n",
    "        print(\"Sensitivity: \" + repr(recall_score(Y_train, Y_pred)))\n",
    "        tn, fp, fn, tp = confusion_matrix(Y_train, Y_pred).ravel()\n",
    "        print(\"Specificity: \" + repr(tn / (tn + fp)))\n",
    "        \n",
    "        sheet_train.write(r,c,roc_auc_score(Y_train, Y_pred))\n",
    "\n",
    "        Y_pred = pipe.predict(X_validate)\n",
    "        print(\"Accuracy: \" + repr(accuracy_score(Y_validate, Y_pred)))\n",
    "        print(\"Average Precision Score: \" + repr(average_precision_score(Y_validate, Y_pred)))\n",
    "        print(\"Kappa: \" + repr(cohen_kappa_score(Y_validate, Y_pred)))\n",
    "        print(\"Hamming Loss: \" + repr(hamming_loss(Y_validate, Y_pred)))\n",
    "        print(\"AUC\"+repr(roc_auc_score(Y_validate,Y_pred)))\n",
    "        print(\"Sensitivity\" + repr(recall_score(Y_validate,Y_pred)))\n",
    "        tn,fp,fn,tp = confusion_matrix(Y_validate,Y_pred).ravel()\n",
    "        print(\"Specificity\" + repr(tn/(tn+fp)))\n",
    "        \n",
    "        sheet_validate.write(r,c,roc_auc_score(Y_validate,Y_pred))\n",
    "        \n",
    "        Y_pred = pipe.predict(X_test)\n",
    "        print(\"Accuracy: \" + repr(accuracy_score(Y_test, Y_pred)))\n",
    "        print(\"Average Precision Score: \" + repr(average_precision_score(Y_test, Y_pred)))\n",
    "        print(\"Kappa: \" + repr(cohen_kappa_score(Y_test, Y_pred)))\n",
    "        print(\"Hamming Loss: \" + repr(hamming_loss(Y_test, Y_pred)))\n",
    "        print(\"AUC: \" + repr(roc_auc_score(Y_test, Y_pred)))\n",
    "        print(\"Sensitivity\" + repr(recall_score(Y_test,Y_pred)))\n",
    "        tn,fp,fn,tp = confusion_matrix(Y_test,Y_pred).ravel()\n",
    "        print(\"Specificity\" + repr(tn/(tn+fp)))\n",
    "        \n",
    "        sheet_test.write(r,c,roc_auc_score(Y_test,Y_pred))\n",
    "        \n",
    "        r = r + 1\n",
    "    c = c + 1\n",
    "    r = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WLCX\n",
      "WLCX GLM: 0.714052 (0.042058)\n",
      "\n",
      "Accuracy: 0.6742424242424242\n",
      "Average Precision Score: 0.5718877968877969\n",
      "Kappa: 0.3130990415335463\n",
      "Hamming Loss: 0.32575757575757575\n",
      "AUC: 0.6500779625779626\n",
      "Sensitivity: 0.4155405405405405\n",
      "Specificity: 0.8846153846153846\n",
      "Accuracy: 0.6867469879518072\n",
      "Average Precision Score: 0.5928073195035782\n",
      "Kappa: 0.35582089552238805\n",
      "Hamming Loss: 0.3132530120481928\n",
      "AUC0.6742690058479531\n",
      "Sensitivity0.5263157894736842\n",
      "Specificity0.8222222222222222\n",
      "Accuracy: 0.7349397590361446\n",
      "Average Precision Score: 0.6511755524120775\n",
      "Kappa: 0.4526378896882495\n",
      "Hamming Loss: 0.26506024096385544\n",
      "AUC0.7207602339181287\n",
      "Sensitivity0.5526315789473685\n",
      "Specificity0.8888888888888888\n",
      "WLCX LDA: 0.709812 (0.041573)\n",
      "\n",
      "Accuracy: 0.6878787878787879\n",
      "Average Precision Score: 0.5850790466481957\n",
      "Kappa: 0.3467983703589822\n",
      "Hamming Loss: 0.31212121212121213\n",
      "AUC: 0.667489604989605\n",
      "Sensitivity: 0.46959459459459457\n",
      "Specificity: 0.8653846153846154\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.6319725362429756\n",
      "Kappa: 0.4313375037235626\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC0.7116959064327485\n",
      "Sensitivity0.5789473684210527\n",
      "Specificity0.8444444444444444\n",
      "Accuracy: 0.7590361445783133\n",
      "Average Precision Score: 0.6826496268474709\n",
      "Kappa: 0.5023980815347722\n",
      "Hamming Loss: 0.24096385542168675\n",
      "AUC0.7450292397660819\n",
      "Sensitivity0.5789473684210527\n",
      "Specificity0.9111111111111111\n",
      "WLCX KNN: 0.639587 (0.057604)\n",
      "\n",
      "Accuracy: 0.746969696969697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6482675905270048\n",
      "Kappa: 0.4791410695045649\n",
      "Hamming Loss: 0.25303030303030305\n",
      "AUC: 0.7352613602613602\n",
      "Sensitivity: 0.6216216216216216\n",
      "Specificity: 0.8489010989010989\n",
      "Accuracy: 0.6265060240963856\n",
      "Average Precision Score: 0.5346884614481318\n",
      "Kappa: 0.23033203709243188\n",
      "Hamming Loss: 0.37349397590361444\n",
      "AUC0.6125730994152047\n",
      "Sensitivity0.4473684210526316\n",
      "Specificity0.7777777777777778\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.6036369587006771\n",
      "Kappa: 0.38445565114209435\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC0.6894736842105263\n",
      "Sensitivity0.5789473684210527\n",
      "Specificity0.8\n",
      "WLCX DT: 0.565695 (0.019468)\n",
      "\n",
      "Accuracy: 0.8621212121212121\n",
      "Average Precision Score: 0.8289475245996984\n",
      "Kappa: 0.7132298172234\n",
      "Hamming Loss: 0.13787878787878788\n",
      "AUC: 0.8465993465993467\n",
      "Sensitivity: 0.6959459459459459\n",
      "Specificity: 0.9972527472527473\n",
      "Accuracy: 0.6506024096385542\n",
      "Average Precision Score: 0.5564939182567591\n",
      "Kappa: 0.2889217134416544\n",
      "Hamming Loss: 0.3493975903614458\n",
      "AUC0.642982456140351\n",
      "Sensitivity0.5526315789473685\n",
      "Specificity0.7333333333333333\n",
      "Accuracy: 0.6626506024096386\n",
      "Average Precision Score: 0.5677446628619742\n",
      "Kappa: 0.3091557669441142\n",
      "Hamming Loss: 0.3373493975903614\n",
      "AUC0.652046783625731\n",
      "Sensitivity0.5263157894736842\n",
      "Specificity0.7777777777777778\n",
      "WLCX BY: 0.719831 (0.040522)\n",
      "\n",
      "Accuracy: 0.6954545454545454\n",
      "Average Precision Score: 0.5889563747671855\n",
      "Kappa: 0.3771012151832166\n",
      "Hamming Loss: 0.30454545454545456\n",
      "AUC: 0.6863491238491238\n",
      "Sensitivity: 0.597972972972973\n",
      "Specificity: 0.7747252747252747\n",
      "Accuracy: 0.7590361445783133\n",
      "Average Precision Score: 0.6677981274944981\n",
      "Kappa: 0.5106132075471698\n",
      "Hamming Loss: 0.24096385542168675\n",
      "AUC0.7532163742690058\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.8222222222222222\n",
      "Accuracy: 0.7710843373493976\n",
      "Average Precision Score: 0.6780921695316116\n",
      "Kappa: 0.5379431585115734\n",
      "Hamming Loss: 0.2289156626506024\n",
      "AUC0.768421052631579\n",
      "Sensitivity0.7368421052631579\n",
      "Specificity0.8\n",
      "WLCX SVM: 0.722069 (0.039479)\n",
      "\n",
      "Accuracy: 0.6439393939393939"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Precision Score: 0.540703090321411\n",
      "Kappa: 0.2407033896645583\n",
      "Hamming Loss: 0.3560606060606061\n",
      "AUC: 0.6140852390852392\n",
      "Sensitivity: 0.32432432432432434\n",
      "Specificity: 0.9038461538461539\n",
      "Accuracy: 0.6024096385542169\n",
      "Average Precision Score: 0.5133652016974781\n",
      "Kappa: 0.1559322033898305\n",
      "Hamming Loss: 0.39759036144578314\n",
      "AUC0.5739766081871346\n",
      "Sensitivity0.23684210526315788\n",
      "Specificity0.9111111111111111\n",
      "Accuracy: 0.5903614457831325\n",
      "Average Precision Score: 0.5017966603255126\n",
      "Kappa: 0.12847436689314407\n",
      "Hamming Loss: 0.40963855421686746\n",
      "AUC0.5608187134502923\n",
      "Sensitivity0.21052631578947367\n",
      "Specificity0.9111111111111111\n",
      "WLCX BAG: 0.691697 (0.036631)\n",
      "\n",
      "Accuracy: 0.8515151515151516\n",
      "Average Precision Score: 0.8088920088920089\n",
      "Kappa: 0.6914710933028048\n",
      "Hamming Loss: 0.1484848484848485\n",
      "AUC: 0.8363528363528363\n",
      "Sensitivity: 0.6891891891891891\n",
      "Specificity: 0.9835164835164835\n",
      "Accuracy: 0.6626506024096386\n",
      "Average Precision Score: 0.5677446628619742\n",
      "Kappa: 0.3091557669441142\n",
      "Hamming Loss: 0.3373493975903614\n",
      "AUC0.652046783625731\n",
      "Sensitivity0.5263157894736842\n",
      "Specificity0.7777777777777778\n",
      "Accuracy: 0.7108433734939759\n",
      "Average Precision Score: 0.6192929613189601\n",
      "Kappa: 0.4053731343283582\n",
      "Hamming Loss: 0.2891566265060241\n",
      "AUC0.6985380116959066\n",
      "Sensitivity0.5526315789473685\n",
      "Specificity0.8444444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WLCX NNet: 0.714365 (0.051229)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6878787878787879\n",
      "Average Precision Score: 0.5865159193284193\n",
      "Kappa: 0.3442275042444821\n",
      "Hamming Loss: 0.31212121212121213\n",
      "AUC: 0.6655962280962281\n",
      "Sensitivity: 0.44932432432432434\n",
      "Specificity: 0.8818681318681318\n",
      "Accuracy: 0.7349397590361446\n",
      "Average Precision Score: 0.6511755524120775\n",
      "Kappa: 0.4526378896882495\n",
      "Hamming Loss: 0.26506024096385544\n",
      "AUC0.7207602339181287\n",
      "Sensitivity0.5526315789473685\n",
      "Specificity0.8888888888888888\n",
      "Accuracy: 0.7710843373493976\n",
      "Average Precision Score: 0.6963174334765965\n",
      "Kappa: 0.5282680227340713\n",
      "Hamming Loss: 0.2289156626506024\n",
      "AUC0.758187134502924\n",
      "Sensitivity0.6052631578947368\n",
      "Specificity0.9111111111111111\n",
      "WLCX RF: 0.710997 (0.014230)\n",
      "\n",
      "Accuracy: 0.8560606060606061\n",
      "Average Precision Score: 0.8131573433899015\n",
      "Kappa: 0.7014001333460329\n",
      "Hamming Loss: 0.14393939393939395\n",
      "AUC: 0.8417359667359667\n",
      "Sensitivity: 0.7027027027027027\n",
      "Specificity: 0.9807692307692307\n",
      "Accuracy: 0.6746987951807228\n",
      "Average Precision Score: 0.5798438764130934\n",
      "Kappa: 0.3324396782841823\n",
      "Hamming Loss: 0.3253012048192771\n",
      "AUC0.6631578947368422\n",
      "Sensitivity0.5263157894736842\n",
      "Specificity0.8\n",
      "Accuracy: 0.7108433734939759\n",
      "Average Precision Score: 0.6157557863031071\n",
      "Kappa: 0.4103019538188277\n",
      "Hamming Loss: 0.2891566265060241\n",
      "AUC0.7026315789473684\n",
      "Sensitivity0.6052631578947368\n",
      "Specificity0.8\n",
      "WLCX BST: 0.694456 (0.036619)\n",
      "\n",
      "Accuracy: 0.7560606060606061\n",
      "Average Precision Score: 0.6602030947775629\n",
      "Kappa: 0.4972082899593072\n",
      "Hamming Loss: 0.24393939393939393\n",
      "AUC: 0.7438186813186813\n",
      "Sensitivity: 0.625\n",
      "Specificity: 0.8626373626373627\n",
      "Accuracy: 0.7710843373493976\n",
      "Average Precision Score: 0.6913607242035291\n",
      "Kappa: 0.5302353291629431\n",
      "Hamming Loss: 0.2289156626506024\n",
      "AUC0.7602339181286549\n",
      "Sensitivity0.631578947368421\n",
      "Specificity0.8888888888888888\n",
      "Accuracy: 0.6867469879518072\n",
      "Average Precision Score: 0.5916613823715916\n",
      "Kappa: 0.3585017835909632\n",
      "Hamming Loss: 0.3132530120481928\n",
      "AUC0.6763157894736844\n",
      "Sensitivity0.5526315789473685\n",
      "Specificity0.8\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validate = scaler.fit_transform(X_validate)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# WILCOXON SCORE FUNCTION\n",
    "def takeSecond(elem):\n",
    "    return elem[1]\n",
    "def WLCX(data, target, n_selected_features):\n",
    "    pval = []\n",
    "    for num in range(len(data[1])):\n",
    "        x = data[:,num]\n",
    "        pval.append([num, wilcoxon(x,target)[1]])\n",
    "    pval.sort(key=takeSecond)\n",
    "    idx = []\n",
    "    for i in range(n_selected_features):\n",
    "        idx.append(pval[i][0])\n",
    "    return idx\n",
    "\n",
    "# MULTIVARIATE FEATURE SELECTION X CLASSIFICATION (10 fold CV)\n",
    "\n",
    "# print('BEFORE')\n",
    "MV_sel = []\n",
    "MV_sel.append(('WLCX', WLCX(X_train, Y_train, n_selected_features=num_fea)))\n",
    "print('WLCX')\n",
    "for name, model in models:\n",
    "    for kind, idx in MV_sel:\n",
    "        # X_sel = X[:, idx[0:num_fea]]\n",
    "        X_test_ = X_test[:,idx[0:num_fea]]\n",
    "        X_validate_ = X_validate[:,idx[0:num_fea]]\n",
    "        X_train_ = X_train[:, idx[0:num_fea]]\n",
    "        # X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X_sel, Y, test_size=validation_size, random_state=seed)\n",
    "        #kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "        cv_results = model_selection.cross_val_score(model, X_train_, Y_train, cv=kfold, scoring='roc_auc')\n",
    "        \n",
    "        sheet.write(r,c,cv_results.mean())\n",
    "        \n",
    "        msg = \"%s %s: %f (%f)\\n\" % (kind, name, cv_results.mean(), cv_results.std())\n",
    "        print(msg)\n",
    "        output.write(msg)\n",
    "        model.fit(X_train_, Y_train)\n",
    "        joblib.dump(model,'./handpkl/Endo'+name+kind+'20.pkl')\n",
    "        \n",
    "        Y_pred = model.predict(X_train_)\n",
    "        print(\"Accuracy: \" + repr(accuracy_score(Y_train, Y_pred)))\n",
    "        print(\"Average Precision Score: \" + repr(average_precision_score(Y_train, Y_pred)))\n",
    "        print(\"Kappa: \" + repr(cohen_kappa_score(Y_train, Y_pred)))\n",
    "        print(\"Hamming Loss: \" + repr(hamming_loss(Y_train, Y_pred)))\n",
    "        print(\"AUC: \" + repr(roc_auc_score(Y_train, Y_pred)))\n",
    "        print(\"Sensitivity: \" + repr(recall_score(Y_train, Y_pred)))\n",
    "        tn, fp, fn, tp = confusion_matrix(Y_train, Y_pred).ravel()\n",
    "        print(\"Specificity: \" + repr(tn / (tn + fp)))\n",
    "        \n",
    "        sheet_train.write(r,c,roc_auc_score(Y_train, Y_pred))\n",
    "        \n",
    "        Y_pred = model.predict(X_validate_)    \n",
    "        print(\"Accuracy: \" + repr(accuracy_score(Y_validate, Y_pred)))\n",
    "        print(\"Average Precision Score: \" + repr(average_precision_score(Y_validate, Y_pred)))\n",
    "        print(\"Kappa: \" + repr(cohen_kappa_score(Y_validate, Y_pred)))\n",
    "        print(\"Hamming Loss: \" + repr(hamming_loss(Y_validate, Y_pred)))\n",
    "        print(\"AUC\"+repr(roc_auc_score(Y_validate,Y_pred)))\n",
    "        print(\"Sensitivity\" + repr(recall_score(Y_validate,Y_pred)))\n",
    "        tn,fp,fn,tp = confusion_matrix(Y_validate,Y_pred).ravel()\n",
    "        print(\"Specificity\" + repr(tn/(tn+fp)))\n",
    "        \n",
    "        sheet_validate.write(r,c,roc_auc_score(Y_validate,Y_pred))\n",
    "        \n",
    "        Y_pred = model.predict(X_test_)\n",
    "        print(\"Accuracy: \" + repr(accuracy_score(Y_test, Y_pred)))\n",
    "        print(\"Average Precision Score: \" + repr(average_precision_score(Y_test, Y_pred)))\n",
    "        print(\"Kappa: \" + repr(cohen_kappa_score(Y_test, Y_pred)))\n",
    "        print(\"Hamming Loss: \" + repr(hamming_loss(Y_test, Y_pred)))\n",
    "        print(\"AUC\"+repr(roc_auc_score(Y_test,Y_pred)))\n",
    "        print(\"Sensitivity\" + repr(recall_score(Y_test,Y_pred)))\n",
    "        tn,fp,fn,tp = confusion_matrix(Y_test,Y_pred).ravel()\n",
    "        print(\"Specificity\" + repr(tn/(tn+fp)))\n",
    "        \n",
    "        sheet_test.write(r,c,roc_auc_score(Y_test,Y_pred))\n",
    "        \n",
    "        r = r + 1\n",
    "    c = c + 1\n",
    "    r = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIM\n",
      "MIFS\n",
      "MRMR\n",
      "CIFE\n",
      "JMI\n",
      "CMIM\n",
      "ICAP\n"
     ]
    }
   ],
   "source": [
    "MV_sel = []\n",
    "MV_sel.append(('MIM', MIM.mim(X_train, Y_train, n_selected_features=num_fea)))\n",
    "print('MIM')\n",
    "MV_sel.append(('MIFS', MIFS.mifs(X_train, Y_train, n_selected_features=num_fea)))\n",
    "print('MIFS')\n",
    "MV_sel.append(('MRMR', MRMR.mrmr(X_train, Y_train, n_selected_features=num_fea)))\n",
    "print('MRMR')\n",
    "MV_sel.append(('CIFE', CIFE.cife(X_train, Y_train, n_selected_features=num_fea)))\n",
    "print('CIFE')\n",
    "MV_sel.append(('JMI', JMI.jmi(X_train, Y_train, n_selected_features=num_fea)))\n",
    "print('JMI')\n",
    "MV_sel.append(('CMIM', CMIM.cmim(X_train, Y_train, n_selected_features=num_fea)))\n",
    "print('CMIM')\n",
    "MV_sel.append(('ICAP', ICAP.icap(X_train, Y_train, n_selected_features=num_fea)))\n",
    "print('ICAP')\n",
    "MV_sel.append(('DISR', DISR.disr(X_train, Y_train, n_selected_features=num_fea)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIM GLM: 0.805010 (0.025410)\n",
      "\n",
      "Accuracy: 0.753030303030303\n",
      "Average Precision Score: 0.6496109746109745\n",
      "Kappa: 0.49902207320480585\n",
      "Hamming Loss: 0.24696969696969698\n",
      "AUC: 0.7486449361449361\n",
      "Sensitivity: 0.706081081081081\n",
      "Specificity: 0.7912087912087912\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.6265513180541716\n",
      "Kappa: 0.4383642247719918\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC0.7178362573099415\n",
      "Sensitivity0.6578947368421053\n",
      "Specificity0.7777777777777778\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.6510222968688409\n",
      "Kappa: 0.48930559624963377\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC0.7441520467836258\n",
      "Sensitivity0.7105263157894737\n",
      "Specificity0.7777777777777778\n",
      "MIFS GLM: 0.739592 (0.005527)\n",
      "\n",
      "Accuracy: 0.6803030303030303\n",
      "Average Precision Score: 0.5750560368981421\n",
      "Kappa: 0.3355028343481\n",
      "Hamming Loss: 0.3196969696969697\n",
      "AUC: 0.6631459756459755\n",
      "Sensitivity: 0.4966216216216216\n",
      "Specificity: 0.8296703296703297\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.6050007653117006\n",
      "Kappa: 0.3818885910038724\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC0.6874269005847954\n",
      "Sensitivity0.5526315789473685\n",
      "Specificity0.8222222222222222\n",
      "Accuracy: 0.6506024096385542\n",
      "Average Precision Score: 0.5588459099556119\n",
      "Kappa: 0.3061400980109541\n",
      "Hamming Loss: 0.3493975903614458\n",
      "AUC0.6552631578947368\n",
      "Sensitivity0.7105263157894737\n",
      "Specificity0.6\n",
      "MRMR GLM: 0.855426 (0.027305)\n",
      "\n",
      "Accuracy: 0.7863636363636364\n",
      "Average Precision Score: 0.6895485585877045\n",
      "Kappa: 0.566089113527426\n",
      "Hamming Loss: 0.21363636363636362\n",
      "AUC: 0.7817047817047816\n",
      "Sensitivity: 0.7364864864864865\n",
      "Specificity: 0.8269230769230769\n",
      "Accuracy: 0.7951807228915663\n",
      "Average Precision Score: 0.7036729915613873\n",
      "Kappa: 0.5882696235774729\n",
      "Hamming Loss: 0.20481927710843373\n",
      "AUC0.7947368421052632\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.8\n",
      "Accuracy: 0.7951807228915663\n",
      "Average Precision Score: 0.6989721431626137\n",
      "Kappa: 0.5916063675832127\n",
      "Hamming Loss: 0.20481927710843373\n",
      "AUC0.7988304093567251\n",
      "Sensitivity0.8421052631578947\n",
      "Specificity0.7555555555555555\n",
      "CIFE GLM: 0.820152 (0.034494)\n",
      "\n",
      "Accuracy: 0.7606060606060606\n",
      "Average Precision Score: 0.659341728091728\n",
      "Kappa: 0.513001569154898\n",
      "Hamming Loss: 0.23939393939393938\n",
      "AUC: 0.7548819423819424\n",
      "Sensitivity: 0.6993243243243243\n",
      "Specificity: 0.8104395604395604\n",
      "Accuracy: 0.6506024096385542\n",
      "Average Precision Score: 0.5662992049168334\n",
      "Kappa: 0.33305624826821845\n",
      "Hamming Loss: 0.3493975903614458\n",
      "AUC0.6757309941520468\n",
      "Sensitivity0.9736842105263158\n",
      "Specificity0.37777777777777777\n",
      "Accuracy: 0.6144578313253012\n",
      "Average Precision Score: 0.5408427970254222\n",
      "Kappa: 0.2654867256637169\n",
      "Hamming Loss: 0.3855421686746988\n",
      "AUC0.6403508771929824\n",
      "Sensitivity0.9473684210526315\n",
      "Specificity0.3333333333333333\n",
      "JMI GLM: 0.795268 (0.025074)\n",
      "\n",
      "Accuracy: 0.7287878787878788\n",
      "Average Precision Score: 0.6230805943439396\n",
      "Kappa: 0.44914859093198045\n",
      "Hamming Loss: 0.27121212121212124\n",
      "AUC: 0.723511286011286\n",
      "Sensitivity: 0.6722972972972973\n",
      "Specificity: 0.7747252747252747\n",
      "Accuracy: 0.7108433734939759\n",
      "Average Precision Score: 0.633275206087508\n",
      "Kappa: 0.395264116575592\n",
      "Hamming Loss: 0.2891566265060241\n",
      "AUC0.6903508771929825\n",
      "Sensitivity0.4473684210526316\n",
      "Specificity0.9333333333333333\n",
      "Accuracy: 0.6506024096385542\n",
      "Average Precision Score: 0.557966419453558\n",
      "Kappa: 0.2739064856711916\n",
      "Hamming Loss: 0.3493975903614458\n",
      "AUC0.6327485380116958\n",
      "Sensitivity0.42105263157894735\n",
      "Specificity0.8444444444444444\n",
      "CMIM GLM: 0.783716 (0.027155)\n",
      "\n",
      "Accuracy: 0.7181818181818181\n",
      "Average Precision Score: 0.6121032777967085\n",
      "Kappa: 0.42633369471755955\n",
      "Hamming Loss: 0.2818181818181818\n",
      "AUC: 0.7116869616869617\n",
      "Sensitivity: 0.6486486486486487\n",
      "Specificity: 0.7747252747252747\n",
      "Accuracy: 0.6746987951807228\n",
      "Average Precision Score: 0.5976960473472839\n",
      "Kappa: 0.3123657563669837\n",
      "Hamming Loss: 0.3253012048192771\n",
      "AUC0.6488304093567251\n",
      "Sensitivity0.34210526315789475\n",
      "Specificity0.9555555555555556\n",
      "Accuracy: 0.5903614457831325\n",
      "Average Precision Score: 0.5039632213062777\n",
      "Kappa: 0.12087227414330204\n",
      "Hamming Loss: 0.40963855421686746\n",
      "AUC0.5567251461988305\n",
      "Sensitivity0.15789473684210525\n",
      "Specificity0.9555555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICAP GLM: 0.783716 (0.027155)\n",
      "\n",
      "Accuracy: 0.7181818181818181\n",
      "Average Precision Score: 0.6121032777967085\n",
      "Kappa: 0.42633369471755955\n",
      "Hamming Loss: 0.2818181818181818\n",
      "AUC: 0.7116869616869617\n",
      "Sensitivity: 0.6486486486486487\n",
      "Specificity: 0.7747252747252747\n",
      "Accuracy: 0.6746987951807228\n",
      "Average Precision Score: 0.5976960473472839\n",
      "Kappa: 0.3123657563669837\n",
      "Hamming Loss: 0.3253012048192771\n",
      "AUC0.6488304093567251\n",
      "Sensitivity0.34210526315789475\n",
      "Specificity0.9555555555555556\n",
      "Accuracy: 0.5903614457831325\n",
      "Average Precision Score: 0.5039632213062777\n",
      "Kappa: 0.12087227414330204\n",
      "Hamming Loss: 0.40963855421686746\n",
      "AUC0.5567251461988305\n",
      "Sensitivity0.15789473684210525\n",
      "Specificity0.9555555555555556\n",
      "DISR GLM: 0.802809 (0.038537)\n",
      "\n",
      "Accuracy: 0.7272727272727273\n",
      "Average Precision Score: 0.621392257828428\n",
      "Kappa: 0.44624678375657234\n",
      "Hamming Loss: 0.2727272727272727\n",
      "AUC: 0.7221376596376596\n",
      "Sensitivity: 0.6722972972972973\n",
      "Specificity: 0.771978021978022\n",
      "Accuracy: 0.7590361445783133\n",
      "Average Precision Score: 0.6617232086239696\n",
      "Kappa: 0.5165987186953989\n",
      "Hamming Loss: 0.24096385542168675\n",
      "AUC0.7593567251461988\n",
      "Sensitivity0.7631578947368421\n",
      "Specificity0.7555555555555555\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.6494967725151619\n",
      "Kappa: 0.4913918879486431\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC0.7461988304093568\n",
      "Sensitivity0.7368421052631579\n",
      "Specificity0.7555555555555555\n",
      "MIM LDA: 0.811474 (0.019854)\n",
      "\n",
      "Accuracy: 0.7606060606060606\n",
      "Average Precision Score: 0.6590965065785209\n",
      "Kappa: 0.5133106821491245\n",
      "Hamming Loss: 0.23939393939393938\n",
      "AUC: 0.7551975051975052\n",
      "Sensitivity: 0.7027027027027027\n",
      "Specificity: 0.8076923076923077\n",
      "Accuracy: 0.5421686746987951\n",
      "Average Precision Score: 0.4600507292327204\n",
      "Kappa: 0.00879949717159012\n",
      "Hamming Loss: 0.4578313253012048\n",
      "AUC0.5040935672514619\n",
      "Sensitivity0.05263157894736842\n",
      "Specificity0.9555555555555556\n",
      "Accuracy: 0.5180722891566265\n",
      "Average Precision Score: 0.4578313253012048\n",
      "Kappa: -0.04797979797979779\n",
      "Hamming Loss: 0.4819277108433735\n",
      "AUC0.4777777777777778\n",
      "Sensitivity0.0\n",
      "Specificity0.9555555555555556\n",
      "MIFS LDA: 0.765043 (0.025160)\n",
      "\n",
      "Accuracy: 0.7075757575757575\n",
      "Average Precision Score: 0.6023333166190309\n",
      "Kappa: 0.3992076219224602\n",
      "Hamming Loss: 0.2924242424242424\n",
      "AUC: 0.6963914463914463\n",
      "Sensitivity: 0.5878378378378378\n",
      "Specificity: 0.804945054945055\n",
      "Accuracy: 0.5060240963855421\n",
      "Average Precision Score: 0.4788332276474318\n",
      "Kappa: 0.07502038597444971\n",
      "Hamming Loss: 0.4939759036144578\n",
      "AUC0.5403508771929824\n",
      "Sensitivity0.9473684210526315\n",
      "Specificity0.13333333333333333\n",
      "Accuracy: 0.5180722891566265\n",
      "Average Precision Score: 0.48717948717948717\n",
      "Kappa: 0.10270270270270254\n",
      "Hamming Loss: 0.4819277108433735\n",
      "AUC0.5555555555555556\n",
      "Sensitivity1.0\n",
      "Specificity0.1111111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRMR LDA: 0.844180 (0.030586)\n",
      "\n",
      "Accuracy: 0.7954545454545454\n",
      "Average Precision Score: 0.7012214560601657\n",
      "Kappa: 0.5842897934046246\n",
      "Hamming Loss: 0.20454545454545456\n",
      "AUC: 0.7905776655776655\n",
      "Sensitivity: 0.7432432432432432\n",
      "Specificity: 0.8379120879120879\n",
      "Accuracy: 0.6385542168674698\n",
      "Average Precision Score: 0.5560936445271749\n",
      "Kappa: 0.30602006688963224\n",
      "Hamming Loss: 0.3614457831325301\n",
      "AUC0.6605263157894736\n",
      "Sensitivity0.9210526315789473\n",
      "Specificity0.4\n",
      "Accuracy: 0.7710843373493976\n",
      "Average Precision Score: 0.6670721162160604\n",
      "Kappa: 0.5543938965809551\n",
      "Hamming Loss: 0.2289156626506024\n",
      "AUC0.786842105263158\n",
      "Sensitivity0.9736842105263158\n",
      "Specificity0.6\n",
      "CIFE LDA: 0.812267 (0.030241)\n",
      "\n",
      "Accuracy: 0.7712121212121212\n",
      "Average Precision Score: 0.6685414487394685\n",
      "Kappa: 0.5385342001444684\n",
      "Hamming Loss: 0.2287878787878788\n",
      "AUC: 0.7698618948618949\n",
      "Sensitivity: 0.7567567567567568\n",
      "Specificity: 0.782967032967033\n",
      "Accuracy: 0.7590361445783133\n",
      "Average Precision Score: 0.6572804375396322\n",
      "Kappa: 0.5243553008595989\n",
      "Hamming Loss: 0.24096385542168675\n",
      "AUC0.7675438596491229\n",
      "Sensitivity0.868421052631579\n",
      "Specificity0.6666666666666666\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.6221748962084684\n",
      "Kappa: 0.4584397163120567\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC0.7362573099415205\n",
      "Sensitivity0.8947368421052632\n",
      "Specificity0.5777777777777777\n",
      "JMI LDA: 0.809509 (0.027706)\n",
      "\n",
      "Accuracy: 0.75\n",
      "Average Precision Score: 0.6456565515843866\n",
      "Kappa: 0.4938368007139272\n",
      "Hamming Loss: 0.25\n",
      "AUC: 0.7465288090288089\n",
      "Sensitivity: 0.7128378378378378\n",
      "Specificity: 0.7802197802197802\n",
      "Accuracy: 0.5421686746987951\n",
      "Average Precision Score: 0.4600507292327204\n",
      "Kappa: 0.00879949717159012\n",
      "Hamming Loss: 0.4578313253012048\n",
      "AUC0.5040935672514619\n",
      "Sensitivity0.05263157894736842\n",
      "Specificity0.9555555555555556\n",
      "Accuracy: 0.5180722891566265\n",
      "Average Precision Score: 0.4578313253012048\n",
      "Kappa: -0.04797979797979779\n",
      "Hamming Loss: 0.4819277108433735\n",
      "AUC0.4777777777777778\n",
      "Sensitivity0.0\n",
      "Specificity0.9555555555555556\n",
      "CMIM LDA: 0.786016 (0.028300)\n",
      "\n",
      "Accuracy: 0.7393939393939394\n",
      "Average Precision Score: 0.6355980980980981\n",
      "Kappa: 0.46883773161145426\n",
      "Hamming Loss: 0.2606060606060606\n",
      "AUC: 0.732495544995545\n",
      "Sensitivity: 0.6655405405405406\n",
      "Specificity: 0.7994505494505495\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.6036369587006771\n",
      "Kappa: 0.38445565114209435\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC0.6894736842105263\n",
      "Sensitivity0.5789473684210527\n",
      "Specificity0.8\n",
      "Accuracy: 0.6867469879518072\n",
      "Average Precision Score: 0.5928073195035782\n",
      "Kappa: 0.35582089552238805\n",
      "Hamming Loss: 0.3132530120481928\n",
      "AUC0.6742690058479531\n",
      "Sensitivity0.5263157894736842\n",
      "Specificity0.8222222222222222\n",
      "ICAP LDA: 0.786016 (0.028300)\n",
      "\n",
      "Accuracy: 0.7393939393939394\n",
      "Average Precision Score: 0.6355980980980981\n",
      "Kappa: 0.46883773161145426\n",
      "Hamming Loss: 0.2606060606060606\n",
      "AUC: 0.732495544995545\n",
      "Sensitivity: 0.6655405405405406\n",
      "Specificity: 0.7994505494505495\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.6036369587006771\n",
      "Kappa: 0.38445565114209435\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC0.6894736842105263\n",
      "Sensitivity0.5789473684210527\n",
      "Specificity0.8\n",
      "Accuracy: 0.6867469879518072\n",
      "Average Precision Score: 0.5928073195035782\n",
      "Kappa: 0.35582089552238805\n",
      "Hamming Loss: 0.3132530120481928\n",
      "AUC0.6742690058479531\n",
      "Sensitivity0.5263157894736842\n",
      "Specificity0.8222222222222222\n",
      "DISR LDA: 0.799383 (0.042390)\n",
      "\n",
      "Accuracy: 0.7378787878787879\n",
      "Average Precision Score: 0.6325699075699076\n",
      "Kappa: 0.46862376440365605\n",
      "Hamming Loss: 0.26212121212121214\n",
      "AUC: 0.7336464211464212\n",
      "Sensitivity: 0.6925675675675675\n",
      "Specificity: 0.7747252747252747\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.657639045144926\n",
      "Kappa: 0.48294274695935924\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC0.7380116959064328\n",
      "Sensitivity0.631578947368421\n",
      "Specificity0.8444444444444444\n",
      "Accuracy: 0.7831325301204819\n",
      "Average Precision Score: 0.6884908053265694\n",
      "Kappa: 0.564938846825859\n",
      "Hamming Loss: 0.21686746987951808\n",
      "AUC0.7836257309941521\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.7777777777777778\n",
      "MIM KNN: 0.776175 (0.028276)\n",
      "\n",
      "Accuracy: 0.8242424242424242\n",
      "Average Precision Score: 0.7310330435330435\n",
      "Kappa: 0.6464982269503545\n",
      "Hamming Loss: 0.17575757575757575\n",
      "AUC: 0.8248811998811999\n",
      "Sensitivity: 0.831081081081081\n",
      "Specificity: 0.8186813186813187\n",
      "Accuracy: 0.7108433734939759\n",
      "Average Precision Score: 0.6112596840451049\n",
      "Kappa: 0.42692750287687\n",
      "Hamming Loss: 0.2891566265060241\n",
      "AUC0.7169590643274854\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.6444444444444445\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.6021516933030964\n",
      "Kappa: 0.41827866554527615\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC0.7181286549707602\n",
      "Sensitivity0.9473684210526315\n",
      "Specificity0.4888888888888889\n",
      "MIFS KNN: 0.672044 (0.029942)\n",
      "\n",
      "Accuracy: 0.759090909090909\n",
      "Average Precision Score: 0.6572120507604379\n",
      "Kappa: 0.510385756676558\n",
      "Hamming Loss: 0.2409090909090909\n",
      "AUC: 0.7538238788238788\n",
      "Sensitivity: 0.7027027027027027\n",
      "Specificity: 0.804945054945055\n",
      "Accuracy: 0.6506024096385542\n",
      "Average Precision Score: 0.5588459099556119\n",
      "Kappa: 0.3061400980109541\n",
      "Hamming Loss: 0.3493975903614458\n",
      "AUC0.6552631578947368\n",
      "Sensitivity0.7105263157894737\n",
      "Specificity0.6\n",
      "Accuracy: 0.6024096385542169\n",
      "Average Precision Score: 0.525022691384734\n",
      "Kappa: 0.21988037596126464\n",
      "Hamming Loss: 0.39759036144578314\n",
      "AUC0.6128654970760234\n",
      "Sensitivity0.7368421052631579\n",
      "Specificity0.4888888888888889\n",
      "MRMR KNN: 0.804800 (0.016908)\n",
      "\n",
      "Accuracy: 0.8106060606060606\n",
      "Average Precision Score: 0.7142645056632195\n",
      "Kappa: 0.6189517246475881\n",
      "Hamming Loss: 0.1893939393939394\n",
      "AUC: 0.8109407484407484\n",
      "Sensitivity: 0.8141891891891891\n",
      "Specificity: 0.8076923076923077\n",
      "Accuracy: 0.7951807228915663\n",
      "Average Precision Score: 0.6970830691185795\n",
      "Kappa: 0.593254540213318\n",
      "Hamming Loss: 0.20481927710843373\n",
      "AUC0.8008771929824562\n",
      "Sensitivity0.868421052631579\n",
      "Specificity0.7333333333333333\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.6456374209042216\n",
      "Kappa: 0.49956933677863913\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC0.7543859649122807\n",
      "Sensitivity0.8421052631578947\n",
      "Specificity0.6666666666666666\n",
      "CIFE KNN: 0.779211 (0.042788)\n",
      "\n",
      "Accuracy: 0.8212121212121212\n",
      "Average Precision Score: 0.7264939637566219\n",
      "Kappa: 0.6408544233748985\n",
      "Hamming Loss: 0.1787878787878788\n",
      "AUC: 0.8224495099495099\n",
      "Sensitivity: 0.8344594594594594\n",
      "Specificity: 0.8104395604395604\n",
      "Accuracy: 0.5542168674698795\n",
      "Average Precision Score: 0.4989188852043203\n",
      "Kappa: 0.14241831890533363\n",
      "Hamming Loss: 0.4457831325301205\n",
      "AUC0.5745614035087719\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.3333333333333333\n",
      "Accuracy: 0.6144578313253012\n",
      "Average Precision Score: 0.5359350937584927\n",
      "Kappa: 0.25098702763677383\n",
      "Hamming Loss: 0.3855421686746988\n",
      "AUC0.6301169590643274\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.4444444444444444\n",
      "JMI KNN: 0.743144 (0.011558)\n",
      "\n",
      "Accuracy: 0.7833333333333333\n",
      "Average Precision Score: 0.68289914045728\n",
      "Kappa: 0.5627073410308208\n",
      "Hamming Loss: 0.21666666666666667\n",
      "AUC: 0.7817975942975943\n",
      "Sensitivity: 0.7668918918918919\n",
      "Specificity: 0.7967032967032966\n",
      "Accuracy: 0.7831325301204819\n",
      "Average Precision Score: 0.6935813429155218\n",
      "Kappa: 0.5613623018203171\n",
      "Hamming Loss: 0.21686746987951808\n",
      "AUC0.7795321637426901\n",
      "Sensitivity0.7368421052631579\n",
      "Specificity0.8222222222222222\n",
      "Accuracy: 0.6867469879518072\n",
      "Average Precision Score: 0.5894519907886394\n",
      "Kappa: 0.36900584795321634\n",
      "Hamming Loss: 0.3132530120481928\n",
      "AUC0.6845029239766083\n",
      "Sensitivity0.6578947368421053\n",
      "Specificity0.7111111111111111\n",
      "CMIM KNN: 0.749222 (0.022779)\n",
      "\n",
      "Accuracy: 0.7818181818181819\n",
      "Average Precision Score: 0.680012747351457\n",
      "Kappa: 0.5608944742191831\n",
      "Hamming Loss: 0.21818181818181817\n",
      "AUC: 0.7816862191862192\n",
      "Sensitivity: 0.7804054054054054\n",
      "Specificity: 0.782967032967033\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.601757405562098\n",
      "Kappa: 0.3895263312739041\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC0.6935672514619883\n",
      "Sensitivity0.631578947368421\n",
      "Specificity0.7555555555555555\n",
      "Accuracy: 0.6385542168674698\n",
      "Average Precision Score: 0.5454751336171755\n",
      "Kappa: 0.25671641791044775\n",
      "Hamming Loss: 0.3614457831325301\n",
      "AUC0.6257309941520468\n",
      "Sensitivity0.47368421052631576\n",
      "Specificity0.7777777777777778\n",
      "ICAP KNN: 0.749222 (0.022779)\n",
      "\n",
      "Accuracy: 0.7818181818181819\n",
      "Average Precision Score: 0.680012747351457\n",
      "Kappa: 0.5608944742191831\n",
      "Hamming Loss: 0.21818181818181817\n",
      "AUC: 0.7816862191862192\n",
      "Sensitivity: 0.7804054054054054\n",
      "Specificity: 0.782967032967033\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.601757405562098\n",
      "Kappa: 0.3895263312739041\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC0.6935672514619883\n",
      "Sensitivity0.631578947368421\n",
      "Specificity0.7555555555555555\n",
      "Accuracy: 0.6385542168674698\n",
      "Average Precision Score: 0.5454751336171755\n",
      "Kappa: 0.25671641791044775\n",
      "Hamming Loss: 0.3614457831325301\n",
      "AUC0.6257309941520468\n",
      "Sensitivity0.47368421052631576\n",
      "Specificity0.7777777777777778\n",
      "DISR KNN: 0.746878 (0.015391)\n",
      "\n",
      "Accuracy: 0.7954545454545454\n",
      "Average Precision Score: 0.6997303289986216\n",
      "Kappa: 0.5853422438988067\n",
      "Hamming Loss: 0.20454545454545456\n",
      "AUC: 0.7918399168399168\n",
      "Sensitivity: 0.7567567567567568\n",
      "Specificity: 0.8269230769230769\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.6482283434121595\n",
      "Kappa: 0.4934612031386225\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC0.7482456140350877\n",
      "Sensitivity0.7631578947368421\n",
      "Specificity0.7333333333333333\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.6236911703295854\n",
      "Kappa: 0.4452194129613485\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC0.7239766081871345\n",
      "Sensitivity0.7368421052631579\n",
      "Specificity0.7111111111111111\n",
      "MIM DT: 0.633678 (0.019129)\n",
      "\n",
      "Accuracy: 1.0\n",
      "Average Precision Score: 1.0\n",
      "Kappa: 1.0\n",
      "Hamming Loss: 0.0\n",
      "AUC: 1.0\n",
      "Sensitivity: 1.0\n",
      "Specificity: 1.0\n",
      "Accuracy: 0.6385542168674698\n",
      "Average Precision Score: 0.5470513633481293\n",
      "Kappa: 0.24408014571949\n",
      "Hamming Loss: 0.3614457831325301\n",
      "AUC0.6175438596491228\n",
      "Sensitivity0.3684210526315789\n",
      "Specificity0.8666666666666667\n",
      "Accuracy: 0.5783132530120482\n",
      "Average Precision Score: 0.49150285351934053\n",
      "Kappa: 0.10862227677201597\n",
      "Hamming Loss: 0.42168674698795183\n",
      "AUC0.5517543859649123\n",
      "Sensitivity0.23684210526315788\n",
      "Specificity0.8666666666666667\n",
      "MIFS DT: 0.584013 (0.050193)\n",
      "\n",
      "Accuracy: 1.0\n",
      "Average Precision Score: 1.0\n",
      "Kappa: 1.0\n",
      "Hamming Loss: 0.0\n",
      "AUC: 1.0\n",
      "Sensitivity: 1.0\n",
      "Specificity: 1.0\n",
      "Accuracy: 0.4819277108433735\n",
      "Average Precision Score: 0.4566743428005652\n",
      "Kappa: -0.0045032367013790875\n",
      "Hamming Loss: 0.5180722891566265\n",
      "AUC0.49766081871345025\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.3111111111111111\n",
      "Accuracy: 0.5662650602409639\n",
      "Average Precision Score: 0.49928001479602624\n",
      "Kappa: 0.14383954154727796\n",
      "Hamming Loss: 0.43373493975903615\n",
      "AUC0.5733918128654971\n",
      "Sensitivity0.6578947368421053\n",
      "Specificity0.4888888888888889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRMR DT: 0.662506 (0.038226)\n",
      "\n",
      "Accuracy: 1.0\n",
      "Average Precision Score: 1.0\n",
      "Kappa: 1.0\n",
      "Hamming Loss: 0.0\n",
      "AUC: 1.0\n",
      "Sensitivity: 1.0\n",
      "Specificity: 1.0\n",
      "Accuracy: 0.6506024096385542\n",
      "Average Precision Score: 0.5567533291058973\n",
      "Kappa: 0.27998803469937183\n",
      "Hamming Loss: 0.3493975903614458\n",
      "AUC0.6368421052631579\n",
      "Sensitivity0.47368421052631576\n",
      "Specificity0.8\n",
      "Accuracy: 0.6144578313253012\n",
      "Average Precision Score: 0.5287492073557387\n",
      "Kappa: 0.22655794991263833\n",
      "Hamming Loss: 0.3855421686746988\n",
      "AUC0.6137426900584795\n",
      "Sensitivity0.6052631578947368\n",
      "Specificity0.6222222222222222\n",
      "CIFE DT: 0.696531 (0.048449)\n",
      "\n",
      "Accuracy: 1.0\n",
      "Average Precision Score: 1.0\n",
      "Kappa: 1.0\n",
      "Hamming Loss: 0.0\n",
      "AUC: 1.0\n",
      "Sensitivity: 1.0\n",
      "Specificity: 1.0\n",
      "Accuracy: 0.4578313253012048\n",
      "Average Precision Score: 0.4348152845299326\n",
      "Kappa: -0.11725994615614721\n",
      "Hamming Loss: 0.5421686746987951\n",
      "AUC0.44269005847953213\n",
      "Sensitivity0.2631578947368421\n",
      "Specificity0.6222222222222222\n",
      "Accuracy: 0.4578313253012048\n",
      "Average Precision Score: 0.4377087296554639\n",
      "Kappa: -0.14605707272169388\n",
      "Hamming Loss: 0.5421686746987951\n",
      "AUC0.4304093567251462\n",
      "Sensitivity0.10526315789473684\n",
      "Specificity0.7555555555555555\n",
      "JMI DT: 0.599180 (0.039878)\n",
      "\n",
      "Accuracy: 1.0\n",
      "Average Precision Score: 1.0\n",
      "Kappa: 1.0\n",
      "Hamming Loss: 0.0\n",
      "AUC: 1.0\n",
      "Sensitivity: 1.0\n",
      "Specificity: 1.0\n",
      "Accuracy: 0.5421686746987951\n",
      "Average Precision Score: 0.4578313253012048\n",
      "Kappa: 0.0\n",
      "Hamming Loss: 0.4578313253012048\n",
      "AUC0.5\n",
      "Sensitivity0.0\n",
      "Specificity1.0\n",
      "Accuracy: 0.5301204819277109\n",
      "Average Precision Score: 0.4578313253012048\n",
      "Kappa: -0.024043024359380105\n",
      "Hamming Loss: 0.46987951807228917\n",
      "AUC0.4888888888888889\n",
      "Sensitivity0.0\n",
      "Specificity0.9777777777777777\n",
      "CMIM DT: 0.630925 (0.037264)\n",
      "\n",
      "Accuracy: 1.0\n",
      "Average Precision Score: 1.0\n",
      "Kappa: 1.0\n",
      "Hamming Loss: 0.0\n",
      "AUC: 1.0\n",
      "Sensitivity: 1.0\n",
      "Specificity: 1.0\n",
      "Accuracy: 0.6024096385542169\n",
      "Average Precision Score: 0.51455583097942\n",
      "Kappa: 0.1522748375116062\n",
      "Hamming Loss: 0.39759036144578314\n",
      "AUC0.5719298245614035\n",
      "Sensitivity0.21052631578947367\n",
      "Specificity0.9333333333333333\n",
      "Accuracy: 0.6746987951807228\n",
      "Average Precision Score: 0.5807675145024542\n",
      "Kappa: 0.3296440323063117\n",
      "Hamming Loss: 0.3253012048192771\n",
      "AUC0.6611111111111111\n",
      "Sensitivity0.5\n",
      "Specificity0.8222222222222222\n",
      "ICAP DT: 0.621802 (0.043727)\n",
      "\n",
      "Accuracy: 1.0\n",
      "Average Precision Score: 1.0\n",
      "Kappa: 1.0\n",
      "Hamming Loss: 0.0\n",
      "AUC: 1.0\n",
      "Sensitivity: 1.0\n",
      "Specificity: 1.0\n",
      "Accuracy: 0.6024096385542169\n",
      "Average Precision Score: 0.5127879940815896\n",
      "Kappa: 0.15955814667075785\n",
      "Hamming Loss: 0.39759036144578314\n",
      "AUC0.5760233918128654\n",
      "Sensitivity0.2631578947368421\n",
      "Specificity0.8888888888888888\n",
      "Accuracy: 0.6867469879518072\n",
      "Average Precision Score: 0.5928073195035782\n",
      "Kappa: 0.35582089552238805\n",
      "Hamming Loss: 0.3132530120481928\n",
      "AUC0.6742690058479531\n",
      "Sensitivity0.5263157894736842\n",
      "Specificity0.8222222222222222\n",
      "DISR DT: 0.631669 (0.024579)\n",
      "\n",
      "Accuracy: 1.0\n",
      "Average Precision Score: 1.0\n",
      "Kappa: 1.0\n",
      "Hamming Loss: 0.0\n",
      "AUC: 1.0\n",
      "Sensitivity: 1.0\n",
      "Specificity: 1.0\n",
      "Accuracy: 0.6746987951807228\n",
      "Average Precision Score: 0.5802093824213261\n",
      "Kappa: 0.3617203076046711\n",
      "Hamming Loss: 0.3253012048192771\n",
      "AUC0.685672514619883\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.5555555555555556\n",
      "Accuracy: 0.6506024096385542\n",
      "Average Precision Score: 0.5600986114166656\n",
      "Kappa: 0.3116957392050329\n",
      "Hamming Loss: 0.3493975903614458\n",
      "AUC0.6593567251461988\n",
      "Sensitivity0.7631578947368421\n",
      "Specificity0.5555555555555556\n",
      "MIM BY: 0.756424 (0.046384)\n",
      "\n",
      "Accuracy: 0.693939393939394\n",
      "Average Precision Score: 0.5882336694836695\n",
      "Kappa: 0.39430835211165227\n",
      "Hamming Loss: 0.30606060606060603\n",
      "AUC: 0.7013847638847639\n",
      "Sensitivity: 0.7736486486486487\n",
      "Specificity: 0.6291208791208791\n",
      "Accuracy: 0.6626506024096386\n",
      "Average Precision Score: 0.5674187979990135\n",
      "Kappa: 0.31767469172049323\n",
      "Hamming Loss: 0.3373493975903614\n",
      "AUC0.658187134502924\n",
      "Sensitivity0.6052631578947368\n",
      "Specificity0.7111111111111111\n",
      "Accuracy: 0.6385542168674698\n",
      "Average Precision Score: 0.5476220672162333\n",
      "Kappa: 0.2748980780430984\n",
      "Hamming Loss: 0.3614457831325301\n",
      "AUC0.6380116959064327\n",
      "Sensitivity0.631578947368421\n",
      "Specificity0.6444444444444445\n",
      "MIFS BY: 0.735409 (0.024977)\n",
      "\n",
      "Accuracy: 0.4772727272727273\n",
      "Average Precision Score: 0.4607946000632805\n",
      "Kappa: 0.043807636100985925\n",
      "Hamming Loss: 0.5227272727272727\n",
      "AUC: 0.5242055242055241\n",
      "Sensitivity: 0.9797297297297297\n",
      "Specificity: 0.06868131868131869\n",
      "Accuracy: 0.5180722891566265\n",
      "Average Precision Score: 0.46320265582453657\n",
      "Kappa: 0.021226415094339535\n",
      "Hamming Loss: 0.4819277108433735\n",
      "AUC0.5105263157894737\n",
      "Sensitivity0.42105263157894735\n",
      "Specificity0.6\n",
      "Accuracy: 0.5060240963855421\n",
      "Average Precision Score: 0.4690952174348363\n",
      "Kappa: 0.04221784407542917\n",
      "Hamming Loss: 0.4939759036144578\n",
      "AUC0.5219298245614036\n",
      "Sensitivity0.7105263157894737\n",
      "Specificity0.3333333333333333\n",
      "MRMR BY: 0.820718 (0.032517)\n",
      "\n",
      "Accuracy: 0.5636363636363636\n",
      "Average Precision Score: 0.5059807392124466\n",
      "Kappa: 0.18905082084712788\n",
      "Hamming Loss: 0.43636363636363634\n",
      "AUC: 0.6028177903177904\n",
      "Sensitivity: 0.9831081081081081\n",
      "Specificity: 0.22252747252747251\n",
      "Accuracy: 0.6746987951807228\n",
      "Average Precision Score: 0.5797367773996092\n",
      "Kappa: 0.3591649985702031\n",
      "Hamming Loss: 0.3253012048192771\n",
      "AUC0.6836257309941519\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.5777777777777777\n",
      "Accuracy: 0.5662650602409639\n",
      "Average Precision Score: 0.5124136898471078\n",
      "Kappa: 0.1831601968288683\n",
      "Hamming Loss: 0.43373493975903615\n",
      "AUC0.597953216374269\n",
      "Sensitivity0.9736842105263158\n",
      "Specificity0.2222222222222222\n",
      "CIFE BY: 0.775909 (0.033060)\n",
      "\n",
      "Accuracy: 0.7484848484848485\n",
      "Average Precision Score: 0.6426144619693006\n",
      "Kappa: 0.4938089077804473\n",
      "Hamming Loss: 0.2515151515151515\n",
      "AUC: 0.7479952479952481\n",
      "Sensitivity: 0.7432432432432432\n",
      "Specificity: 0.7527472527472527\n",
      "Accuracy: 0.5542168674698795\n",
      "Average Precision Score: 0.47209892200380466\n",
      "Kappa: 0.028472002530844587\n",
      "Hamming Loss: 0.4457831325301205\n",
      "AUC0.5131578947368421\n",
      "Sensitivity0.02631578947368421\n",
      "Specificity1.0\n",
      "Accuracy: 0.5301204819277109\n",
      "Average Precision Score: 0.4578313253012048\n",
      "Kappa: -0.024043024359380105\n",
      "Hamming Loss: 0.46987951807228917\n",
      "AUC0.4888888888888889\n",
      "Sensitivity0.0\n",
      "Specificity0.9777777777777777\n",
      "JMI BY: 0.771049 (0.026112)\n",
      "\n",
      "Accuracy: 0.5666666666666667\n",
      "Average Precision Score: 0.507376378459125\n",
      "Kappa: 0.19327817286652071\n",
      "Hamming Loss: 0.43333333333333335\n",
      "AUC: 0.6049339174339174\n",
      "Sensitivity: 0.9763513513513513\n",
      "Specificity: 0.23351648351648352\n",
      "Accuracy: 0.5421686746987951\n",
      "Average Precision Score: 0.4600507292327204\n",
      "Kappa: 0.00879949717159012\n",
      "Hamming Loss: 0.4578313253012048\n",
      "AUC0.5040935672514619\n",
      "Sensitivity0.05263157894736842\n",
      "Specificity0.9555555555555556\n",
      "Accuracy: 0.5180722891566265\n",
      "Average Precision Score: 0.4578313253012048\n",
      "Kappa: -0.04797979797979779\n",
      "Hamming Loss: 0.4819277108433735\n",
      "AUC0.4777777777777778\n",
      "Sensitivity0.0\n",
      "Specificity0.9555555555555556\n",
      "CMIM BY: 0.761916 (0.029579)\n",
      "\n",
      "Accuracy: 0.6136363636363636\n",
      "Average Precision Score: 0.5329486163153497\n",
      "Kappa: 0.26600143049037905\n",
      "Hamming Loss: 0.38636363636363635\n",
      "AUC: 0.6415206415206416\n",
      "Sensitivity: 0.9121621621621622\n",
      "Specificity: 0.3708791208791209\n",
      "Accuracy: 0.7349397590361446\n",
      "Average Precision Score: 0.6387303600366377\n",
      "Kappa: 0.46388725778038753\n",
      "Hamming Loss: 0.26506024096385544\n",
      "AUC0.7309941520467835\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.7777777777777778\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.657639045144926\n",
      "Kappa: 0.48294274695935924\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC0.7380116959064328\n",
      "Sensitivity0.631578947368421\n",
      "Specificity0.8444444444444444\n",
      "ICAP BY: 0.761916 (0.029579)\n",
      "\n",
      "Accuracy: 0.6136363636363636\n",
      "Average Precision Score: 0.5329486163153497\n",
      "Kappa: 0.26600143049037905\n",
      "Hamming Loss: 0.38636363636363635\n",
      "AUC: 0.6415206415206416\n",
      "Sensitivity: 0.9121621621621622\n",
      "Specificity: 0.3708791208791209\n",
      "Accuracy: 0.7349397590361446\n",
      "Average Precision Score: 0.6387303600366377\n",
      "Kappa: 0.46388725778038753\n",
      "Hamming Loss: 0.26506024096385544\n",
      "AUC0.7309941520467835\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.7777777777777778\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.657639045144926\n",
      "Kappa: 0.48294274695935924\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC0.7380116959064328\n",
      "Sensitivity0.631578947368421\n",
      "Specificity0.8444444444444444\n",
      "DISR BY: 0.785053 (0.042293)\n",
      "\n",
      "Accuracy: 0.7227272727272728\n",
      "Average Precision Score: 0.6159857854773109\n",
      "Kappa: 0.43932782471451115\n",
      "Hamming Loss: 0.2772727272727273\n",
      "AUC: 0.7195945945945945\n",
      "Sensitivity: 0.6891891891891891\n",
      "Specificity: 0.75\n",
      "Accuracy: 0.7108433734939759\n",
      "Average Precision Score: 0.6127223575743417\n",
      "Kappa: 0.4175438596491228\n",
      "Hamming Loss: 0.2891566265060241\n",
      "AUC0.7087719298245614\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.7333333333333333\n",
      "Accuracy: 0.7108433734939759\n",
      "Average Precision Score: 0.6114241655617686\n",
      "Kappa: 0.42461005199306767\n",
      "Hamming Loss: 0.2891566265060241\n",
      "AUC0.7149122807017545\n",
      "Sensitivity0.7631578947368421\n",
      "Specificity0.6666666666666666\n",
      "MIM SVM: 0.783045 (0.034564)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7075757575757575\n",
      "Average Precision Score: 0.6013134918795295\n",
      "Kappa: 0.4030368356921924\n",
      "Hamming Loss: 0.2924242424242424\n",
      "AUC: 0.6995470745470744\n",
      "Sensitivity: 0.6216216216216216\n",
      "Specificity: 0.7774725274725275\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.6446839991545128\n",
      "Kappa: 0.5035602392480775\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC0.7584795321637428\n",
      "Sensitivity0.8947368421052632\n",
      "Specificity0.6222222222222222\n",
      "Accuracy: 0.6867469879518072\n",
      "Average Precision Score: 0.5914258537911042\n",
      "Kappa: 0.39142695995487875\n",
      "Hamming Loss: 0.3132530120481928\n",
      "AUC0.7029239766081872\n",
      "Sensitivity0.8947368421052632\n",
      "Specificity0.5111111111111111\n",
      "MIFS SVM: 0.742335 (0.012526)\n",
      "\n",
      "Accuracy: 0.6\n",
      "Average Precision Score: 0.4959020709020709\n",
      "Kappa: 0.1334447362138934\n",
      "Hamming Loss: 0.4\n",
      "AUC: 0.5622586872586873\n",
      "Sensitivity: 0.19594594594594594\n",
      "Specificity: 0.9285714285714286\n",
      "Accuracy: 0.5662650602409639\n",
      "Average Precision Score: 0.48027901077996193\n",
      "Kappa: 0.07320099255583123\n",
      "Hamming Loss: 0.43373493975903615\n",
      "AUC0.5345029239766083\n",
      "Sensitivity0.15789473684210525\n",
      "Specificity0.9111111111111111\n",
      "Accuracy: 0.6024096385542169\n",
      "Average Precision Score: 0.51455583097942\n",
      "Kappa: 0.1522748375116062\n",
      "Hamming Loss: 0.39759036144578314\n",
      "AUC0.5719298245614035\n",
      "Sensitivity0.21052631578947367\n",
      "Specificity0.9333333333333333\n",
      "MRMR SVM: 0.836402 (0.031419)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7651515151515151\n",
      "Average Precision Score: 0.66510539010539\n",
      "Kappa: 0.5217927862231446\n",
      "Hamming Loss: 0.23484848484848486\n",
      "AUC: 0.7590028215028215\n",
      "Sensitivity: 0.6993243243243243\n",
      "Specificity: 0.8186813186813187\n",
      "Accuracy: 0.7831325301204819\n",
      "Average Precision Score: 0.6967716065500391\n",
      "Kappa: 0.5595518867924527\n",
      "Hamming Loss: 0.21686746987951808\n",
      "AUC0.777485380116959\n",
      "Sensitivity0.7105263157894737\n",
      "Specificity0.8444444444444444\n",
      "Accuracy: 0.7349397590361446\n",
      "Average Precision Score: 0.6387303600366377\n",
      "Kappa: 0.46388725778038753\n",
      "Hamming Loss: 0.26506024096385544\n",
      "AUC0.7309941520467835\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.7777777777777778\n",
      "CIFE SVM: 0.805298 (0.043641)\n",
      "\n",
      "Accuracy: 0.7196969696969697\n",
      "Average Precision Score: 0.6168313455867104\n",
      "Kappa: 0.4218859491297513\n",
      "Hamming Loss: 0.2803030303030303\n",
      "AUC: 0.7067493317493319\n",
      "Sensitivity: 0.581081081081081\n",
      "Specificity: 0.8324175824175825\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.6222689802271286\n",
      "Kappa: 0.46058208533484035\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC0.7383040935672516\n",
      "Sensitivity0.9210526315789473\n",
      "Specificity0.5555555555555556\n",
      "Accuracy: 0.6626506024096386\n",
      "Average Precision Score: 0.5719890841561736\n",
      "Kappa: 0.3446136491821772\n",
      "Hamming Loss: 0.3373493975903614\n",
      "AUC0.678654970760234\n",
      "Sensitivity0.868421052631579\n",
      "Specificity0.4888888888888889\n",
      "JMI SVM: 0.777377 (0.032765)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7045454545454546\n",
      "Average Precision Score: 0.6002857483033694\n",
      "Kappa: 0.3894571054479212\n",
      "Hamming Loss: 0.29545454545454547\n",
      "AUC: 0.6904885654885653\n",
      "Sensitivity: 0.5540540540540541\n",
      "Specificity: 0.8269230769230769\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.625374899312756\n",
      "Kappa: 0.44066803398769405\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC0.7198830409356725\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.7555555555555555\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.6528489899447414\n",
      "Kappa: 0.4872021182700794\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC0.7421052631578947\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.8\n",
      "CMIM SVM: 0.766104 (0.035016)\n",
      "\n",
      "Accuracy: 0.6818181818181818\n",
      "Average Precision Score: 0.5778457079238329\n",
      "Kappa: 0.3349838783970521\n",
      "Hamming Loss: 0.3181818181818182\n",
      "AUC: 0.6619950994950994\n",
      "Sensitivity: 0.46959459459459457\n",
      "Specificity: 0.8543956043956044\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.6690298034242232\n",
      "Kappa: 0.47641934514869333\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC0.7318713450292398\n",
      "Sensitivity0.5526315789473685\n",
      "Specificity0.9111111111111111\n",
      "Accuracy: 0.6746987951807228\n",
      "Average Precision Score: 0.5887427827654107\n",
      "Kappa: 0.3182233039245512\n",
      "Hamming Loss: 0.3253012048192771\n",
      "AUC0.6529239766081872\n",
      "Sensitivity0.39473684210526316\n",
      "Specificity0.9111111111111111\n",
      "ICAP SVM: 0.766104 (0.035016)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6818181818181818\n",
      "Average Precision Score: 0.5778457079238329\n",
      "Kappa: 0.3349838783970521\n",
      "Hamming Loss: 0.3181818181818182\n",
      "AUC: 0.6619950994950994\n",
      "Sensitivity: 0.46959459459459457\n",
      "Specificity: 0.8543956043956044\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.6690298034242232\n",
      "Kappa: 0.47641934514869333\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC0.7318713450292398\n",
      "Sensitivity0.5526315789473685\n",
      "Specificity0.9111111111111111\n",
      "Accuracy: 0.6746987951807228\n",
      "Average Precision Score: 0.5887427827654107\n",
      "Kappa: 0.3182233039245512\n",
      "Hamming Loss: 0.3253012048192771\n",
      "AUC0.6529239766081872\n",
      "Sensitivity0.39473684210526316\n",
      "Specificity0.9111111111111111\n",
      "DISR SVM: 0.795327 (0.036453)\n",
      "\n",
      "Accuracy: 0.7257575757575757\n",
      "Average Precision Score: 0.6196129143497564\n",
      "Kappa: 0.44369935736239174\n",
      "Hamming Loss: 0.27424242424242423\n",
      "AUC: 0.7210795960795959\n",
      "Sensitivity: 0.6756756756756757\n",
      "Specificity: 0.7664835164835165\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.6494967725151619\n",
      "Kappa: 0.4913918879486431\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC0.7461988304093568\n",
      "Sensitivity0.7368421052631579\n",
      "Specificity0.7555555555555555\n",
      "Accuracy: 0.7349397590361446\n",
      "Average Precision Score: 0.6362714013950539\n",
      "Kappa: 0.4682585905649388\n",
      "Hamming Loss: 0.26506024096385544\n",
      "AUC0.7350877192982457\n",
      "Sensitivity0.7368421052631579\n",
      "Specificity0.7333333333333333\n",
      "MIM BAG: 0.760249 (0.028554)\n",
      "\n",
      "Accuracy: 0.9803030303030303\n",
      "Average Precision Score: 0.9713378052478397\n",
      "Kappa: 0.9600952504976467\n",
      "Hamming Loss: 0.019696969696969695\n",
      "AUC: 0.978987228987229\n",
      "Sensitivity: 0.9662162162162162\n",
      "Specificity: 0.9917582417582418\n",
      "Accuracy: 0.6626506024096386\n",
      "Average Precision Score: 0.5731610653138871\n",
      "Kappa: 0.2944748026715239\n",
      "Hamming Loss: 0.3373493975903614\n",
      "AUC0.641812865497076\n",
      "Sensitivity0.39473684210526316\n",
      "Specificity0.8888888888888888\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.6089156626506024\n",
      "Kappa: 0.37668969660558727\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC0.6833333333333333\n",
      "Sensitivity0.5\n",
      "Specificity0.8666666666666667\n",
      "MIFS BAG: 0.688867 (0.015711)\n",
      "\n",
      "Accuracy: 0.9833333333333333\n",
      "Average Precision Score: 0.9795045045045045\n",
      "Kappa: 0.9661916736518581\n",
      "Hamming Loss: 0.016666666666666666\n",
      "AUC: 0.9814189189189189\n",
      "Sensitivity: 0.9628378378378378\n",
      "Specificity: 1.0\n",
      "Accuracy: 0.6024096385542169\n",
      "Average Precision Score: 0.5126077063672647\n",
      "Kappa: 0.16315307057745188\n",
      "Hamming Loss: 0.39759036144578314\n",
      "AUC0.5780701754385965\n",
      "Sensitivity0.2894736842105263\n",
      "Specificity0.8666666666666667\n",
      "Accuracy: 0.6144578313253012\n",
      "Average Precision Score: 0.5271876981610654\n",
      "Kappa: 0.22019964768056377\n",
      "Hamming Loss: 0.3855421686746988\n",
      "AUC0.6096491228070178\n",
      "Sensitivity0.5526315789473685\n",
      "Specificity0.6666666666666666\n",
      "MRMR BAG: 0.804407 (0.028782)\n",
      "\n",
      "Accuracy: 0.9863636363636363\n",
      "Average Precision Score: 0.9773535862518913\n",
      "Kappa: 0.9724259585925169\n",
      "Hamming Loss: 0.013636363636363636\n",
      "AUC: 0.9860595485595486\n",
      "Sensitivity: 0.9831081081081081\n",
      "Specificity: 0.989010989010989\n",
      "Accuracy: 0.7590361445783133\n",
      "Average Precision Score: 0.6566138237159164\n",
      "Kappa: 0.5262557077625571\n",
      "Hamming Loss: 0.24096385542168675\n",
      "AUC0.7695906432748538\n",
      "Sensitivity0.8947368421052632\n",
      "Specificity0.6444444444444445\n",
      "Accuracy: 0.7108433734939759\n",
      "Average Precision Score: 0.6112596840451049\n",
      "Kappa: 0.42692750287687\n",
      "Hamming Loss: 0.2891566265060241\n",
      "AUC0.7169590643274854\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.6444444444444445\n",
      "CIFE BAG: 0.810409 (0.041676)\n",
      "\n",
      "Accuracy: 0.9848484848484849\n",
      "Average Precision Score: 0.9769264049229802\n",
      "Kappa: 0.9693331350828934\n",
      "Hamming Loss: 0.015151515151515152\n",
      "AUC: 0.9840547965547966\n",
      "Sensitivity: 0.9763513513513513\n",
      "Specificity: 0.9917582417582418\n",
      "Accuracy: 0.6385542168674698\n",
      "Average Precision Score: 0.5510653138871274\n",
      "Kappa: 0.2893835616438356\n",
      "Hamming Loss: 0.3614457831325301\n",
      "AUC0.6482456140350877\n",
      "Sensitivity0.7631578947368421\n",
      "Specificity0.5333333333333333\n",
      "Accuracy: 0.6144578313253012\n",
      "Average Precision Score: 0.5331135066582119\n",
      "Kappa: 0.24200913242009126\n",
      "Hamming Loss: 0.3855421686746988\n",
      "AUC0.6239766081871345\n",
      "Sensitivity0.7368421052631579\n",
      "Specificity0.5111111111111111\n",
      "JMI BAG: 0.742776 (0.013614)\n",
      "\n",
      "Accuracy: 0.9787878787878788\n",
      "Average Precision Score: 0.970931770931771\n",
      "Kappa: 0.956984842277755\n",
      "Hamming Loss: 0.021212121212121213\n",
      "AUC: 0.976982476982477\n",
      "Sensitivity: 0.9594594594594594\n",
      "Specificity: 0.9945054945054945\n",
      "Accuracy: 0.46987951807228917\n",
      "Average Precision Score: 0.43971374218679227\n",
      "Kappa: -0.12300123001230023\n",
      "Hamming Loss: 0.5301204819277109\n",
      "AUC0.44152046783625726\n",
      "Sensitivity0.10526315789473684\n",
      "Specificity0.7777777777777778\n",
      "Accuracy: 0.6144578313253012\n",
      "Average Precision Score: 0.5238189600507293\n",
      "Kappa: 0.2004816375677302\n",
      "Hamming Loss: 0.3855421686746988\n",
      "AUC0.5973684210526317\n",
      "Sensitivity0.39473684210526316\n",
      "Specificity0.8\n",
      "CMIM BAG: 0.728254 (0.024202)\n",
      "\n",
      "Accuracy: 0.9878787878787879\n",
      "Average Precision Score: 0.9850941850941851\n",
      "Kappa: 0.9754354622599375\n",
      "Hamming Loss: 0.012121212121212121\n",
      "AUC: 0.9864864864864865\n",
      "Sensitivity: 0.972972972972973\n",
      "Specificity: 1.0\n",
      "Accuracy: 0.6144578313253012\n",
      "Average Precision Score: 0.543436905516804\n",
      "Kappa: 0.16896120150187743\n",
      "Hamming Loss: 0.3855421686746988\n",
      "AUC0.5789473684210527\n",
      "Sensitivity0.15789473684210525\n",
      "Specificity1.0\n",
      "Accuracy: 0.5783132530120482\n",
      "Average Precision Score: 0.49071885628638956\n",
      "Kappa: 0.10089755493655206\n",
      "Hamming Loss: 0.42168674698795183\n",
      "AUC0.5476608187134502\n",
      "Sensitivity0.18421052631578946\n",
      "Specificity0.9111111111111111\n",
      "ICAP BAG: 0.735986 (0.033759)\n",
      "\n",
      "Accuracy: 0.9818181818181818\n",
      "Average Precision Score: 0.9746578965328966\n",
      "Kappa: 0.9631531933899062\n",
      "Hamming Loss: 0.01818181818181818\n",
      "AUC: 0.9803608553608554\n",
      "Sensitivity: 0.9662162162162162\n",
      "Specificity: 0.9945054945054945\n",
      "Accuracy: 0.5783132530120482\n",
      "Average Precision Score: 0.5006341154090044\n",
      "Kappa: 0.08503937007874018\n",
      "Hamming Loss: 0.42168674698795183\n",
      "AUC0.5394736842105263\n",
      "Sensitivity0.07894736842105263\n",
      "Specificity1.0\n",
      "Accuracy: 0.5662650602409639\n",
      "Average Precision Score: 0.48089727330374127\n",
      "Kappa: 0.0609679446888749\n",
      "Hamming Loss: 0.43373493975903615\n",
      "AUC0.5283625730994153\n",
      "Sensitivity0.07894736842105263\n",
      "Specificity0.9777777777777777\n",
      "DISR BAG: 0.746326 (0.031064)\n",
      "\n",
      "Accuracy: 0.9696969696969697\n",
      "Average Precision Score: 0.954066850941851\n",
      "Kappa: 0.9385886556498437\n",
      "Hamming Loss: 0.030303030303030304\n",
      "AUC: 0.968109593109593\n",
      "Sensitivity: 0.9527027027027027\n",
      "Specificity: 0.9835164835164835\n",
      "Accuracy: 0.5903614457831325\n",
      "Average Precision Score: 0.5092651306982315\n",
      "Kappa: 0.17146212566059904\n",
      "Hamming Loss: 0.40963855421686746\n",
      "AUC0.5853801169590642\n",
      "Sensitivity0.5263157894736842\n",
      "Specificity0.6444444444444445\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.6297891054881666\n",
      "Kappa: 0.43369919905072685\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC0.7137426900584795\n",
      "Sensitivity0.6052631578947368\n",
      "Specificity0.8222222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIM NNet: 0.810278 (0.020335)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7606060606060606\n",
      "Average Precision Score: 0.6565869903618247\n",
      "Kappa: 0.5169896616889613\n",
      "Hamming Loss: 0.23939393939393938\n",
      "AUC: 0.758984258984259\n",
      "Sensitivity: 0.7432432432432432\n",
      "Specificity: 0.7747252747252747\n",
      "Accuracy: 0.7108433734939759\n",
      "Average Precision Score: 0.6127223575743417\n",
      "Kappa: 0.4175438596491228\n",
      "Hamming Loss: 0.2891566265060241\n",
      "AUC0.7087719298245614\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.7333333333333333\n",
      "Accuracy: 0.7590361445783133\n",
      "Average Precision Score: 0.663418215799486\n",
      "Kappa: 0.5146198830409356\n",
      "Hamming Loss: 0.24096385542168675\n",
      "AUC0.7573099415204678\n",
      "Sensitivity0.7368421052631579\n",
      "Specificity0.7777777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIFS NNet: 0.764546 (0.021365)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7121212121212122\n",
      "Average Precision Score: 0.6070821279490635\n",
      "Kappa: 0.4091148974668275\n",
      "Hamming Loss: 0.2878787878787879\n",
      "AUC: 0.701459013959014\n",
      "Sensitivity: 0.597972972972973\n",
      "Specificity: 0.804945054945055\n",
      "Accuracy: 0.6746987951807228\n",
      "Average Precision Score: 0.5807300702312727\n",
      "Kappa: 0.36425531914893616\n",
      "Hamming Loss: 0.3253012048192771\n",
      "AUC0.687719298245614\n",
      "Sensitivity0.8421052631578947\n",
      "Specificity0.5333333333333333\n",
      "Accuracy: 0.6385542168674698\n",
      "Average Precision Score: 0.5560936445271749\n",
      "Kappa: 0.30602006688963224\n",
      "Hamming Loss: 0.3614457831325301\n",
      "AUC0.6605263157894736\n",
      "Sensitivity0.9210526315789473\n",
      "Specificity0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRMR NNet: 0.856624 (0.026605)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.796969696969697\n",
      "Average Precision Score: 0.7021672520792239\n",
      "Kappa: 0.5880226578221659\n",
      "Hamming Loss: 0.20303030303030303\n",
      "AUC: 0.7928979803979804\n",
      "Sensitivity: 0.7533783783783784\n",
      "Specificity: 0.8324175824175825\n",
      "Accuracy: 0.7710843373493976\n",
      "Average Precision Score: 0.6759101182056161\n",
      "Kappa: 0.5398307557630581\n",
      "Hamming Loss: 0.2289156626506024\n",
      "AUC0.7704678362573099\n",
      "Sensitivity0.7631578947368421\n",
      "Specificity0.7777777777777778\n",
      "Accuracy: 0.7590361445783133\n",
      "Average Precision Score: 0.6581015136059111\n",
      "Kappa: 0.522439585730725\n",
      "Hamming Loss: 0.24096385542168675\n",
      "AUC0.7654970760233917\n",
      "Sensitivity0.8421052631578947\n",
      "Specificity0.6888888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFE NNet: 0.836274 (0.024860)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7909090909090909\n",
      "Average Precision Score: 0.6968652894204364\n",
      "Kappa: 0.5741059404459075\n",
      "Hamming Loss: 0.20909090909090908\n",
      "AUC: 0.7848789723789723\n",
      "Sensitivity: 0.7263513513513513\n",
      "Specificity: 0.8434065934065934\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.6021516933030964\n",
      "Kappa: 0.41827866554527615\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC0.7181286549707602\n",
      "Sensitivity0.9473684210526315\n",
      "Specificity0.4888888888888889\n",
      "Accuracy: 0.6626506024096386\n",
      "Average Precision Score: 0.5713144977571103\n",
      "Kappa: 0.3420158550396376\n",
      "Hamming Loss: 0.3373493975903614\n",
      "AUC0.6766081871345029\n",
      "Sensitivity0.8421052631578947\n",
      "Specificity0.5111111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JMI NNet: 0.809015 (0.025627)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.746969696969697\n",
      "Average Precision Score: 0.6422840835211969\n",
      "Kappa: 0.4877014892074293\n",
      "Hamming Loss: 0.25303030303030305\n",
      "AUC: 0.7434659934659934\n",
      "Sensitivity: 0.7094594594594594\n",
      "Specificity: 0.7774725274725275\n",
      "Accuracy: 0.6626506024096386\n",
      "Average Precision Score: 0.579165345592898\n",
      "Kappa: 0.288426209430496\n",
      "Hamming Loss: 0.3373493975903614\n",
      "AUC0.637719298245614\n",
      "Sensitivity0.34210526315789475\n",
      "Specificity0.9333333333333333\n",
      "Accuracy: 0.6867469879518072\n",
      "Average Precision Score: 0.5943002780352178\n",
      "Kappa: 0.35311750599520386\n",
      "Hamming Loss: 0.3132530120481928\n",
      "AUC0.6722222222222222\n",
      "Sensitivity0.5\n",
      "Specificity0.8444444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMIM NNet: 0.793929 (0.025993)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7348484848484849\n",
      "Average Precision Score: 0.6302618552618553\n",
      "Kappa: 0.4600886296067762\n",
      "Hamming Loss: 0.26515151515151514\n",
      "AUC: 0.7283746658746659\n",
      "Sensitivity: 0.6655405405405406\n",
      "Specificity: 0.7912087912087912\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.6196308780829689\n",
      "Kappa: 0.36872528141162153\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC0.6771929824561403\n",
      "Sensitivity0.42105263157894735\n",
      "Specificity0.9333333333333333\n",
      "Accuracy: 0.6626506024096386\n",
      "Average Precision Score: 0.579165345592898\n",
      "Kappa: 0.288426209430496\n",
      "Hamming Loss: 0.3373493975903614\n",
      "AUC0.637719298245614\n",
      "Sensitivity0.34210526315789475\n",
      "Specificity0.9333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICAP NNet: 0.789785 (0.025762)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7318181818181818\n",
      "Average Precision Score: 0.6258804258804259\n",
      "Kappa: 0.45668148754488114\n",
      "Hamming Loss: 0.2681818181818182\n",
      "AUC: 0.7278363528363527\n",
      "Sensitivity: 0.6891891891891891\n",
      "Specificity: 0.7664835164835165\n",
      "Accuracy: 0.6867469879518072\n",
      "Average Precision Score: 0.6060558021559924\n",
      "Kappa: 0.3420731707317073\n",
      "Hamming Loss: 0.3132530120481928\n",
      "AUC0.6640350877192983\n",
      "Sensitivity0.39473684210526316\n",
      "Specificity0.9333333333333333\n",
      "Accuracy: 0.6626506024096386\n",
      "Average Precision Score: 0.5839297037775161\n",
      "Kappa: 0.2853628536285362\n",
      "Hamming Loss: 0.3373493975903614\n",
      "AUC0.6356725146198832\n",
      "Sensitivity0.3157894736842105\n",
      "Specificity0.9555555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISR NNet: 0.803598 (0.037243)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbai/miniconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7257575757575757\n",
      "Average Precision Score: 0.6201678951678952\n",
      "Kappa: 0.4419321685508736\n",
      "Hamming Loss: 0.27424242424242423\n",
      "AUC: 0.719501782001782\n",
      "Sensitivity: 0.6587837837837838\n",
      "Specificity: 0.7802197802197802\n",
      "Accuracy: 0.7710843373493976\n",
      "Average Precision Score: 0.6759101182056161\n",
      "Kappa: 0.5398307557630581\n",
      "Hamming Loss: 0.2289156626506024\n",
      "AUC0.7704678362573099\n",
      "Sensitivity0.7631578947368421\n",
      "Specificity0.7777777777777778\n",
      "Accuracy: 0.7710843373493976\n",
      "Average Precision Score: 0.6740492135422305\n",
      "Kappa: 0.5417029933158966\n",
      "Hamming Loss: 0.2289156626506024\n",
      "AUC0.7725146198830409\n",
      "Sensitivity0.7894736842105263\n",
      "Specificity0.7555555555555555\n",
      "MIM RF: 0.769053 (0.031685)\n",
      "\n",
      "Accuracy: 0.9848484848484849\n",
      "Average Precision Score: 0.9754909826338398\n",
      "Kappa: 0.9693525019502953\n",
      "Hamming Loss: 0.015151515151515152\n",
      "AUC: 0.9843703593703595\n",
      "Sensitivity: 0.9797297297297297\n",
      "Specificity: 0.989010989010989\n",
      "Accuracy: 0.5662650602409639\n",
      "Average Precision Score: 0.5058285774677659\n",
      "Kappa: 0.1639619473978735\n",
      "Hamming Loss: 0.43373493975903615\n",
      "AUC0.5856725146198831\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.35555555555555557\n",
      "Accuracy: 0.5662650602409639\n",
      "Average Precision Score: 0.5036405018570523\n",
      "Kappa: 0.15736040609137059\n",
      "Hamming Loss: 0.43373493975903615\n",
      "AUC0.581578947368421\n",
      "Sensitivity0.7631578947368421\n",
      "Specificity0.4\n",
      "MIFS RF: 0.720339 (0.008514)\n",
      "\n",
      "Accuracy: 0.9712121212121212\n",
      "Average Precision Score: 0.9573172158538014\n",
      "Kappa: 0.9416407602524246\n",
      "Hamming Loss: 0.02878787878787879\n",
      "AUC: 0.9694832194832196\n",
      "Sensitivity: 0.9527027027027027\n",
      "Specificity: 0.9862637362637363\n",
      "Accuracy: 0.7108433734939759\n",
      "Average Precision Score: 0.6121353836398225\n",
      "Kappa: 0.4199184624344787\n",
      "Hamming Loss: 0.2891566265060241\n",
      "AUC0.7108187134502923\n",
      "Sensitivity0.7105263157894737\n",
      "Specificity0.7111111111111111\n",
      "Accuracy: 0.7108433734939759\n",
      "Average Precision Score: 0.6127223575743417\n",
      "Kappa: 0.4175438596491228\n",
      "Hamming Loss: 0.2891566265060241\n",
      "AUC0.7087719298245614\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.7333333333333333\n",
      "MRMR RF: 0.808151 (0.024176)\n",
      "\n",
      "Accuracy: 0.990909090909091\n",
      "Average Precision Score: 0.9888206388206389\n",
      "Kappa: 0.9815882462339595\n",
      "Hamming Loss: 0.00909090909090909\n",
      "AUC: 0.9898648648648649\n",
      "Sensitivity: 0.9797297297297297\n",
      "Specificity: 1.0\n",
      "Accuracy: 0.7590361445783133\n",
      "Average Precision Score: 0.6677981274944981\n",
      "Kappa: 0.5106132075471698\n",
      "Hamming Loss: 0.24096385542168675\n",
      "AUC0.7532163742690058\n",
      "Sensitivity0.6842105263157895\n",
      "Specificity0.8222222222222222\n",
      "Accuracy: 0.7108433734939759\n",
      "Average Precision Score: 0.6112013844853097\n",
      "Kappa: 0.4292263610315187\n",
      "Hamming Loss: 0.2891566265060241\n",
      "AUC0.7190058479532164\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.6222222222222222\n",
      "CIFE RF: 0.801311 (0.059882)\n",
      "\n",
      "Accuracy: 0.9878787878787879\n",
      "Average Precision Score: 0.9850941850941851\n",
      "Kappa: 0.9754354622599375\n",
      "Hamming Loss: 0.012121212121212121\n",
      "AUC: 0.9864864864864865\n",
      "Sensitivity: 0.972972972972973\n",
      "Specificity: 1.0\n",
      "Accuracy: 0.5542168674698795\n",
      "Average Precision Score: 0.4801897649075835\n",
      "Kappa: 0.08136404427161226\n",
      "Hamming Loss: 0.4457831325301205\n",
      "AUC0.5397660818713451\n",
      "Sensitivity0.3684210526315789\n",
      "Specificity0.7111111111111111\n",
      "Accuracy: 0.6867469879518072\n",
      "Average Precision Score: 0.5987058280970773\n",
      "Kappa: 0.3476420798065296\n",
      "Hamming Loss: 0.3132530120481928\n",
      "AUC0.6681286549707602\n",
      "Sensitivity0.4473684210526316\n",
      "Specificity0.8888888888888888\n",
      "JMI RF: 0.744819 (0.029308)\n",
      "\n",
      "Accuracy: 0.9863636363636363\n",
      "Average Precision Score: 0.9817274966063894\n",
      "Kappa: 0.9723736349599092\n",
      "Hamming Loss: 0.013636363636363636\n",
      "AUC: 0.9851128601128601\n",
      "Sensitivity: 0.972972972972973\n",
      "Specificity: 0.9972527472527473\n",
      "Accuracy: 0.5783132530120482\n",
      "Average Precision Score: 0.49150285351934053\n",
      "Kappa: 0.10862227677201597\n",
      "Hamming Loss: 0.42168674698795183\n",
      "AUC0.5517543859649123\n",
      "Sensitivity0.23684210526315788\n",
      "Specificity0.8666666666666667\n",
      "Accuracy: 0.5783132530120482\n",
      "Average Precision Score: 0.4908053265694356\n",
      "Kappa: 0.09698476841778059\n",
      "Hamming Loss: 0.42168674698795183\n",
      "AUC0.5456140350877193\n",
      "Sensitivity0.15789473684210525\n",
      "Specificity0.9333333333333333\n",
      "CMIM RF: 0.754516 (0.024368)\n",
      "\n",
      "Accuracy: 0.9757575757575757\n",
      "Average Precision Score: 0.9686851987383903\n",
      "Kappa: 0.9507774918894731\n",
      "Hamming Loss: 0.024242424242424242\n",
      "AUC: 0.9732885357885358\n",
      "Sensitivity: 0.9493243243243243\n",
      "Specificity: 0.9972527472527473\n",
      "Accuracy: 0.6746987951807228\n",
      "Average Precision Score: 0.5820164870006341\n",
      "Kappa: 0.32682487233403423\n",
      "Hamming Loss: 0.3253012048192771\n",
      "AUC0.6590643274853801\n",
      "Sensitivity0.47368421052631576\n",
      "Specificity0.8444444444444444\n",
      "Accuracy: 0.5783132530120482\n",
      "Average Precision Score: 0.4954724159797083\n",
      "Kappa: 0.1273655752478221\n",
      "Hamming Loss: 0.42168674698795183\n",
      "AUC0.5619883040935673\n",
      "Sensitivity0.3684210526315789\n",
      "Specificity0.7555555555555555\n",
      "ICAP RF: 0.748728 (0.033013)\n",
      "\n",
      "Accuracy: 0.9833333333333333\n",
      "Average Precision Score: 0.9795045045045045\n",
      "Kappa: 0.9661916736518581\n",
      "Hamming Loss: 0.016666666666666666\n",
      "AUC: 0.9814189189189189\n",
      "Sensitivity: 0.9628378378378378\n",
      "Specificity: 1.0\n",
      "Accuracy: 0.6024096385542169\n",
      "Average Precision Score: 0.5184750381326158\n",
      "Kappa: 0.1974802226779958\n",
      "Hamming Loss: 0.39759036144578314\n",
      "AUC0.5985380116959064\n",
      "Sensitivity0.5526315789473685\n",
      "Specificity0.6444444444444445\n",
      "Accuracy: 0.5903614457831325\n",
      "Average Precision Score: 0.5111921369689284\n",
      "Kappa: 0.17821782178217815\n",
      "Hamming Loss: 0.40963855421686746\n",
      "AUC0.5894736842105263\n",
      "Sensitivity0.5789473684210527\n",
      "Specificity0.6\n",
      "DISR RF: 0.747532 (0.030195)\n",
      "\n",
      "Accuracy: 0.9666666666666667\n",
      "Average Precision Score: 0.9531400839911478\n",
      "Kappa: 0.9323190513480255\n",
      "Hamming Loss: 0.03333333333333333\n",
      "AUC: 0.964100089100089\n",
      "Sensitivity: 0.9391891891891891\n",
      "Specificity: 0.989010989010989\n",
      "Accuracy: 0.4939759036144578\n",
      "Average Precision Score: 0.4707073909673783\n",
      "Kappa: 0.04702022963367969\n",
      "Hamming Loss: 0.5060240963855421\n",
      "AUC0.5251461988304094\n",
      "Sensitivity0.8947368421052632\n",
      "Specificity0.15555555555555556\n",
      "Accuracy: 0.5542168674698795\n",
      "Average Precision Score: 0.5000251633098811\n",
      "Kappa: 0.1457579972183588\n",
      "Hamming Loss: 0.4457831325301205\n",
      "AUC0.5766081871345029\n",
      "Sensitivity0.8421052631578947\n",
      "Specificity0.3111111111111111\n",
      "MIM BST: 0.772199 (0.013514)\n",
      "\n",
      "Accuracy: 0.8227272727272728\n",
      "Average Precision Score: 0.7322049322049322\n",
      "Kappa: 0.6417636252296387\n",
      "Hamming Loss: 0.17727272727272728\n",
      "AUC: 0.820983070983071\n",
      "Sensitivity: 0.8040540540540541\n",
      "Specificity: 0.8379120879120879\n",
      "Accuracy: 0.5783132530120482\n",
      "Average Precision Score: 0.4908053265694356\n",
      "Kappa: 0.09698476841778059\n",
      "Hamming Loss: 0.42168674698795183\n",
      "AUC0.5456140350877193\n",
      "Sensitivity0.15789473684210525\n",
      "Specificity0.9333333333333333\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.6319725362429756\n",
      "Kappa: 0.4313375037235626\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC0.7116959064327485\n",
      "Sensitivity0.5789473684210527\n",
      "Specificity0.8444444444444444\n",
      "MIFS BST: 0.726508 (0.018814)\n",
      "\n",
      "Accuracy: 0.759090909090909\n",
      "Average Precision Score: 0.6595641880124639\n",
      "Kappa: 0.5075736246410271\n",
      "Hamming Loss: 0.2409090909090909\n",
      "AUC: 0.7509838134838135\n",
      "Sensitivity: 0.6722972972972973\n",
      "Specificity: 0.8296703296703297\n",
      "Accuracy: 0.6265060240963856\n",
      "Average Precision Score: 0.5407048125311998\n",
      "Kappa: 0.26126902095894344\n",
      "Hamming Loss: 0.37349397590361444\n",
      "AUC0.6330409356725146\n",
      "Sensitivity0.7105263157894737\n",
      "Specificity0.5555555555555556\n",
      "Accuracy: 0.6867469879518072\n",
      "Average Precision Score: 0.590797400126823\n",
      "Kappa: 0.36116044997039665\n",
      "Hamming Loss: 0.3132530120481928\n",
      "AUC0.6783625730994152\n",
      "Sensitivity0.5789473684210527\n",
      "Specificity0.7777777777777778\n",
      "MRMR BST: 0.836804 (0.023555)\n",
      "\n",
      "Accuracy: 0.853030303030303\n",
      "Average Precision Score: 0.7700782749963078\n",
      "Kappa: 0.7037482646922721\n",
      "Hamming Loss: 0.14696969696969697\n",
      "AUC: 0.8528734778734779\n",
      "Sensitivity: 0.8513513513513513\n",
      "Specificity: 0.8543956043956044\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.6009559588901784\n",
      "Kappa: 0.4113475177304965\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC0.7119883040935673\n",
      "Sensitivity0.868421052631579\n",
      "Specificity0.5555555555555556\n",
      "Accuracy: 0.6746987951807228\n",
      "Average Precision Score: 0.5802093824213261\n",
      "Kappa: 0.3617203076046711\n",
      "Hamming Loss: 0.3253012048192771\n",
      "AUC0.685672514619883\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.5555555555555556\n",
      "CIFE BST: 0.824981 (0.037705)\n",
      "\n",
      "Accuracy: 0.8636363636363636\n",
      "Average Precision Score: 0.7859735872235872\n",
      "Kappa: 0.724694104560623\n",
      "Hamming Loss: 0.13636363636363635\n",
      "AUC: 0.8628044253044254\n",
      "Sensitivity: 0.8547297297297297\n",
      "Specificity: 0.8708791208791209\n",
      "Accuracy: 0.7590361445783133\n",
      "Average Precision Score: 0.6590981149478297\n",
      "Kappa: 0.5205083766608898\n",
      "Hamming Loss: 0.24096385542168675\n",
      "AUC0.7634502923976608\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.7111111111111111\n",
      "Accuracy: 0.7469879518072289\n",
      "Average Precision Score: 0.6463256534911577\n",
      "Kappa: 0.49754972614586335\n",
      "Hamming Loss: 0.25301204819277107\n",
      "AUC0.7523391812865498\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.6888888888888889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JMI BST: 0.751993 (0.044356)\n",
      "\n",
      "Accuracy: 0.8106060606060606\n",
      "Average Precision Score: 0.7205879633298988\n",
      "Kappa: 0.6150831420413191\n",
      "Hamming Loss: 0.1893939393939394\n",
      "AUC: 0.8058917433917433\n",
      "Sensitivity: 0.7601351351351351\n",
      "Specificity: 0.8516483516483516\n",
      "Accuracy: 0.5783132530120482\n",
      "Average Precision Score: 0.5006341154090044\n",
      "Kappa: 0.08503937007874018\n",
      "Hamming Loss: 0.42168674698795183\n",
      "AUC0.5394736842105263\n",
      "Sensitivity0.07894736842105263\n",
      "Specificity1.0\n",
      "Accuracy: 0.6987951807228916\n",
      "Average Precision Score: 0.6254056473572307\n",
      "Kappa: 0.3660250534677666\n",
      "Hamming Loss: 0.30120481927710846\n",
      "AUC0.6751461988304095\n",
      "Sensitivity0.39473684210526316\n",
      "Specificity0.9555555555555556\n",
      "CMIM BST: 0.756432 (0.014082)\n",
      "\n",
      "Accuracy: 0.793939393939394\n",
      "Average Precision Score: 0.6973357762150865\n",
      "Kappa: 0.5826669146364143\n",
      "Hamming Loss: 0.20606060606060606\n",
      "AUC: 0.7907818532818532\n",
      "Sensitivity: 0.7601351351351351\n",
      "Specificity: 0.8214285714285714\n",
      "Accuracy: 0.6144578313253012\n",
      "Average Precision Score: 0.5238189600507293\n",
      "Kappa: 0.2004816375677302\n",
      "Hamming Loss: 0.3855421686746988\n",
      "AUC0.5973684210526317\n",
      "Sensitivity0.39473684210526316\n",
      "Specificity0.8\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.6280048423358506\n",
      "Kappa: 0.43604135893648455\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC0.7157894736842105\n",
      "Sensitivity0.631578947368421\n",
      "Specificity0.8\n",
      "ICAP BST: 0.756432 (0.014082)\n",
      "\n",
      "Accuracy: 0.793939393939394\n",
      "Average Precision Score: 0.6973357762150865\n",
      "Kappa: 0.5826669146364143\n",
      "Hamming Loss: 0.20606060606060606\n",
      "AUC: 0.7907818532818532\n",
      "Sensitivity: 0.7601351351351351\n",
      "Specificity: 0.8214285714285714\n",
      "Accuracy: 0.6144578313253012\n",
      "Average Precision Score: 0.5251743817374762\n",
      "Kappa: 0.21046373365041615\n",
      "Hamming Loss: 0.3855421686746988\n",
      "AUC0.6035087719298246\n",
      "Sensitivity0.47368421052631576\n",
      "Specificity0.7333333333333333\n",
      "Accuracy: 0.7228915662650602\n",
      "Average Precision Score: 0.6280048423358506\n",
      "Kappa: 0.43604135893648455\n",
      "Hamming Loss: 0.27710843373493976\n",
      "AUC0.7157894736842105\n",
      "Sensitivity0.631578947368421\n",
      "Specificity0.8\n",
      "DISR BST: 0.750690 (0.034893)\n",
      "\n",
      "Accuracy: 0.7818181818181819\n",
      "Average Precision Score: 0.6832065457065457\n",
      "Kappa: 0.5575583777140516\n",
      "Hamming Loss: 0.21818181818181817\n",
      "AUC: 0.7778994653994654\n",
      "Sensitivity: 0.7398648648648649\n",
      "Specificity: 0.8159340659340659\n",
      "Accuracy: 0.7590361445783133\n",
      "Average Precision Score: 0.6581015136059111\n",
      "Kappa: 0.522439585730725\n",
      "Hamming Loss: 0.24096385542168675\n",
      "AUC0.7654970760233917\n",
      "Sensitivity0.8421052631578947\n",
      "Specificity0.6888888888888889\n",
      "Accuracy: 0.7349397590361446\n",
      "Average Precision Score: 0.6341085164456454\n",
      "Kappa: 0.47468354430379744\n",
      "Hamming Loss: 0.26506024096385544\n",
      "AUC0.7412280701754387\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "for name, model in models:\n",
    "    for kind, idx in MV_sel:\n",
    "        #print(idx[0:num_fea][0])\n",
    "        # X_sel = X[:, idx[0:num_fea]]\n",
    "        X_test_ = X_test[:,idx[0:num_fea][0]]\n",
    "        X_validate_ = X_validate[:,idx[0:num_fea][0]]\n",
    "        X_train_ = X_train[:, idx[0:num_fea][0]]\n",
    "        # X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X_sel, Y, test_size=validation_size, random_state=seed)\n",
    "        #kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "        cv_results = model_selection.cross_val_score(model, X_train_, Y_train, cv=kfold, scoring='roc_auc')\n",
    "        \n",
    "        sheet.write(r,c,cv_results.mean())\n",
    "        \n",
    "        msg = \"%s %s: %f (%f)\\n\" % (kind, name, cv_results.mean(), cv_results.std())\n",
    "        print(msg)\n",
    "        output.write(msg)\n",
    "        model.fit(X_train_, Y_train)\n",
    "        joblib.dump(model,'./handpkl/Endo'+name+kind+'20.pkl')\n",
    "        \n",
    "        Y_pred = model.predict(X_train_)\n",
    "        print(\"Accuracy: \" + repr(accuracy_score(Y_train, Y_pred)))\n",
    "        print(\"Average Precision Score: \" + repr(average_precision_score(Y_train, Y_pred)))\n",
    "        print(\"Kappa: \" + repr(cohen_kappa_score(Y_train, Y_pred)))\n",
    "        print(\"Hamming Loss: \" + repr(hamming_loss(Y_train, Y_pred)))\n",
    "        print(\"AUC: \" + repr(roc_auc_score(Y_train, Y_pred)))\n",
    "        print(\"Sensitivity: \" + repr(recall_score(Y_train, Y_pred)))\n",
    "        tn, fp, fn, tp = confusion_matrix(Y_train, Y_pred).ravel()\n",
    "        print(\"Specificity: \" + repr(tn / (tn + fp)))\n",
    "        \n",
    "        sheet_train.write(r,c,roc_auc_score(Y_train, Y_pred))\n",
    "        \n",
    "        Y_pred = model.predict(X_validate_)    \n",
    "        print(\"Accuracy: \" + repr(accuracy_score(Y_validate, Y_pred)))\n",
    "        print(\"Average Precision Score: \" + repr(average_precision_score(Y_validate, Y_pred)))\n",
    "        print(\"Kappa: \" + repr(cohen_kappa_score(Y_validate, Y_pred)))\n",
    "        print(\"Hamming Loss: \" + repr(hamming_loss(Y_validate, Y_pred)))\n",
    "        print(\"AUC\"+repr(roc_auc_score(Y_validate,Y_pred)))\n",
    "        print(\"Sensitivity\" + repr(recall_score(Y_validate,Y_pred)))\n",
    "        tn,fp,fn,tp = confusion_matrix(Y_validate,Y_pred).ravel()\n",
    "        print(\"Specificity\" + repr(tn/(tn+fp)))\n",
    "        \n",
    "        sheet_validate.write(r,c,roc_auc_score(Y_validate,Y_pred))\n",
    "        \n",
    "        Y_pred = model.predict(X_test_)\n",
    "        print(\"Accuracy: \" + repr(accuracy_score(Y_test, Y_pred)))\n",
    "        print(\"Average Precision Score: \" + repr(average_precision_score(Y_test, Y_pred)))\n",
    "        print(\"Kappa: \" + repr(cohen_kappa_score(Y_test, Y_pred)))\n",
    "        print(\"Hamming Loss: \" + repr(hamming_loss(Y_test, Y_pred)))\n",
    "        print(\"AUC\"+repr(roc_auc_score(Y_test,Y_pred)))\n",
    "        print(\"Sensitivity\" + repr(recall_score(Y_test,Y_pred)))\n",
    "        tn,fp,fn,tp = confusion_matrix(Y_test,Y_pred).ravel()\n",
    "        print(\"Specificity\" + repr(tn/(tn+fp)))\n",
    "        \n",
    "        sheet_test.write(r,c,roc_auc_score(Y_test,Y_pred))\n",
    "        \n",
    "        r = r + 1\n",
    "    c = c + 1\n",
    "    r = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "book.save(file)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False ... False False  True]\n",
      "22\n",
      "1300\n",
      "1332\n",
      "1335\n",
      "1357\n",
      "1360\n",
      "1400\n",
      "1435\n",
      "1460\n",
      "2375\n",
      "2949\n",
      "3973\n",
      "4799\n",
      "4963\n",
      "4983\n",
      "5506\n",
      "5513\n",
      "5577\n",
      "5833\n",
      "6092\n"
     ]
    }
   ],
   "source": [
    "sc = MinMaxScaler()\n",
    "sct = sc.fit(X_train,Y_train)\n",
    "X_train = sct.transform(X_train)\n",
    "X_validate = sct.transform(X_validate)\n",
    "X_test = sct.transform(X_test)\n",
    "\n",
    "sk = SelectKBest(reliefF.reliefF, k=20)\n",
    "idx  = sk.fit(X_train,Y_train).get_support()\n",
    "print(idx)\n",
    "for i in range(len(idx)):\n",
    "    if idx[i] == True:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(849, 6097)\n",
      "[121, 62, 193, 810, 288, 237, 40, 34, 179, 17, 258, 153, 623, 49, 285, 624, 93, 848, 640, 296, 203, 228, 212, 211, 115, 264, 2, 209, 174, 649, 108, 91, 122, 817, 180, 243, 166, 635, 700, 771, 349, 701, 585, 367, 432, 687, 356, 445, 658, 495, 745, 430, 569, 684, 599, 660, 546, 756, 733, 368, 805, 679, 415, 464, 399, 419, 402, 499, 688, 611, 448, 500, 414, 406, 665, 777, 720, 407, 664, 527, 523, 409, 710]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1]\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "import joblib\n",
    "\n",
    "file_feature = \"./csv/endometrium.csv\"\n",
    "file_train = \"./csv/train.csv\"\n",
    "file_validate = \"./csv/validation.csv\"\n",
    "file_test = \"./csv/test.csv\"\n",
    "\n",
    "f = open(file_feature)\n",
    "csv_f = csv.reader(f)\n",
    "features = next(csv_f)\n",
    "dataset = pd.read_csv(file_feature, names=features, usecols=range(1,6098), dtype=np.float64, skiprows=1, low_memory=False)\n",
    "f = open(file_train)\n",
    "csv_f = csv.reader(f)\n",
    "features = next(csv_f)\n",
    "dataset_train = pd.read_csv(file_train, names=features, usecols=range(1,1), dtype=np.float64, skiprows=1, low_memory=False)\n",
    "\n",
    "with open('./csv/train.csv','r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    train_list = [row[1] for row in reader]\n",
    "with open('./csv/validation.csv','r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    validation_list = [row['patient'] for row in reader]\n",
    "with open('./csv/test.csv','r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    test_list = [row['patient'] for row in reader]\n",
    "\n",
    "dataset['outcome'] = pd.to_numeric(dataset['outcome'],errors='coerce')\n",
    "array_OG = dataset.values\n",
    "print(array_OG.shape)\n",
    "train_list = train_list[1:]\n",
    "validation_list = validation_list[0:]\n",
    "test_list = test_list[0:]\n",
    "#print(test_list)\n",
    "#print(train_list)\n",
    "#print(validation_list)\n",
    "\n",
    "def cat_str(num_list):\n",
    "    n_list = []\n",
    "    for i in num_list:\n",
    "        temp = i[12:]\n",
    "        n_list.append(temp)\n",
    "    n_list = [int(x) for x in n_list]\n",
    "    return n_list\n",
    "\n",
    "train_list = cat_str(train_list)\n",
    "validation_list = cat_str(validation_list)\n",
    "test_list = cat_str(test_list)\n",
    "\n",
    "#print(train_list)\n",
    "#print(validation_list)\n",
    "print(test_list)\n",
    "#print(len(test_list))\n",
    "\n",
    "train_feature = []\n",
    "validate_feature = []\n",
    "test_feature = []\n",
    "count = 1\n",
    "for i in range(len(array_OG)):\n",
    "    num = i + 1\n",
    "    if num in train_list:\n",
    "        train_feature.append(array_OG[i])\n",
    "    elif num in validation_list:\n",
    "        validate_feature.append(array_OG[i])\n",
    "    elif num in test_list:\n",
    "        #print(count)\n",
    "        count = count + 1\n",
    "        test_feature.append(array_OG[i])\n",
    "        #print(num)\n",
    "        #print(array_OG[i,6096])\n",
    "        \n",
    "train_feature = np.array(train_feature)\n",
    "validate_feature = np.array(validate_feature)\n",
    "test_feature = np.array(test_feature)\n",
    "\n",
    "train_feature = pd.DataFrame(train_feature)\n",
    "train_feature.dropna(axis=1, thresh=2, inplace=True)\n",
    "train_feature.dropna(how='all',thresh = 20,inplace=True)\n",
    "train_feature = np.array(train_feature)\n",
    "wh_inf = np.isinf(train_feature)\n",
    "train_feature[wh_inf]=0\n",
    "wh_nan = np.isnan(train_feature)\n",
    "train_feature[wh_nan]=0\n",
    "\n",
    "validate_feature = pd.DataFrame(validate_feature)\n",
    "validate_feature.dropna(axis=1, thresh=2, inplace=True)\n",
    "#validate_feature.dropna(how='all',thresh = 20,inplace=True)\n",
    "validate_feature = np.array(validate_feature)\n",
    "wh_inf = np.isinf(validate_feature)\n",
    "validate_feature[wh_inf]=0\n",
    "wh_nan = np.isnan(validate_feature)\n",
    "validate_feature[wh_nan]=0\n",
    "\n",
    "test_feature = pd.DataFrame(test_feature)\n",
    "test_feature.dropna(axis=1, thresh=2, inplace=True)\n",
    "#test_feature.dropna(how='all',thresh = 20,inplace=True)\n",
    "test_feature = np.array(test_feature)\n",
    "wh_inf = np.isinf(test_feature)\n",
    "test_feature[wh_inf]=0\n",
    "wh_nan = np.isnan(test_feature)\n",
    "test_feature[wh_nan]=0\n",
    "\n",
    "#only use image features\n",
    "X_train = train_feature[:,:6093]\n",
    "Y_train = train_feature[:,6093]\n",
    "Y_train = Y_train.astype('int32')\n",
    "\n",
    "X_validate = validate_feature[:,:6093]\n",
    "Y_validate = validate_feature[:,6093]\n",
    "Y_validate = Y_validate.astype('int32')\n",
    "\n",
    "X_test = test_feature[:,:6093]\n",
    "Y_test = test_feature[:,6093]\n",
    "Y_test = Y_test.astype('int32')\n",
    "seed = 7\n",
    "\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(X_train) \n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(Y_train)\n",
    "\n",
    "print(Y_test)\n",
    "print(len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 1]\n",
      "Accuracy: 0.8313253012048193\n",
      "Average Precision Score: 0.7498498147715515\n",
      "Kappa: 0.660233918128655\n",
      "Hamming Loss: 0.1686746987951807\n",
      "AUC0.8301169590643276\n",
      "Sensitivity0.8157894736842105\n",
      "Specificity0.8444444444444444\n",
      "38 7 7 31\n"
     ]
    }
   ],
   "source": [
    "pipe = joblib.load('./handpkl/EndoBAGRELF20.pkl')\n",
    "Y_pred = pipe.predict(X_validate)\n",
    "print(\"Accuracy: \" + repr(accuracy_score(Y_validate, Y_pred)))\n",
    "print(\"Average Precision Score: \" + repr(average_precision_score(Y_validate, Y_pred)))\n",
    "print(\"Kappa: \" + repr(cohen_kappa_score(Y_validate, Y_pred)))\n",
    "print(\"Hamming Loss: \" + repr(hamming_loss(Y_validate, Y_pred)))\n",
    "print(\"AUC\"+repr(roc_auc_score(Y_validate,Y_pred)))\n",
    "print(\"Sensitivity\" + repr(recall_score(Y_validate,Y_pred)))\n",
    "tn,fp,fn,tp = confusion_matrix(Y_validate,Y_pred).ravel()\n",
    "print(\"Specificity\" + repr(tn/(tn+fp)))\n",
    "print(tn,fp,fn,tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.73-0.90)\n",
      "(0.66-0.91)\n",
      "(0.71-0.93)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#p is proportion of trials that were successes\n",
    "#n is the number of trials\n",
    "import math\n",
    "def adjusted_wald(p, n, z=1.96):\n",
    "    p_adj = (n * p + (z**2)/2)/(n+z**2)\n",
    "    n_adj = n + z**2\n",
    "    span = z * math.sqrt(p_adj*(1-p_adj)/n_adj)\n",
    "    return max(0, p_adj - span), min(p_adj + span, 1.0)\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.831), float(83))))\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.816), float(38))))\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.844), float(45))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n",
      "258\n",
      "211\n",
      "174\n",
      "733\n",
      "523\n",
      "Accuracy: 0.8433734939759037\n",
      "Average Precision Score: 0.7632473212688812\n",
      "Kappa: 0.6851473592063029\n",
      "Hamming Loss: 0.1566265060240964\n",
      "AUC: 0.8856725146198831\n",
      "Sensitivity0.8421052631578947\n",
      "Specificity0.8444444444444444\n",
      "38 7 6 32\n"
     ]
    }
   ],
   "source": [
    "Y_pred = pipe.predict(X_test)\n",
    "for i in range(83):\n",
    "    if Y_test[i] == 1:\n",
    "        if Y_pred[i] == 0:\n",
    "            print(test_list[i])\n",
    "Y_proba = pipe.predict_proba(X_test)\n",
    "proba = np.empty((len(Y_pred),1))\n",
    "for i in range(len(Y_pred)):\n",
    "    proba[i] = Y_proba[i][1]\n",
    "print(\"Accuracy: \" + repr(accuracy_score(Y_test, Y_pred)))\n",
    "print(\"Average Precision Score: \" + repr(average_precision_score(Y_test, Y_pred)))\n",
    "print(\"Kappa: \" + repr(cohen_kappa_score(Y_test, Y_pred)))\n",
    "print(\"Hamming Loss: \" + repr(hamming_loss(Y_test, Y_pred)))\n",
    "print(\"AUC: \" + repr(roc_auc_score(Y_test, proba)))\n",
    "print(\"Sensitivity\" + repr(recall_score(Y_test,Y_pred)))\n",
    "tn,fp,fn,tp = confusion_matrix(Y_test,Y_pred).ravel()\n",
    "print(\"Specificity\" + repr(tn/(tn+fp)))\n",
    "print(tn,fp,fn,tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2]\n",
      "[0.8]\n",
      "[0.8]\n",
      "[0.8]\n",
      "[1.]\n",
      "[1.]\n",
      "[0.8]\n",
      "[0.7]\n",
      "[0.9]\n",
      "[1.]\n",
      "[0.4]\n",
      "[0.6]\n",
      "[0.6]\n",
      "[0.8]\n",
      "[0.8]\n",
      "[0.8]\n",
      "[0.7]\n",
      "[1.]\n",
      "[0.6]\n",
      "[0.8]\n",
      "[0.8]\n",
      "[0.7]\n",
      "[0.7]\n",
      "[0.2]\n",
      "[0.6]\n",
      "[0.6]\n",
      "[0.6]\n",
      "[0.8]\n",
      "[0.]\n",
      "[1.]\n",
      "[0.]\n",
      "[0.6]\n",
      "[0.3]\n",
      "[0.4]\n",
      "[0.2]\n",
      "[0.]\n",
      "[0.8]\n",
      "[0.1]\n",
      "[0.]\n",
      "[0.3]\n",
      "[0.5]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.1]\n",
      "[0.1]\n",
      "[0.1]\n",
      "[0.2]\n",
      "[0.4]\n",
      "[0.]\n",
      "[0.3]\n",
      "[0.]\n",
      "[0.4]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.1]\n",
      "[0.6]\n",
      "[1.]\n",
      "[0.3]\n",
      "[0.9]\n",
      "[0.9]\n",
      "[0.7]\n",
      "[0.4]\n",
      "[0.1]\n",
      "[0.]\n",
      "[0.2]\n",
      "[0.3]\n",
      "[0.1]\n",
      "[0.7]\n",
      "[0.1]\n",
      "[0.4]\n",
      "[0.]\n",
      "[0.3]\n",
      "[0.3]\n",
      "[0.8]\n",
      "[0.1]\n",
      "[0.9]\n",
      "[0.]\n",
      "[0.6]\n",
      "[0.5]\n",
      "[0.4]\n",
      "[1.]\n",
      "[0.8]\n"
     ]
    }
   ],
   "source": [
    "for i in proba:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.75-0.91)\n",
      "(0.69-0.93)\n",
      "(0.71-0.93)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#p is proportion of trials that were successes\n",
    "#n is the number of trials\n",
    "import math\n",
    "def adjusted_wald(p, n, z=1.96):\n",
    "    p_adj = (n * p + (z**2)/2)/(n+z**2)\n",
    "    n_adj = n + z**2\n",
    "    span = z * math.sqrt(p_adj*(1-p_adj)/n_adj)\n",
    "    return max(0, p_adj - span), min(p_adj + span, 1.0)\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.843), float(83))))\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.842), float(38))))\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.844), float(45))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8433734939759037\n",
      "Average Precision Score: 0.7632473212688812\n",
      "Kappa: 0.6851473592063029\n",
      "Hamming Loss: 0.1566265060240964\n",
      "AUC: 0.8432748538011696\n",
      "Sensitivity: 0.8421052631578947\n",
      "Specificity: 0.8444444444444444\n",
      "38 7 6 32\n",
      "0.8856725146198831\n"
     ]
    }
   ],
   "source": [
    "Y_pred = pipe.predict(X_test)\n",
    "Y_prob = pipe.predict_proba(X_test)\n",
    "print(\"Accuracy: \" + repr(accuracy_score(Y_test, Y_pred)))\n",
    "print(\"Average Precision Score: \" + repr(average_precision_score(Y_test, Y_pred)))\n",
    "print(\"Kappa: \" + repr(cohen_kappa_score(Y_test, Y_pred)))\n",
    "print(\"Hamming Loss: \" + repr(hamming_loss(Y_test, Y_pred)))\n",
    "print(\"AUC: \" + repr(roc_auc_score(Y_test, Y_pred)))\n",
    "print(\"Sensitivity: \" + repr(recall_score(Y_test, Y_pred)))\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test, Y_pred).ravel()\n",
    "print(\"Specificity: \" + repr(tn / (tn + fp)))\n",
    "print(tn,fp,fn,tp)\n",
    "y_prob = np.empty((len(Y_prob),1))\n",
    "\n",
    "for i in range(len(Y_prob)):\n",
    "    #print(i)\n",
    "    y_prob[i] = Y_prob[i][1]\n",
    "    #print(y_prob[i][0])\n",
    "    #print(Y_test[i])\n",
    "print(roc_auc_score(Y_test,y_prob))\n",
    "\n",
    "\n",
    "Accuracy: 0.8433734939759037\n",
    "Average Precision Score: 0.7632473212688812\n",
    "Kappa: 0.6851473592063029\n",
    "Hamming Loss: 0.1566265060240964\n",
    "AUC: 0.89\n",
    "Sensitivity: 0.8421052631578947\n",
    "Specificity: 0.8444444444444444\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012265615324065763\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from scipy.stats import binom_test\n",
    "#x accuracy\n",
    "#n = number of trials\n",
    "#p accuracy compare to\n",
    "x = 0.84\n",
    "n = 45\n",
    "p = 0.645\n",
    "\n",
    "print(binom_test(int(x * n), n, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(849, 6097)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1.]\n",
      "Accuracy: 0.7341772151898734\n",
      "Average Precision Score: 0.5255970568061359\n",
      "Kappa: 0.42375824939215\n",
      "Hamming Loss: 0.26582278481012656\n",
      "AUC: 0.7335434173669468\n",
      "Sensitivity0.6428571428571429\n",
      "Specificity0.7843137254901961\n",
      "40 11 10 18\n",
      "(0.62-0.82)\n",
      "(0.45-0.79)\n",
      "(0.65-0.87)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "import joblib\n",
    "\n",
    "file_feature = \"./csv/endometrium.csv\"\n",
    "file_train = \"./csv/train.csv\"\n",
    "file_validate = \"./csv/validation.csv\"\n",
    "file_test = \"./csv/test.csv\"\n",
    "\n",
    "f = open(file_feature)\n",
    "csv_f = csv.reader(f)\n",
    "features = next(csv_f)\n",
    "dataset = pd.read_csv(file_feature, names=features, usecols=range(1,6098), dtype=np.float64, skiprows=1, low_memory=False)\n",
    "f = open(file_train)\n",
    "csv_f = csv.reader(f)\n",
    "features = next(csv_f)\n",
    "dataset_train = pd.read_csv(file_train, names=features, usecols=range(1,1), dtype=np.float64, skiprows=1, low_memory=False)\n",
    "\n",
    "with open('./csv/train.csv','r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    train_list = [row[1] for row in reader]\n",
    "with open('./csv/validation.csv','r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    validation_list = [row['patient'] for row in reader]\n",
    "with open('./csv/test.csv','r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    test_list = [row['patient'] for row in reader]\n",
    "\n",
    "dataset['outcome'] = pd.to_numeric(dataset['outcome'],errors='coerce')\n",
    "array_OG = dataset.values\n",
    "print(array_OG.shape)\n",
    "train_list = train_list[1:]\n",
    "validation_list = validation_list[0:]\n",
    "test_list = test_list[0:]\n",
    "#print(test_list)\n",
    "#print(train_list)\n",
    "#print(validation_list)\n",
    "\n",
    "def cat_str(num_list):\n",
    "    n_list = []\n",
    "    for i in num_list:\n",
    "        temp = i[12:]\n",
    "        n_list.append(temp)\n",
    "    n_list = [int(x) for x in n_list]\n",
    "    return n_list\n",
    "\n",
    "train_list = cat_str(train_list)\n",
    "validation_list = cat_str(validation_list)\n",
    "test_list = cat_str(test_list)\n",
    "\n",
    "#print(train_list)\n",
    "#print(validation_list)\n",
    "#print(test_list)\n",
    "#print(len(test_list))\n",
    "\n",
    "train_feature = []\n",
    "validate_feature = []\n",
    "test_feature = []\n",
    "count = 1\n",
    "for i in range(len(array_OG)):\n",
    "    num = i + 1\n",
    "    if num in train_list:\n",
    "        train_feature.append(array_OG[i])\n",
    "    elif num in validation_list:\n",
    "        validate_feature.append(array_OG[i])\n",
    "    elif num in test_list:\n",
    "        #print(count)\n",
    "        count = count + 1\n",
    "        test_feature.append(array_OG[i])\n",
    "        #print(num)\n",
    "        #print(array_OG[i,6096])\n",
    "        \n",
    "train_feature = np.array(train_feature)\n",
    "validate_feature = np.array(validate_feature)\n",
    "test_feature = np.array(test_feature)\n",
    "\n",
    "train_feature = pd.DataFrame(train_feature)\n",
    "train_feature.dropna(axis=1, thresh=2, inplace=True)\n",
    "train_feature.dropna(how='all',thresh = 20,inplace=True)\n",
    "train_feature = np.array(train_feature)\n",
    "wh_inf = np.isinf(train_feature)\n",
    "train_feature[wh_inf]=0\n",
    "wh_nan = np.isnan(train_feature)\n",
    "train_feature[wh_nan]=0\n",
    "\n",
    "validate_feature = pd.DataFrame(validate_feature)\n",
    "validate_feature.dropna(axis=1, thresh=2, inplace=True)\n",
    "#validate_feature.dropna(how='all',thresh = 20,inplace=True)\n",
    "validate_feature = np.array(validate_feature)\n",
    "wh_inf = np.isinf(validate_feature)\n",
    "validate_feature[wh_inf]=0\n",
    "wh_nan = np.isnan(validate_feature)\n",
    "validate_feature[wh_nan]=0\n",
    "\n",
    "test_feature = pd.DataFrame(test_feature)\n",
    "test_feature.dropna(axis=1, thresh=2, inplace=True)\n",
    "#test_feature.dropna(how='all',thresh = 20,inplace=True)\n",
    "test_feature = np.array(test_feature)\n",
    "wh_inf = np.isinf(test_feature)\n",
    "test_feature[wh_inf]=0\n",
    "wh_nan = np.isnan(test_feature)\n",
    "test_feature[wh_nan]=0\n",
    "\n",
    "#only use image features\n",
    "X_train = train_feature[:,:6093]\n",
    "Y_train = train_feature[:,6093]\n",
    "Y_train = Y_train.astype('int32')\n",
    "\n",
    "X_validate = validate_feature[:,:6093]\n",
    "Y_validate = validate_feature[:,6093]\n",
    "Y_validate = Y_validate.astype('int32')\n",
    "\n",
    "X_test = test_feature[:,:6093]\n",
    "Y_test = test_feature[:,6093]\n",
    "Y_test = Y_test.astype('int32')\n",
    "seed = 7\n",
    "\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(X_train) \n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(Y_train)\n",
    "\n",
    "#print(Y_train)\n",
    "\n",
    "f = open('./csv/Brown_endo.csv')\n",
    "t = csv.reader(f)\n",
    "features = next(t)\n",
    "dataset = pd.read_csv('./csv/Brown_endo.csv', names=features, usecols=range(1,6102), dtype=np.float64, skiprows=1, low_memory=False)\n",
    "#dataset['outcome']=pd.to_numeric(dataset['outcome'],errors='coerce')\n",
    "\n",
    "dataset.dropna(axis=1, thresh=2, inplace=True)\n",
    "dataset.dropna(how='all',thresh = 20,inplace=True)\n",
    "dataset = np.array(dataset)\n",
    "wh_inf = np.isinf(dataset)\n",
    "dataset[wh_inf]=0\n",
    "wh_nan = np.isnan(dataset)\n",
    "dataset[wh_nan]=0\n",
    "\n",
    "#print(dataset.shape)\n",
    "wh_inf = np.isinf(dataset)\n",
    "dataset[wh_inf]=0\n",
    "wh_nan = np.isnan(dataset)\n",
    "dataset[wh_nan]=0\n",
    "\n",
    "features = dataset\n",
    "X_test = features[:,0:6093]\n",
    "Y_test = features[:,6093]\n",
    "\n",
    "expert1 = features[:,6094]\n",
    "expert2 = features[:,6095]\n",
    "expert3 = features[:,6096]\n",
    "expert4 = features[:,6097]\n",
    "print(Y_test)\n",
    "\n",
    "\n",
    "\n",
    "pipe = joblib.load('./handpkl/EndoBAGRELF20.pkl')\n",
    "Y_pred = pipe.predict(X_test)\n",
    "Y_proba = pipe.predict_proba(X_test)\n",
    "proba1 = np.empty((len(Y_pred),1))\n",
    "for i in range(len(Y_pred)):\n",
    "    proba1[i] = Y_proba[i][1]\n",
    "print(\"Accuracy: \" + repr(accuracy_score(Y_test, Y_pred)))\n",
    "print(\"Average Precision Score: \" + repr(average_precision_score(Y_test, Y_pred)))\n",
    "print(\"Kappa: \" + repr(cohen_kappa_score(Y_test, Y_pred)))\n",
    "print(\"Hamming Loss: \" + repr(hamming_loss(Y_test, Y_pred)))\n",
    "print(\"AUC: \" + repr(roc_auc_score(Y_test, proba1)))\n",
    "print(\"Sensitivity\" + repr(recall_score(Y_test,Y_pred)))\n",
    "tn,fp,fn,tp = confusion_matrix(Y_test,Y_pred).ravel()\n",
    "print(\"Specificity\" + repr(tn/(tn+fp)))\n",
    "print(tn,fp,fn,tp)\n",
    "\n",
    "import sys\n",
    "#p is proportion of trials that were successes\n",
    "#n is the number of trials\n",
    "import math\n",
    "def adjusted_wald(p, n, z=1.96):\n",
    "    p_adj = (n * p + (z**2)/2)/(n+z**2)\n",
    "    n_adj = n + z**2\n",
    "    span = z * math.sqrt(p_adj*(1-p_adj)/n_adj)\n",
    "    return max(0, p_adj - span), min(p_adj + span, 1.0)\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.73), float(79))))\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.64), float(28))))\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.78), float(51))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5443037974683544\n",
      "Average Precision Score: 0.3716596343178622\n",
      "Kappa: 0.06447368421052624\n",
      "Hamming Loss: 0.45569620253164556\n",
      "AUC: 0.7871148459383753\n",
      "Sensitivity0.5\n",
      "Specificity0.5686274509803921\n",
      "29 22 14 14\n",
      "(0.38-0.60)\n",
      "(0.29-0.64)\n",
      "(0.38-0.64)\n"
     ]
    }
   ],
   "source": [
    "Y_pred = expert4\n",
    "print(\"Accuracy: \" + repr(accuracy_score(Y_test, Y_pred)))\n",
    "print(\"Average Precision Score: \" + repr(average_precision_score(Y_test, Y_pred)))\n",
    "print(\"Kappa: \" + repr(cohen_kappa_score(Y_test, Y_pred)))\n",
    "print(\"Hamming Loss: \" + repr(hamming_loss(Y_test, Y_pred)))\n",
    "print(\"AUC: \" + repr(roc_auc_score(Y_test, proba2)))\n",
    "print(\"Sensitivity\" + repr(recall_score(Y_test,Y_pred)))\n",
    "tn,fp,fn,tp = confusion_matrix(Y_test,Y_pred).ravel()\n",
    "print(\"Specificity\" + repr(tn/(tn+fp)))\n",
    "print(tn,fp,fn,tp)\n",
    "\n",
    "import sys\n",
    "#p is proportion of trials that were successes\n",
    "#n is the number of trials\n",
    "import math\n",
    "def adjusted_wald(p, n, z=1.96):\n",
    "    p_adj = (n * p + (z**2)/2)/(n+z**2)\n",
    "    n_adj = n + z**2\n",
    "    span = z * math.sqrt(p_adj*(1-p_adj)/n_adj)\n",
    "    return max(0, p_adj - span), min(p_adj + span, 1.0)\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.49), float(79))))\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.46), float(28))))\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.51), float(51))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7974683544303798\n",
      "Average Precision Score: 0.6083963504849581\n",
      "Kappa: 0.5349521707137601\n",
      "Hamming Loss: 0.20253164556962025\n",
      "AUC: 0.7871148459383753\n",
      "Sensitivity0.6071428571428571\n",
      "Specificity0.9019607843137255\n",
      "46 5 11 17\n",
      "(0.70-0.87)\n",
      "(0.43-0.77)\n",
      "(0.78-0.96)\n"
     ]
    }
   ],
   "source": [
    "pipe = joblib.load('./pkl/tpot_running_5.pkl')\n",
    "Y_pred = pipe.predict(X_test)\n",
    "Y_proba = pipe.predict_proba(X_test)\n",
    "proba2 = np.empty((len(Y_pred),1))\n",
    "for i in range(len(Y_pred)):\n",
    "    proba2[i] = Y_proba[i][1]\n",
    "print(\"Accuracy: \" + repr(accuracy_score(Y_test, Y_pred)))\n",
    "print(\"Average Precision Score: \" + repr(average_precision_score(Y_test, Y_pred)))\n",
    "print(\"Kappa: \" + repr(cohen_kappa_score(Y_test, Y_pred)))\n",
    "print(\"Hamming Loss: \" + repr(hamming_loss(Y_test, Y_pred)))\n",
    "print(\"AUC: \" + repr(roc_auc_score(Y_test, proba2)))\n",
    "print(\"Sensitivity\" + repr(recall_score(Y_test,Y_pred)))\n",
    "tn,fp,fn,tp = confusion_matrix(Y_test,Y_pred).ravel()\n",
    "print(\"Specificity\" + repr(tn/(tn+fp)))\n",
    "print(tn,fp,fn,tp)\n",
    "\n",
    "import sys\n",
    "#p is proportion of trials that were successes\n",
    "#n is the number of trials\n",
    "import math\n",
    "def adjusted_wald(p, n, z=1.96):\n",
    "    p_adj = (n * p + (z**2)/2)/(n+z**2)\n",
    "    n_adj = n + z**2\n",
    "    span = z * math.sqrt(p_adj*(1-p_adj)/n_adj)\n",
    "    return max(0, p_adj - span), min(p_adj + span, 1.0)\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.80), float(79))))\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.61), float(28))))\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.90), float(51))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from scipy.stats import binom_test\n",
    "#x accuracy\n",
    "#n = number of trials\n",
    "#p accuracy compare to 28,51\n",
    "#83,38,45\n",
    "x = 0.78\n",
    "n = 79\n",
    "p = 0.66\n",
    "\n",
    "print(binom_test(int(x * n), n, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plot for tpot\n",
    "fpr, tpr, thresholds= metrics.roc_curve(Y_test, proba2, pos_label=1)\n",
    "auc_CT =metrics.auc(fpr, tpr) \n",
    "plt.plot(fpr, tpr,\n",
    "         label='TPOT (area = {0:0.2f}, acc = 0.80)'\n",
    "         ''.format(auc_CT),\n",
    "         color='mediumpurple', linestyle=':', linewidth=3)\n",
    "\n",
    "#plot for hand optimization\n",
    "fpr, tpr, thresholds= metrics.roc_curve(Y_test, proba1, pos_label=1)\n",
    "auc = metrics.auc(fpr,tpr)\n",
    "plt.plot(fpr, tpr,\n",
    "         label='Radiomics curve (area = {0:0.2f}, acc = 0.73)'\n",
    "               ''.format(auc),\n",
    "         color='green', linestyle=':', linewidth=3)\n",
    "\n",
    "#plot for expert1\n",
    "fpr, tpr, thresholds= metrics.roc_curve(Y_test, expert1, pos_label=1)\n",
    "acc_1 = accuracy_score(Y_test, expert1)\n",
    "plt.scatter(fpr[1],tpr[1], marker = 's', color = 'black', label='Expert1(acc = {0:0.2f})'\n",
    "            ''.format(acc_1), s = 30)\n",
    "\n",
    "#plot for expert2\n",
    "fpr, tpr, thresholds= metrics.roc_curve(Y_test, expert2, pos_label=1)\n",
    "acc_2 = accuracy_score(Y_test, expert2)\n",
    "plt.scatter(fpr[1], tpr[1], marker = 'o', color = 'black', label='Expert2(acc = {0:0.2f})'\n",
    "            ''.format(acc_2), s = 30)\n",
    "\n",
    "#plot for expert3\n",
    "fpr, tpr, thresholds= metrics.roc_curve(Y_test, expert3, pos_label=1)\n",
    "acc_3 = accuracy_score(Y_test, expert3)\n",
    "plt.scatter(fpr[1], tpr[1], marker = 'v', color = 'black', label='Expert3(acc = {0:0.2f})'\n",
    "            ''.format(acc_3), s = 30)\n",
    "\n",
    "#plot for expert4\n",
    "fpr, tpr, thresholds= metrics.roc_curve(Y_test, expert4, pos_label=1)\n",
    "acc_4 = accuracy_score(Y_test, expert4)\n",
    "plt.scatter(fpr[1], tpr[1], marker = '<', color = 'black', label='Expert4(acc = {0:0.2f})'\n",
    "            ''.format(acc_4), s = 30)\n",
    "\n",
    "plt.plot([0,1],[0,1],color='y',linestyle=':',linewidth=2)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.4,0.25),loc=10)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.savefig('roc_brown.jpg',dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "outcome = Y_test\n",
    "#plot for tpot\n",
    "fpr, tpr, thresholds= metrics.precision_recall_curve(outcome, proba2, pos_label=1)\n",
    "auc_CT =metrics.auc(tpr, fpr) \n",
    "plt.plot(tpr, fpr,\n",
    "         label='TPOT (area = {0:0.2f}, acc = 0.80)'\n",
    "         ''.format(auc_CT),\n",
    "         color='mediumpurple', linestyle=':', linewidth=3)\n",
    "\n",
    "#plot for hand optimization\n",
    "fpr, tpr, thresholds= metrics.precision_recall_curve(outcome, proba1, pos_label=1)\n",
    "auc = metrics.auc(tpr,fpr)\n",
    "plt.plot(tpr, fpr,\n",
    "         label='Radiomics curve (area = {0:0.2f}, acc = 0.73)'\n",
    "               ''.format(auc),\n",
    "         color='green', linestyle=':', linewidth=3)\n",
    "\n",
    "#plot for expert1\n",
    "fpr, tpr, thresholds= metrics.precision_recall_curve(outcome, expert1, pos_label=1)\n",
    "acc_1 = accuracy_score(outcome, expert1)\n",
    "plt.scatter(tpr[1],fpr[1], marker = 's', color = 'black', label='Expert1(acc = {0:0.2f})'\n",
    "            ''.format(acc_1), s = 30)\n",
    "\n",
    "#plot for expert2\n",
    "fpr, tpr, thresholds= metrics.precision_recall_curve(outcome, expert2, pos_label=1)\n",
    "acc_2 = accuracy_score(outcome, expert2)\n",
    "plt.scatter(tpr[1], fpr[1], marker = 'o', color = 'black', label='Expert2(acc = {0:0.2f})'\n",
    "            ''.format(acc_2), s = 30)\n",
    "\n",
    "#plot for expert2\n",
    "fpr, tpr, thresholds= metrics.precision_recall_curve(outcome, expert3, pos_label=1)\n",
    "acc_3 = accuracy_score(outcome, expert3)\n",
    "plt.scatter(tpr[1], fpr[1], marker = 'v', color = 'black', label='Expert3(acc = {0:0.2f})'\n",
    "            ''.format(acc_3), s = 30)\n",
    "\n",
    "#plot for expert2\n",
    "fpr, tpr, thresholds= metrics.precision_recall_curve(outcome, expert4, pos_label=1)\n",
    "acc_4 = accuracy_score(outcome, expert4)\n",
    "plt.scatter(tpr[1], fpr[1], marker = '<', color = 'black', label='Expert4(acc = {0:0.2f})'\n",
    "            ''.format(acc_4), s = 30)\n",
    "\n",
    "plt.plot([0,1],[1,0],color='y',linestyle=':',linewidth=2)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.4,0.25),loc=10)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.savefig('prroc_brown.jpg',dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.38-0.60)\n",
      "(0.19-0.53)\n",
      "(0.49-0.75)\n"
     ]
    }
   ],
   "source": [
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.49), float(79))))\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.34), float(28))))\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.63), float(51))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11513227400752077\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from scipy.stats import binom_test\n",
    "#x accuracy\n",
    "#n = number of trials\n",
    "#p accuracy compare to 28,51\n",
    "#83,38,45\n",
    "x = 0.78\n",
    "n = 45\n",
    "p = 0.66\n",
    "\n",
    "print(binom_test(int(x * n), n, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(849, 6097)\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "Accuracy: 0.6626506024096386\n",
      "Average Precision Score: 0.5677446628619742\n",
      "Kappa: 0.3091557669441142\n",
      "Hamming Loss: 0.3373493975903614\n",
      "AUC: 0.7809941520467836\n",
      "Sensitivity0.5263157894736842\n",
      "Specificity0.7777777777777778\n",
      "35 10 18 20\n",
      "(0.55-0.75)\n",
      "(0.38-0.68)\n",
      "(0.64-0.88)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "import joblib\n",
    "\n",
    "file_feature = \"./csv/endometrium.csv\"\n",
    "file_train = \"./csv/train.csv\"\n",
    "file_validate = \"./csv/validation.csv\"\n",
    "file_test = \"./csv/test.csv\"\n",
    "\n",
    "f = open(file_feature)\n",
    "csv_f = csv.reader(f)\n",
    "features = next(csv_f)\n",
    "dataset = pd.read_csv(file_feature, names=features, usecols=range(1,6098), dtype=np.float64, skiprows=1, low_memory=False)\n",
    "f = open(file_train)\n",
    "csv_f = csv.reader(f)\n",
    "features = next(csv_f)\n",
    "dataset_train = pd.read_csv(file_train, names=features, usecols=range(1,1), dtype=np.float64, skiprows=1, low_memory=False)\n",
    "\n",
    "with open('./csv/train.csv','r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    train_list = [row[1] for row in reader]\n",
    "with open('./csv/validation.csv','r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    validation_list = [row['patient'] for row in reader]\n",
    "with open('./csv/test.csv','r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    test_list = [row['patient'] for row in reader]\n",
    "\n",
    "dataset['outcome'] = pd.to_numeric(dataset['outcome'],errors='coerce')\n",
    "array_OG = dataset.values\n",
    "print(array_OG.shape)\n",
    "train_list = train_list[1:]\n",
    "validation_list = validation_list[0:]\n",
    "test_list = test_list[0:]\n",
    "#print(test_list)\n",
    "#print(train_list)\n",
    "#print(validation_list)\n",
    "\n",
    "def cat_str(num_list):\n",
    "    n_list = []\n",
    "    for i in num_list:\n",
    "        temp = i[12:]\n",
    "        n_list.append(temp)\n",
    "    n_list = [int(x) for x in n_list]\n",
    "    return n_list\n",
    "\n",
    "train_list = cat_str(train_list)\n",
    "validation_list = cat_str(validation_list)\n",
    "test_list = cat_str(test_list)\n",
    "\n",
    "#print(train_list)\n",
    "#print(validation_list)\n",
    "#print(test_list)\n",
    "#print(len(test_list))\n",
    "\n",
    "train_feature = []\n",
    "validate_feature = []\n",
    "test_feature = []\n",
    "count = 1\n",
    "for i in range(len(array_OG)):\n",
    "    num = i + 1\n",
    "    if num in train_list:\n",
    "        train_feature.append(array_OG[i])\n",
    "    elif num in validation_list:\n",
    "        validate_feature.append(array_OG[i])\n",
    "    elif num in test_list:\n",
    "        #print(count)\n",
    "        count = count + 1\n",
    "        test_feature.append(array_OG[i])\n",
    "        #print(num)\n",
    "        #print(array_OG[i,6096])\n",
    "        \n",
    "train_feature = np.array(train_feature)\n",
    "validate_feature = np.array(validate_feature)\n",
    "test_feature = np.array(test_feature)\n",
    "\n",
    "train_feature = pd.DataFrame(train_feature)\n",
    "train_feature.dropna(axis=1, thresh=2, inplace=True)\n",
    "train_feature.dropna(how='all',thresh = 20,inplace=True)\n",
    "train_feature = np.array(train_feature)\n",
    "wh_inf = np.isinf(train_feature)\n",
    "train_feature[wh_inf]=0\n",
    "wh_nan = np.isnan(train_feature)\n",
    "train_feature[wh_nan]=0\n",
    "\n",
    "validate_feature = pd.DataFrame(validate_feature)\n",
    "validate_feature.dropna(axis=1, thresh=2, inplace=True)\n",
    "#validate_feature.dropna(how='all',thresh = 20,inplace=True)\n",
    "validate_feature = np.array(validate_feature)\n",
    "wh_inf = np.isinf(validate_feature)\n",
    "validate_feature[wh_inf]=0\n",
    "wh_nan = np.isnan(validate_feature)\n",
    "validate_feature[wh_nan]=0\n",
    "\n",
    "test_feature = pd.DataFrame(test_feature)\n",
    "test_feature.dropna(axis=1, thresh=2, inplace=True)\n",
    "#test_feature.dropna(how='all',thresh = 20,inplace=True)\n",
    "test_feature = np.array(test_feature)\n",
    "wh_inf = np.isinf(test_feature)\n",
    "test_feature[wh_inf]=0\n",
    "wh_nan = np.isnan(test_feature)\n",
    "test_feature[wh_nan]=0\n",
    "\n",
    "#only use image features\n",
    "X_train = train_feature[:,:6093]\n",
    "Y_train = train_feature[:,6093]\n",
    "Y_train = Y_train.astype('int32')\n",
    "\n",
    "X_validate = validate_feature[:,:6093]\n",
    "Y_validate = validate_feature[:,6093]\n",
    "Y_validate = Y_validate.astype('int32')\n",
    "\n",
    "X_test = test_feature[:,:6093]\n",
    "Y_test = test_feature[:,6093]\n",
    "Y_test = Y_test.astype('int32')\n",
    "seed = 7\n",
    "\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(X_train) \n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(Y_train)\n",
    "\n",
    "#print(Y_train)\n",
    "\n",
    "f = open('./csv/reseg_2.csv')\n",
    "t = csv.reader(f)\n",
    "features = next(t)\n",
    "dataset = pd.read_csv('./csv/reseg_2.csv', names=features, usecols=range(1,6098), dtype=np.float64, skiprows=1, low_memory=False)\n",
    "#dataset['outcome']=pd.to_numeric(dataset['outcome'],errors='coerce')\n",
    "\n",
    "dataset.dropna(axis=1, thresh=2, inplace=True)\n",
    "dataset.dropna(how='all',thresh = 20,inplace=True)\n",
    "dataset = np.array(dataset)\n",
    "wh_inf = np.isinf(dataset)\n",
    "dataset[wh_inf]=0\n",
    "wh_nan = np.isnan(dataset)\n",
    "dataset[wh_nan]=0\n",
    "\n",
    "#print(dataset.shape)\n",
    "wh_inf = np.isinf(dataset)\n",
    "dataset[wh_inf]=0\n",
    "wh_nan = np.isnan(dataset)\n",
    "dataset[wh_nan]=0\n",
    "\n",
    "features = dataset\n",
    "X_test = features[:,0:6093]\n",
    "Y_test = features[:,6093]\n",
    "\n",
    "print(Y_test)\n",
    "\n",
    "\n",
    "\n",
    "pipe = joblib.load('./handpkl/EndoBAGRELF20.pkl')\n",
    "Y_pred = pipe.predict(X_test)\n",
    "Y_proba = pipe.predict_proba(X_test)\n",
    "proba1 = np.empty((len(Y_pred),1))\n",
    "for i in range(len(Y_pred)):\n",
    "    proba1[i] = Y_proba[i][1]\n",
    "print(\"Accuracy: \" + repr(accuracy_score(Y_test, Y_pred)))\n",
    "print(\"Average Precision Score: \" + repr(average_precision_score(Y_test, Y_pred)))\n",
    "print(\"Kappa: \" + repr(cohen_kappa_score(Y_test, Y_pred)))\n",
    "print(\"Hamming Loss: \" + repr(hamming_loss(Y_test, Y_pred)))\n",
    "print(\"AUC: \" + repr(roc_auc_score(Y_test, proba1)))\n",
    "print(\"Sensitivity\" + repr(recall_score(Y_test,Y_pred)))\n",
    "tn,fp,fn,tp = confusion_matrix(Y_test,Y_pred).ravel()\n",
    "print(\"Specificity\" + repr(tn/(tn+fp)))\n",
    "print(tn,fp,fn,tp)\n",
    "\n",
    "import sys\n",
    "#p is proportion of trials that were successes\n",
    "#n is the number of trials\n",
    "import math\n",
    "def adjusted_wald(p, n, z=1.96):\n",
    "    p_adj = (n * p + (z**2)/2)/(n+z**2)\n",
    "    n_adj = n + z**2\n",
    "    span = z * math.sqrt(p_adj*(1-p_adj)/n_adj)\n",
    "    return max(0, p_adj - span), min(p_adj + span, 1.0)\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.66), float(83))))\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.53), float(38))))\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.78), float(45))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(849, 6097)\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "Accuracy: 0.6746987951807228\n",
      "Average Precision Score: 0.5798438764130934\n",
      "Kappa: 0.3324396782841823\n",
      "Hamming Loss: 0.3253012048192771\n",
      "AUC: 0.7926900584795321\n",
      "PRAUC0.7863537860832776\n",
      "Sensitivity0.5263157894736842\n",
      "Specificity0.8\n",
      "36 9 18 20\n",
      "(0.66-0.84)\n",
      "(0.38-0.68)\n",
      "(0.66-0.89)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score,precision_recall_curve,auc\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "import joblib\n",
    "\n",
    "file_feature = \"./csv/endometrium.csv\"\n",
    "file_train = \"./csv/train.csv\"\n",
    "file_validate = \"./csv/validation.csv\"\n",
    "file_test = \"./csv/test.csv\"\n",
    "\n",
    "f = open(file_feature)\n",
    "csv_f = csv.reader(f)\n",
    "features = next(csv_f)\n",
    "dataset = pd.read_csv(file_feature, names=features, usecols=range(1,6098), dtype=np.float64, skiprows=1, low_memory=False)\n",
    "f = open(file_train)\n",
    "csv_f = csv.reader(f)\n",
    "features = next(csv_f)\n",
    "dataset_train = pd.read_csv(file_train, names=features, usecols=range(1,1), dtype=np.float64, skiprows=1, low_memory=False)\n",
    "\n",
    "with open('./csv/train.csv','r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    train_list = [row[1] for row in reader]\n",
    "with open('./csv/validation.csv','r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    validation_list = [row['patient'] for row in reader]\n",
    "with open('./csv/test.csv','r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    test_list = [row['patient'] for row in reader]\n",
    "\n",
    "dataset['outcome'] = pd.to_numeric(dataset['outcome'],errors='coerce')\n",
    "array_OG = dataset.values\n",
    "print(array_OG.shape)\n",
    "train_list = train_list[1:]\n",
    "validation_list = validation_list[0:]\n",
    "test_list = test_list[0:]\n",
    "#print(test_list)\n",
    "#print(train_list)\n",
    "#print(validation_list)\n",
    "\n",
    "def cat_str(num_list):\n",
    "    n_list = []\n",
    "    for i in num_list:\n",
    "        temp = i[12:]\n",
    "        n_list.append(temp)\n",
    "    n_list = [int(x) for x in n_list]\n",
    "    return n_list\n",
    "\n",
    "train_list = cat_str(train_list)\n",
    "validation_list = cat_str(validation_list)\n",
    "test_list = cat_str(test_list)\n",
    "\n",
    "#print(train_list)\n",
    "#print(validation_list)\n",
    "#print(test_list)\n",
    "#print(len(test_list))\n",
    "\n",
    "train_feature = []\n",
    "validate_feature = []\n",
    "test_feature = []\n",
    "count = 1\n",
    "for i in range(len(array_OG)):\n",
    "    num = i + 1\n",
    "    if num in train_list:\n",
    "        train_feature.append(array_OG[i])\n",
    "    elif num in validation_list:\n",
    "        validate_feature.append(array_OG[i])\n",
    "    elif num in test_list:\n",
    "        #print(count)\n",
    "        count = count + 1\n",
    "        test_feature.append(array_OG[i])\n",
    "        #print(num)\n",
    "        #print(array_OG[i,6096])\n",
    "        \n",
    "train_feature = np.array(train_feature)\n",
    "validate_feature = np.array(validate_feature)\n",
    "test_feature = np.array(test_feature)\n",
    "\n",
    "train_feature = pd.DataFrame(train_feature)\n",
    "train_feature.dropna(axis=1, thresh=2, inplace=True)\n",
    "train_feature.dropna(how='all',thresh = 20,inplace=True)\n",
    "train_feature = np.array(train_feature)\n",
    "wh_inf = np.isinf(train_feature)\n",
    "train_feature[wh_inf]=0\n",
    "wh_nan = np.isnan(train_feature)\n",
    "train_feature[wh_nan]=0\n",
    "\n",
    "validate_feature = pd.DataFrame(validate_feature)\n",
    "validate_feature.dropna(axis=1, thresh=2, inplace=True)\n",
    "#validate_feature.dropna(how='all',thresh = 20,inplace=True)\n",
    "validate_feature = np.array(validate_feature)\n",
    "wh_inf = np.isinf(validate_feature)\n",
    "validate_feature[wh_inf]=0\n",
    "wh_nan = np.isnan(validate_feature)\n",
    "validate_feature[wh_nan]=0\n",
    "\n",
    "test_feature = pd.DataFrame(test_feature)\n",
    "test_feature.dropna(axis=1, thresh=2, inplace=True)\n",
    "#test_feature.dropna(how='all',thresh = 20,inplace=True)\n",
    "test_feature = np.array(test_feature)\n",
    "wh_inf = np.isinf(test_feature)\n",
    "test_feature[wh_inf]=0\n",
    "wh_nan = np.isnan(test_feature)\n",
    "test_feature[wh_nan]=0\n",
    "\n",
    "#only use image features\n",
    "X_train = train_feature[:,:6093]\n",
    "Y_train = train_feature[:,6093]\n",
    "Y_train = Y_train.astype('int32')\n",
    "\n",
    "X_validate = validate_feature[:,:6093]\n",
    "Y_validate = validate_feature[:,6093]\n",
    "Y_validate = Y_validate.astype('int32')\n",
    "\n",
    "X_test = test_feature[:,:6093]\n",
    "Y_test = test_feature[:,6093]\n",
    "Y_test = Y_test.astype('int32')\n",
    "seed = 7\n",
    "\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(X_train) \n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(Y_train)\n",
    "\n",
    "#print(Y_train)\n",
    "\n",
    "f = open('./csv/rereseg_2.csv')\n",
    "t = csv.reader(f)\n",
    "features = next(t)\n",
    "dataset = pd.read_csv('./csv/rereseg_2.csv', names=features, usecols=range(1,6098), dtype=np.float64, skiprows=1, low_memory=False)\n",
    "#dataset['outcome']=pd.to_numeric(dataset['outcome'],errors='coerce')\n",
    "\n",
    "dataset.dropna(axis=1, thresh=2, inplace=True)\n",
    "dataset.dropna(how='all',thresh = 20,inplace=True)\n",
    "dataset = np.array(dataset)\n",
    "wh_inf = np.isinf(dataset)\n",
    "dataset[wh_inf]=0\n",
    "wh_nan = np.isnan(dataset)\n",
    "dataset[wh_nan]=0\n",
    "\n",
    "#print(dataset.shape)\n",
    "wh_inf = np.isinf(dataset)\n",
    "dataset[wh_inf]=0\n",
    "wh_nan = np.isnan(dataset)\n",
    "dataset[wh_nan]=0\n",
    "\n",
    "features = dataset\n",
    "X_test = features[:,0:6093]\n",
    "Y_test = features[:,6093]\n",
    "\n",
    "print(Y_test)\n",
    "\n",
    "\n",
    "\n",
    "pipe = joblib.load('./handpkl/EndoBAGRELF20.pkl')\n",
    "Y_pred = pipe.predict(X_test)\n",
    "Y_proba = pipe.predict_proba(X_test)\n",
    "proba1 = np.empty((len(Y_pred),1))\n",
    "for i in range(len(Y_pred)):\n",
    "    proba1[i] = Y_proba[i][1]\n",
    "print(\"Accuracy: \" + repr(accuracy_score(Y_test, Y_pred)))\n",
    "print(\"Average Precision Score: \" + repr(average_precision_score(Y_test, Y_pred)))\n",
    "print(\"Kappa: \" + repr(cohen_kappa_score(Y_test, Y_pred)))\n",
    "print(\"Hamming Loss: \" + repr(hamming_loss(Y_test, Y_pred)))\n",
    "print(\"AUC: \" + repr(roc_auc_score(Y_test, proba1)))\n",
    "fpr, tpr, _= precision_recall_curve(Y_test,proba1)\n",
    "print(\"PRAUC\"+repr(auc(tpr,fpr)))\n",
    "print(\"Sensitivity\" + repr(recall_score(Y_test,Y_pred)))\n",
    "tn,fp,fn,tp = confusion_matrix(Y_test,Y_pred).ravel()\n",
    "print(\"Specificity\" + repr(tn/(tn+fp)))\n",
    "print(tn,fp,fn,tp)\n",
    "\n",
    "import sys\n",
    "#p is proportion of trials that were successes\n",
    "#n is the number of trials\n",
    "import math\n",
    "def adjusted_wald(p, n, z=1.96):\n",
    "    p_adj = (n * p + (z**2)/2)/(n+z**2)\n",
    "    n_adj = n + z**2\n",
    "    span = z * math.sqrt(p_adj*(1-p_adj)/n_adj)\n",
    "    return max(0, p_adj - span), min(p_adj + span, 1.0)\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.76), float(83))))\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.53), float(38))))\n",
    "print(\"({:.2f}-{:.2f})\".format(*adjusted_wald(float(0.80), float(45))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
